{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"YAML structure","text":""},{"location":"#geniusrise-documentation","title":"Geniusrise Documentation","text":""},{"location":"#about","title":"About","text":"<p>Geniusrise is a modular, loosely-coupled MLOps framework designed for the era of Large Language Models, offering flexibility, inclusivity, and standardization. It seamlessly integrates tasks, state management, data handling, and model versioning, all while supporting diverse infrastructures and user expertise levels. With its plug-and-play architecture, Geniusrise empowers teams to build, share, and deploy ML workflows across various platforms efficiently.</p>"},{"location":"#guides","title":"Guides","text":""},{"location":"#getting-started","title":"Getting started","text":"<ol> <li>Home: Front page, you are here.</li> <li>Concepts: Concepts of the framework, start here.</li> <li>Architecture: Design and architecture of the framework.</li> <li>Installation: Installation and setup.</li> </ol>"},{"location":"#development","title":"Development","text":"<ol> <li>Local experimentation: Local setup and project creation.</li> <li>Dev Cycle: Describes one full local development cycle.</li> <li>Packaging: Packaging your application</li> <li>Staged deployment: Deploying parts or whole of your application</li> <li>Workflow ops: Operations and management of workflows</li> <li>Data ops: Operations and management of data</li> <li>Model ops: Operations and management of models</li> </ol>"},{"location":"#deployment","title":"Deployment","text":"<ol> <li>Kubernetes: Running geniusrise on kubernetes</li> <li>Apache Airflow: Orchestrating batch jobs on Apache Airflow</li> <li>Apache Spark: Using geniusrise as a spark library</li> <li>Apache Flink: Using geniusrise as a flink library</li> <li>Apache Beam: Using geniusrise as a beam library</li> <li>Apache Storm: Using geniusrise as a storm library</li> <li>AWS ECS: Running geniusrise on AWS ECS</li> <li>AWS Batch: Running geniusrise batch jobs on AWS Batch</li> </ol>"},{"location":"#reference","title":"Reference","text":"<ol> <li>YAML structure: Geniusfile structure and configuration</li> <li>Community Plugins: Building and shipping community plugins (spouts and bolts)</li> <li>Project templates: Project templates for community plugins</li> </ol>"},{"location":"#examples","title":"Examples","text":"<ol> <li>Write a confluence PRD and have jira and github issues created and linked automatically</li> <li>Monitor, alert and summarize machinery metrics in a process industry</li> <li>Read news and stock market tickers to generate buy and sell alerts</li> <li>Read infosec logs generated by various services and decide whether to generate an alert</li> <li>Batch or streaming fine-tuning and managing a huggingface-hosted model</li> <li>Batch or streaming fine-tuning and managing an openAI model</li> </ol>"},{"location":"#library-reference","title":"Library Reference","text":"<ul> <li>geniusrise.cli:<ul> <li>geniusctl: The main command line application</li> <li>yamlctl: Control spouts and bolts defined in a YAML file</li> <li>boltctl: The main bolt controller</li> <li>spoutctl: The main spout controller</li> <li>schema: YAML schema definition as pydantic</li> <li>discover: Module discovery</li> </ul> </li> <li>geniusrise.core:<ul> <li>bolt: Core Bolt class</li> <li>spout: Core Spout class</li> <li>geniusrise.core.data:<ul> <li>batch_input: Batch input manager</li> <li>batch_output: Batch output manager</li> <li>input: Input manager base class</li> <li>output: Output manager base class</li> <li>streaming_input: Streaming input manager</li> <li>streaming_output: Streaming output manager</li> </ul> </li> <li>geniusrise.core.state:<ul> <li>base: Base class for task state mnager</li> <li>dynamo: State manager using dynamoDB</li> <li>memory: State manager using local memory</li> <li>postgres: State manager using postgres database</li> <li>redis: State manager using redis</li> </ul> </li> <li>geniusrise.core.task:<ul> <li>base: Base class for Task</li> </ul> </li> <li>geniusrise.runners:<ul> <li>ecs: Runner class using AWS ECS</li> <li>k8s: Runner class using kubernetes</li> </ul> </li> </ul> </li> </ul>"},{"location":"core/cli_boltctl/","title":"Boltctl","text":"<p>The main bolt controller</p>"},{"location":"core/cli_boltctl/#cli.boltctl.BoltCtl","title":"<code>BoltCtl</code>","text":"<p>Class for managing bolts end-to-end from the command line.</p>"},{"location":"core/cli_boltctl/#cli.boltctl.BoltCtl.__init__","title":"<code>__init__(discovered_bolt)</code>","text":"<p>Initialize BoltCtl with a DiscoveredBolt object.</p> <p>Parameters:</p> Name Type Description Default <code>discovered_bolt</code> <code>DiscoveredBolt</code> <p>DiscoveredBolt object used to create and manage bolts.</p> required"},{"location":"core/cli_boltctl/#cli.boltctl.BoltCtl.create_bolt","title":"<code>create_bolt(input_type, output_type, state_type, **kwargs)</code>","text":"<p>Create a bolt of a specific type.</p> <p>Parameters:</p> Name Type Description Default <code>input_type</code> <code>str</code> <p>The type of input config (\"batch\" or \"streaming\").</p> required <code>output_type</code> <code>str</code> <p>The type of output config (\"batch\" or \"streaming\").</p> required <code>state_type</code> <code>str</code> <p>The type of state manager (\"in_memory\", \"redis\", \"postgres\", or \"dynamodb\").</p> required <code>**kwargs</code> <p>Additional keyword arguments for initializing the bolt. <pre><code>Keyword Arguments:\n    Batch input config:\n    - input_folder (str): The input folder argument.\n    - input_s3_bucket (str): The input bucket argument.\n    - input_s3_folder (str): The input S3 folder argument.\n    Batch outupt config:\n    - output_folder (str): The output folder argument.\n    - output_s3_bucket (str): The output bucket argument.\n    - output_s3_folder (str): The output S3 folder argument.\n    Streaming input config:\n    - input_kafka_cluster_connection_string (str): The input Kafka servers argument.\n    - input_kafka_topic (str): The input kafka topic argument.\n    - input_kafka_consumer_group_id (str): The Kafka consumer group id.\n    Streaming output config:\n    - output_kafka_cluster_connection_string (str): The output Kafka servers argument.\n    - output_kafka_topic (str): The output kafka topic argument.\n    Redis state manager config:\n    - redis_host (str): The Redis host argument.\n    - redis_port (str): The Redis port argument.\n    - redis_db (str): The Redis database argument.\n    Postgres state manager config:\n    - postgres_host (str): The PostgreSQL host argument.\n    - postgres_port (str): The PostgreSQL port argument.\n    - postgres_user (str): The PostgreSQL user argument.\n    - postgres_password (str): The PostgreSQL password argument.\n    - postgres_database (str): The PostgreSQL database argument.\n    - postgres_table (str): The PostgreSQL table argument.\n    DynamoDB state manager config:\n    - dynamodb_table_name (str): The DynamoDB table name argument.\n    - dynamodb_region_name (str): The DynamoDB region name argument.\n</code></pre></p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Bolt</code> <code>Bolt</code> <p>The created bolt.</p>"},{"location":"core/cli_boltctl/#cli.boltctl.BoltCtl.create_parser","title":"<code>create_parser(parser)</code>","text":"<p>Add arguments to the command-line parser for managing the bolt.</p> <p>Parameters:</p> Name Type Description Default <code>parser</code> <code>argparse.ArgumentParser</code> <p>Command-line parser.</p> required"},{"location":"core/cli_boltctl/#cli.boltctl.BoltCtl.execute_bolt","title":"<code>execute_bolt(bolt, method_name, *args, **kwargs)</code>","text":"<p>Execute a method of a bolt.</p> <p>Parameters:</p> Name Type Description Default <code>bolt</code> <code>Bolt</code> <p>The bolt to execute.</p> required <code>method_name</code> <code>str</code> <p>The name of the method to execute.</p> required <code>*args</code> <p>Positional arguments to pass to the method.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments to pass to the method.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <p>The result of the method.</p>"},{"location":"core/cli_boltctl/#cli.boltctl.BoltCtl.run","title":"<code>run(args)</code>","text":"<p>Run the command-line interface.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>argparse.Namespace</code> <p>Parsed command-line arguments.</p> required"},{"location":"core/cli_discover/","title":"Discover","text":"<p>Module discovery</p>"},{"location":"core/cli_discover/#cli.discover.Discover","title":"<code>Discover</code>","text":""},{"location":"core/cli_discover/#cli.discover.Discover.__init__","title":"<code>__init__(directory=None)</code>","text":"<p>Initialize the Discover class.</p>"},{"location":"core/cli_discover/#cli.discover.Discover.discover_installed_extensions","title":"<code>discover_installed_extensions()</code>","text":"<p>Discover installed geniusrise extensions.</p>"},{"location":"core/cli_discover/#cli.discover.Discover.find_classes","title":"<code>find_classes(module)</code>","text":"<p>Discover spout/bolt classes in a module.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Any</code> <p>Module to scan for spout/bolt classes.</p> required"},{"location":"core/cli_discover/#cli.discover.Discover.get_geniusignore_patterns","title":"<code>get_geniusignore_patterns(directory)</code>  <code>staticmethod</code>","text":"<p>Read the .geniusignore file and return a list of patterns to ignore.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Directory containing the .geniusignore file.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of patterns to ignore.</p>"},{"location":"core/cli_discover/#cli.discover.Discover.get_init_args","title":"<code>get_init_args(cls)</code>","text":"<p>Extract initialization arguments of a class.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>type</code> <p>Class to extract initialization arguments from.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Initialization arguments.</p>"},{"location":"core/cli_discover/#cli.discover.Discover.import_module","title":"<code>import_module(path)</code>","text":"<p>Import a module given its path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the module.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <p>Imported module.</p>"},{"location":"core/cli_discover/#cli.discover.Discover.scan_directory","title":"<code>scan_directory(directory=None)</code>","text":"<p>Scan for spouts/bolts in installed extensions and user's codebase.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Optional[str]</code> <p>Directory to scan for user-defined spouts/bolts.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Discovered spouts/bolts.</p>"},{"location":"core/cli_geniusctl/","title":"Geniusctl","text":"<p>The main command line application</p>"},{"location":"core/cli_geniusctl/#cli.geniusctl.GeniusCtl","title":"<code>GeniusCtl</code>","text":"<p>Main class for managing the geniusrise CLI application.</p>"},{"location":"core/cli_geniusctl/#cli.geniusctl.GeniusCtl.__init__","title":"<code>__init__()</code>","text":"<p>Initialize GeniusCtl.v</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>The directory to scan for spouts and bolts.</p> required"},{"location":"core/cli_geniusctl/#cli.geniusctl.GeniusCtl.cli","title":"<code>cli()</code>","text":"<p>Main function to be called when geniusrise is run from the command line.</p>"},{"location":"core/cli_geniusctl/#cli.geniusctl.GeniusCtl.create_parser","title":"<code>create_parser()</code>","text":"<p>Create a command-line parser with arguments for managing the application.</p> <p>Returns:</p> Type Description <p>argparse.ArgumentParser: Command-line parser.</p>"},{"location":"core/cli_geniusctl/#cli.geniusctl.GeniusCtl.list_spouts_and_bolts","title":"<code>list_spouts_and_bolts(verbose=False)</code>","text":"<p>List all discovered spouts and bolts in a table.</p>"},{"location":"core/cli_geniusctl/#cli.geniusctl.GeniusCtl.run","title":"<code>run(args)</code>","text":"<p>Run the command-line interface.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>argparse.Namespace</code> <p>Parsed command-line arguments.</p> required"},{"location":"core/cli_schema/","title":"YAML schema","text":"<p>YAML schema definition as pydantic</p>"},{"location":"core/cli_schema/#cli.schema.Bolt","title":"<code>Bolt</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This class defines a bolt. A bolt has a name, method, optional arguments, input, output, state, and deployment.</p>"},{"location":"core/cli_schema/#cli.schema.Deploy","title":"<code>Deploy</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This class defines the deployment of the spout or bolt. The deployment can be of type k8s or ecs.</p>"},{"location":"core/cli_schema/#cli.schema.DeployArgs","title":"<code>DeployArgs</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This class defines the arguments for the deployment. Depending on the type of deployment (k8s, ecs), different arguments are required.</p>"},{"location":"core/cli_schema/#cli.schema.ExtraKwargs","title":"<code>ExtraKwargs</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This class is used to handle any extra arguments that are not explicitly defined in the schema.</p>"},{"location":"core/cli_schema/#cli.schema.Geniusfile","title":"<code>Geniusfile</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This class defines the overall structure of the YAML file. It includes a version, spouts, and bolts.</p>"},{"location":"core/cli_schema/#cli.schema.Input","title":"<code>Input</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This class defines the input of the bolt. The input can be of type batch, streaming, spout, or bolt.</p>"},{"location":"core/cli_schema/#cli.schema.InputArgs","title":"<code>InputArgs</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This class defines the arguments for the input. Depending on the type of input (batch, streaming, spout, bolt), different arguments are required.</p>"},{"location":"core/cli_schema/#cli.schema.Output","title":"<code>Output</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This class defines the output of the spout or bolt. The output can be of type batch or streaming.</p>"},{"location":"core/cli_schema/#cli.schema.OutputArgs","title":"<code>OutputArgs</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This class defines the arguments for the output. Depending on the type of output (batch, streaming), different arguments are required.</p>"},{"location":"core/cli_schema/#cli.schema.Spout","title":"<code>Spout</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This class defines a spout. A spout has a name, method, optional arguments, output, state, and deployment.</p>"},{"location":"core/cli_schema/#cli.schema.State","title":"<code>State</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This class defines the state of the spout or bolt. The state can be of type in_memory, redis, postgres, or dynamodb.</p>"},{"location":"core/cli_schema/#cli.schema.StateArgs","title":"<code>StateArgs</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This class defines the arguments for the state. Depending on the type of state (in_memory, redis, postgres, dynamodb), different arguments are required.</p>"},{"location":"core/cli_spoutctl/","title":"Spoutctl","text":"<p>The main spout controller</p>"},{"location":"core/cli_spoutctl/#cli.spoutctl.SpoutCtl","title":"<code>SpoutCtl</code>","text":"<p>Class for managing spouts end-to-end from the command line.</p>"},{"location":"core/cli_spoutctl/#cli.spoutctl.SpoutCtl.__init__","title":"<code>__init__(discovered_spout)</code>","text":"<p>Initialize SpoutCtl with a DiscoveredSpout object.</p> <p>Parameters:</p> Name Type Description Default <code>discovered_spout</code> <code>DiscoveredSpout</code> <p>DiscoveredSpout object used to create and manage spouts.</p> required"},{"location":"core/cli_spoutctl/#cli.spoutctl.SpoutCtl.create_parser","title":"<code>create_parser(parser)</code>","text":"<p>Add arguments to the command-line parser for managing the spout.</p> <p>Parameters:</p> Name Type Description Default <code>parser</code> <code>argparse.ArgumentParser</code> <p>Command-line parser.</p> required"},{"location":"core/cli_spoutctl/#cli.spoutctl.SpoutCtl.create_spout","title":"<code>create_spout(output_type, state_type, **kwargs)</code>","text":"<p>Create a spout of a specific type.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>str</code> <p>The type of output config (\"batch\" or \"streaming\").</p> required <code>state_type</code> <code>str</code> <p>The type of state manager (\"in_memory\", \"redis\", \"postgres\", or \"dynamodb\").</p> required <code>**kwargs</code> <p>Additional keyword arguments for initializing the spout. <pre><code>Keyword Arguments:\n    Batch output config:\n    - output_folder (str): The directory where output files should be stored temporarily.\n    - output_s3_bucket (str): The name of the S3 bucket for output storage.\n    - output_s3_folder (str): The S3 folder for output storage.\n    Streaming output config:\n    - output_kafka_topic (str): Kafka output topic for streaming spouts.\n    - output_kafka_cluster_connection_string (str): Kafka connection string for streaming spouts.\n    Redis state manager config:\n    - redis_host (str): The Redis host argument.\n    - redis_port (str): The Redis port argument.\n    - redis_db (str): The Redis database argument.\n    Postgres state manager config:\n    - postgres_host (str): The PostgreSQL host argument.\n    - postgres_port (str): The PostgreSQL port argument.\n    - postgres_user (str): The PostgreSQL user argument.\n    - postgres_password (str): The PostgreSQL password argument.\n    - postgres_database (str): The PostgreSQL database argument.\n    - postgres_table (str): The PostgreSQL table argument.\n    DynamoDB state manager config:\n    - dynamodb_table_name (str): The DynamoDB table name argument.\n    - dynamodb_region_name (str): The DynamoDB region name argument.\n</code></pre></p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Spout</code> <code>Spout</code> <p>The created spout.</p>"},{"location":"core/cli_spoutctl/#cli.spoutctl.SpoutCtl.execute_spout","title":"<code>execute_spout(spout, method_name, *args, **kwargs)</code>","text":"<p>Execute a method of a spout.</p> <p>Parameters:</p> Name Type Description Default <code>spout</code> <code>Spout</code> <p>The spout to execute.</p> required <code>method_name</code> <code>str</code> <p>The name of the method to execute.</p> required <code>*args</code> <p>Positional arguments to pass to the method.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments to pass to the method.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <p>The result of the method.</p>"},{"location":"core/cli_spoutctl/#cli.spoutctl.SpoutCtl.run","title":"<code>run(args)</code>","text":"<p>Run the command-line interface.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>argparse.Namespace</code> <p>Parsed command-line arguments.</p> required"},{"location":"core/cli_yamlctl/","title":"YamlCtl","text":"<p>Control spouts and bolts defined in a YAML file</p>"},{"location":"core/cli_yamlctl/#cli.yamlctl.YamlCtl","title":"<code>YamlCtl</code>","text":"<p>Command-line interface for managing spouts and bolts based on a YAML configuration.</p> <p>The YamlCtl class provides methods to run specific or all spouts and bolts defined in a YAML file. The YAML file's structure is defined by the Geniusfile schema.</p> <p>Example YAML structure: <pre><code>version: \"1\"\nspouts:\n  spout_name1:\n    name: \"spout1\"\n    method: \"method_name\"\n    ...\nbolts:\n  bolt_name1:\n    name: \"bolt1\"\n    method: \"method_name\"\n    ...\n</code></pre></p> <p>Attributes:</p> Name Type Description <code>geniusfile</code> <code>Geniusfile</code> <p>Parsed YAML configuration.</p> <code>spout_ctls</code> <code>Dict[str, SpoutCtl]</code> <p>Dictionary of SpoutCtl instances.</p> <code>bolt_ctls</code> <code>Dict[str, BoltCtl]</code> <p>Dictionary of BoltCtl instances.</p>"},{"location":"core/cli_yamlctl/#cli.yamlctl.YamlCtl.__init__","title":"<code>__init__(spout_ctls, bolt_ctls)</code>","text":"<p>Initialize YamlCtl with the path to the YAML file and control instances for spouts and bolts.</p> <p>Parameters:</p> Name Type Description Default <code>spout_ctls</code> <code>Dict[str, SpoutCtl]</code> <p>Dictionary of SpoutCtl instances.</p> required <code>bolt_ctls</code> <code>Dict[str, BoltCtl]</code> <p>Dictionary of BoltCtl instances.</p> required"},{"location":"core/cli_yamlctl/#cli.yamlctl.YamlCtl.create_parser","title":"<code>create_parser(parser)</code>","text":"<p>Create and return the command-line parser for managing spouts and bolts.</p>"},{"location":"core/cli_yamlctl/#cli.yamlctl.YamlCtl.resolve_reference","title":"<code>resolve_reference(input_type, ref_name)</code>","text":"<p>Resolve the reference of a bolt's input based on the input type (spout or bolt).</p> <p>Parameters:</p> Name Type Description Default <code>input_type</code> <code>str</code> <p>Type of the input (\"spout\" or \"bolt\").</p> required <code>ref_name</code> <code>str</code> <p>Name of the spout or bolt to refer to.</p> required <p>Returns:</p> Name Type Description <code>Output</code> <p>The output configuration of the referred spout or bolt.</p>"},{"location":"core/cli_yamlctl/#cli.yamlctl.YamlCtl.run","title":"<code>run(args)</code>","text":"<p>Run the command-line interface for managing spouts and bolts based on provided arguments. Please note that there is no ordering of the spouts and bolts in the YAML configuration. Each spout and bolt is an independent entity even when connected together.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>argparse.Namespace</code> <p>Parsed command-line arguments.</p> required"},{"location":"core/cli_yamlctl/#cli.yamlctl.YamlCtl.run_bolt","title":"<code>run_bolt(bolt_name)</code>","text":"<p>Run a specific bolt based on its name.</p> <p>Parameters:</p> Name Type Description Default <code>bolt_name</code> <code>str</code> <p>Name of the bolt to run.</p> required"},{"location":"core/cli_yamlctl/#cli.yamlctl.YamlCtl.run_bolts","title":"<code>run_bolts()</code>","text":"<p>Run all bolts defined in the YAML configuration.</p>"},{"location":"core/cli_yamlctl/#cli.yamlctl.YamlCtl.run_spout","title":"<code>run_spout(spout_name)</code>","text":"<p>Run a specific spout based on its name.</p> <p>Parameters:</p> Name Type Description Default <code>spout_name</code> <code>str</code> <p>Name of the spout to run.</p> required"},{"location":"core/cli_yamlctl/#cli.yamlctl.YamlCtl.run_spouts","title":"<code>run_spouts()</code>","text":"<p>Run all spouts defined in the YAML configuration.</p>"},{"location":"core/config/","title":"Encironment Configuration","text":""},{"location":"core/core_bolt/","title":"Bolt","text":"<p>Core Bolt class</p>"},{"location":"core/core_bolt/#core.bolt.Bolt","title":"<code>Bolt</code>","text":"<p>             Bases: <code>Task</code></p> <p>Base class for all bolts.</p> <p>A bolt is a component that consumes streams of data, processes them, and possibly emits new data streams.</p>"},{"location":"core/core_bolt/#core.bolt.Bolt.__call__","title":"<code>__call__(method_name, *args, **kwargs)</code>","text":"<p>Execute a method locally and manage the state.</p> <p>Parameters:</p> Name Type Description Default <code>method_name</code> <code>str</code> <p>The name of the method to execute.</p> required <code>*args</code> <p>Positional arguments to pass to the method.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments to pass to the method. Keyword Arguments:     - Additional keyword arguments specific to the method.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The result of the method.</p>"},{"location":"core/core_bolt/#core.bolt.Bolt.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>The <code>Bolt</code> class is a base class for all bolts in the given context. It inherits from the <code>Task</code> class and provides methods for executing tasks both locally and remotely, as well as managing their state, with state management options including in-memory, Redis, PostgreSQL, and DynamoDB, and input and output configurations for batch or streaming data.</p> <p>The <code>Bolt</code> class uses the <code>Input</code>, <code>Output</code> and <code>State</code> classes, which are abstract base classes for managing input configurations, output configurations and states, respectively. The <code>Input</code> and <code>Output</code> classes each have two subclasses: <code>StreamingInput</code>, <code>BatchInput</code>, <code>StreamingOutput</code> and <code>BatchOutput</code>, which manage streaming and batch input and output configurations, respectively. The <code>State</code> class is used to get and set state, and it has several subclasses for different types of state managers.</p> <p>The <code>Bolt</code> class also uses the <code>ECSManager</code> and <code>K8sManager</code> classes in the <code>execute_remote</code> method, which are used to manage tasks on Amazon ECS and Kubernetes, respectively.</p> Usage <ul> <li>Create an instance of the Bolt class by providing an Input object, an Output object and a State object.</li> <li>The Input object specifies the input configuration for the bolt.</li> <li>The Output object specifies the output configuration for the bolt.</li> <li>The State object handles the management of the bolt's state.</li> </ul> Example <p>input = Input(...) output = Output(...) state = State(...) bolt = Bolt(input, output, state)</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Input</code> <p>The input configuration.</p> required <code>output</code> <code>Output</code> <p>The output configuration.</p> required <code>state</code> <code>State</code> <p>The state manager.</p> required"},{"location":"core/core_bolt/#core.bolt.Bolt.create","title":"<code>create(klass, input_type, output_type, state_type, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a bolt of a specific type.</p> <p>This static method is used to create a bolt of a specific type. It takes in an input type, an output type, a state type, and additional keyword arguments for initializing the bolt.</p> <p>The method creates the input config, output config, and state manager based on the provided types, and then creates and returns a bolt using these configurations.</p> <p>Parameters:</p> Name Type Description Default <code>klass</code> <code>type</code> <p>The Bolt class to create.</p> required <code>input_type</code> <code>str</code> <p>The type of input config (\"batch\" or \"streaming\").</p> required <code>output_type</code> <code>str</code> <p>The type of output config (\"batch\" or \"streaming\").</p> required <code>state_type</code> <code>str</code> <p>The type of state manager (\"in_memory\", \"redis\", \"postgres\", or \"dynamodb\").</p> required <code>**kwargs</code> <p>Additional keyword arguments for initializing the bolt. <pre><code>Keyword Arguments:\n    Batch input config:\n    - input_folder (str): The input folder argument.\n    - input_s3_bucket (str): The input bucket argument.\n    - input_s3_folder (str): The input S3 folder argument.\n    Batch outupt config:\n    - output_folder (str): The output folder argument.\n    - output_s3_bucket (str): The output bucket argument.\n    - output_s3_folder (str): The output S3 folder argument.\n    Streaming input config:\n    - input_kafka_cluster_connection_string (str): The input Kafka servers argument.\n    - input_kafka_topic (str): The input kafka topic argument.\n    - input_kafka_consumer_group_id (str): The Kafka consumer group id.\n    Streaming output config:\n    - output_kafka_cluster_connection_string (str): The output Kafka servers argument.\n    - output_kafka_topic (str): The output kafka topic argument.\n    Redis state manager config:\n    - redis_host (str): The Redis host argument.\n    - redis_port (str): The Redis port argument.\n    - redis_db (str): The Redis database argument.\n    Postgres state manager config:\n    - postgres_host (str): The PostgreSQL host argument.\n    - postgres_port (str): The PostgreSQL port argument.\n    - postgres_user (str): The PostgreSQL user argument.\n    - postgres_password (str): The PostgreSQL password argument.\n    - postgres_database (str): The PostgreSQL database argument.\n    - postgres_table (str): The PostgreSQL table argument.\n    DynamoDB state manager config:\n    - dynamodb_table_name (str): The DynamoDB table name argument.\n    - dynamodb_region_name (str): The DynamoDB region name argument.\n</code></pre></p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Bolt</code> <code>Bolt</code> <p>The created bolt.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid input type, output type, or state type is provided.</p>"},{"location":"core/core_data_batch_input/","title":"Batch data input","text":"<p>Batch input manager</p>"},{"location":"core/core_data_batch_input/#core.data.batch_input.BatchInput","title":"<code>BatchInput</code>","text":"<p>             Bases: <code>Input</code></p> <p>\ud83d\udcc1 BatchInput: Manages batch input configurations.</p> <p>Attributes:</p> Name Type Description <code>input_folder</code> <code>str</code> <p>Folder to read input files.</p> <code>bucket</code> <code>str</code> <p>S3 bucket name.</p> <code>s3_folder</code> <code>str</code> <p>Folder within the S3 bucket.</p> <p>Usage: <pre><code>config = BatchInput(\"/path/to/input\", \"my_bucket\", \"s3/folder\")\nfiles = config.list_files()\ncontent = config.read_file(\"example.txt\")\n</code></pre></p>"},{"location":"core/core_data_batch_input/#core.data.batch_input.BatchInput.__init__","title":"<code>__init__(input_folder, bucket, s3_folder)</code>","text":"<p>Initialize a new batch input configuration.</p> <p>Parameters:</p> Name Type Description Default <code>input_folder</code> <code>str</code> <p>Folder to read input files.</p> required <code>bucket</code> <code>str</code> <p>S3 bucket name.</p> required <code>s3_folder</code> <code>str</code> <p>Folder within the S3 bucket.</p> required"},{"location":"core/core_data_batch_input/#core.data.batch_input.BatchInput.copy_from_remote","title":"<code>copy_from_remote()</code>","text":"<p>\ud83d\udd04 Copy contents from a given S3 bucket and location to the input folder.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If no input folder is specified.</p>"},{"location":"core/core_data_batch_input/#core.data.batch_input.BatchInput.copy_to_remote","title":"<code>copy_to_remote(filename, bucket, s3_folder)</code>","text":"<p>\u2601\ufe0f Copy a file from the input folder to an S3 bucket.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The name of the file.</p> required <code>bucket</code> <code>str</code> <p>The name of the S3 bucket.</p> required <code>s3_folder</code> <code>str</code> <p>The folder in the S3 bucket.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If no input folder is specified.</p>"},{"location":"core/core_data_batch_input/#core.data.batch_input.BatchInput.delete_file","title":"<code>delete_file(filename)</code>","text":"<p>\ud83d\uddd1\ufe0f Delete a file from the input folder.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The name of the file.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If no input folder is specified.</p>"},{"location":"core/core_data_batch_input/#core.data.batch_input.BatchInput.get","title":"<code>get()</code>","text":"<p>\ud83d\udccd Get the input folder location.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The input folder location.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If no input folder is specified.</p>"},{"location":"core/core_data_batch_input/#core.data.batch_input.BatchInput.list_files","title":"<code>list_files()</code>","text":"<p>\ud83d\udcdc List all files in the input folder.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of file paths.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If no input folder is specified.</p>"},{"location":"core/core_data_batch_input/#core.data.batch_input.BatchInput.read_file","title":"<code>read_file(filename)</code>","text":"<p>\ud83d\udcd6 Read a file from the input folder.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The name of the file.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The contents of the file.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If no input folder is specified.</p>"},{"location":"core/core_data_batch_output/","title":"Batch data output","text":"<p>Batch output manager</p>"},{"location":"core/core_data_batch_output/#core.data.batch_output.BatchOutput","title":"<code>BatchOutput</code>","text":"<p>             Bases: <code>Output</code></p> <p>\ud83d\udcc1 BatchOutput: Manages batch output configurations.</p> <p>Attributes:</p> Name Type Description <code>output_folder</code> <code>str</code> <p>Folder to save output files.</p> <code>bucket</code> <code>str</code> <p>S3 bucket name.</p> <code>s3_folder</code> <code>str</code> <p>Folder within the S3 bucket.</p> <p>Usage: <pre><code>config = BatchOutput(\"/path/to/output\", \"my_bucket\", \"s3/folder\")\nconfig.save({\"key\": \"value\"}, \"example.json\")\nfiles = config.list_files()\ncontent = config.read_file(\"example.json\")\n</code></pre></p>"},{"location":"core/core_data_batch_output/#core.data.batch_output.BatchOutput.__init__","title":"<code>__init__(output_folder, bucket, s3_folder)</code>","text":"<p>Initialize a new batch output configuration.</p> <p>Parameters:</p> Name Type Description Default <code>output_folder</code> <code>str</code> <p>Folder to save output files.</p> required <code>bucket</code> <code>str</code> <p>S3 bucket name.</p> required <code>s3_folder</code> <code>str</code> <p>Folder within the S3 bucket.</p> required"},{"location":"core/core_data_batch_output/#core.data.batch_output.BatchOutput.copy_file_to_remote","title":"<code>copy_file_to_remote(filename)</code>","text":"<p>\u2601\ufe0f Copy a specific file from the output folder to the S3 bucket.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The name of the file to copy.</p> required"},{"location":"core/core_data_batch_output/#core.data.batch_output.BatchOutput.copy_to_remote","title":"<code>copy_to_remote()</code>","text":"<p>\u2601\ufe0f Recursively copy all files and directories from the output folder to a given S3 bucket and folder.</p>"},{"location":"core/core_data_batch_output/#core.data.batch_output.BatchOutput.delete_file","title":"<code>delete_file(filename)</code>","text":"<p>\ud83d\uddd1\ufe0f Delete a file from the output folder.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The name of the file to delete.</p> required"},{"location":"core/core_data_batch_output/#core.data.batch_output.BatchOutput.flush","title":"<code>flush()</code>","text":"<p>\ud83d\udd04 Flush the output by copying all files and directories from the output folder to a given S3 bucket and folder.</p>"},{"location":"core/core_data_batch_output/#core.data.batch_output.BatchOutput.list_files","title":"<code>list_files()</code>","text":"<p>\ud83d\udcdc List all files in the output folder.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>List[str]</code> <p>The list of files in the output folder.</p>"},{"location":"core/core_data_batch_output/#core.data.batch_output.BatchOutput.read_file","title":"<code>read_file(filename)</code>","text":"<p>\ud83d\udcd6 Read a file from the output folder.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The name of the file to read.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The contents of the file.</p>"},{"location":"core/core_data_batch_output/#core.data.batch_output.BatchOutput.save","title":"<code>save(data, filename=None)</code>","text":"<p>\ud83d\udcbe Save data to a file in the output folder.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The data to save.</p> required <code>filename</code> <code>str</code> <p>The filename to use when saving the data to a file.</p> <code>None</code>"},{"location":"core/core_data_input/","title":"Data input","text":"<p>Input manager base class</p>"},{"location":"core/core_data_input/#core.data.input.Input","title":"<code>Input</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract class for managing input configurations.</p>"},{"location":"core/core_data_input/#core.data.input.Input.get","title":"<code>get()</code>  <code>abstractmethod</code>","text":"<p>Abstract method to get data from the input source.</p> <p>Returns:</p> Type Description <p>The data from the input source.</p>"},{"location":"core/core_data_output/","title":"Data output","text":"<p>Output manager base class</p>"},{"location":"core/core_data_output/#core.data.output.Output","title":"<code>Output</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for managing output configurations.</p>"},{"location":"core/core_data_output/#core.data.output.Output.flush","title":"<code>flush()</code>  <code>abstractmethod</code>","text":"<p>Flush the output. This method should be implemented by subclasses.</p>"},{"location":"core/core_data_output/#core.data.output.Output.save","title":"<code>save(data, filename=None)</code>  <code>abstractmethod</code>","text":"<p>Save data to a file or ingest it into a Kafka topic.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The data to save or ingest.</p> required <code>filename</code> <code>str</code> <p>The filename to use when saving the data to a file.</p> <code>None</code>"},{"location":"core/core_data_streaming_input/","title":"Streaming data input","text":"<p>Streaming input manager</p>"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.StreamingInput","title":"<code>StreamingInput</code>","text":"<p>             Bases: <code>Input</code></p> <p>\ud83d\udce1 StreamingInput: Manages streaming input configurations.</p> <p>Attributes:</p> Name Type Description <code>input_topic</code> <code>str</code> <p>Kafka topic to consume data.</p> <code>consumer</code> <code>KafkaConsumer</code> <p>Kafka consumer for consuming data.</p> <p>Usage: <pre><code>config = StreamingInput(\"my_topic\", \"localhost:9092\")\nfor message in config.iterator():\nprint(message.value)\n</code></pre></p> <p>Note: - Ensure the Kafka cluster is running and accessible. - Adjust the <code>group_id</code> if needed.</p>"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.StreamingInput.__init__","title":"<code>__init__(input_topic, kafka_cluster_connection_string, group_id='geniusrise')</code>","text":"<p>Initialize a new streaming input configuration.</p> <p>Parameters:</p> Name Type Description Default <code>input_topic</code> <code>str</code> <p>Kafka topic to consume data.</p> required <code>kafka_cluster_connection_string</code> <code>str</code> <p>Kafka cluster connection string.</p> required <code>group_id</code> <code>str</code> <p>Kafka consumer group id. Defaults to \"geniusrise\".</p> <code>'geniusrise'</code>"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.StreamingInput.__iter__","title":"<code>__iter__()</code>","text":"<p>Make the class iterable.</p>"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.StreamingInput.__next__","title":"<code>__next__()</code>","text":"<p>Get the next message from the Kafka consumer.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If no Kafka consumer is available or an error occurs.</p>"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.StreamingInput.close","title":"<code>close()</code>","text":"<p>\ud83d\udeaa Close the Kafka consumer.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If an error occurs while closing the consumer.</p>"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.StreamingInput.commit","title":"<code>commit()</code>","text":"<p>\u2705 Manually commit offsets.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If an error occurs while committing offsets.</p>"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.StreamingInput.filter_messages","title":"<code>filter_messages(filter_func)</code>","text":"<p>\ud83d\udd0d Filter messages from the Kafka consumer based on a filter function.</p> <p>Parameters:</p> Name Type Description Default <code>filter_func</code> <code>callable</code> <p>A function that takes a Kafka message and returns a boolean.</p> required <p>Yields:</p> Type Description <code>Iterator</code> <p>Kafka message: The next message from the Kafka consumer that passes the filter.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If no Kafka consumer is available or an error occurs.</p>"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.StreamingInput.get","title":"<code>get()</code>","text":"<p>\ud83d\udce5 Get data from the input topic.</p> <p>Returns:</p> Name Type Description <code>KafkaConsumer</code> <code>KafkaConsumer</code> <p>The Kafka consumer.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If no input source or consumer is specified.</p>"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.StreamingInput.iterator","title":"<code>iterator()</code>","text":"<p>\ud83d\udd04 Iterator method for yielding data from the Kafka consumer.</p> <p>Yields:</p> Type Description <code>Iterator</code> <p>Kafka message: The next message from the Kafka consumer.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If no Kafka consumer is available.</p>"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.StreamingInput.seek","title":"<code>seek(partition, offset)</code>","text":"<p>\ud83d\udd0d Change the position from which the Kafka consumer reads.</p> <p>Parameters:</p> Name Type Description Default <code>partition</code> <code>int</code> <p>The partition to seek.</p> required <code>offset</code> <code>int</code> <p>The offset to seek to.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If an error occurs while seeking.</p>"},{"location":"core/core_data_streaming_output/","title":"Streaming data output","text":"<p>Streaming output manager</p>"},{"location":"core/core_data_streaming_output/#core.data.streaming_output.StreamingOutput","title":"<code>StreamingOutput</code>","text":"<p>             Bases: <code>Output</code></p> <p>\ud83d\udce1 StreamingOutput: Manages streaming output configurations.</p> <p>Attributes:</p> Name Type Description <code>output_topic</code> <code>str</code> <p>Kafka topic to ingest data.</p> <code>producer</code> <code>KafkaProducer</code> <p>Kafka producer for ingesting data.</p> <p>Usage: <pre><code>config = StreamingOutput(\"my_topic\", \"localhost:9092\")\nconfig.save({\"key\": \"value\"}, \"ignored_filename\")\nconfig.flush()\n</code></pre></p> <p>Note: - Ensure the Kafka cluster is running and accessible.</p>"},{"location":"core/core_data_streaming_output/#core.data.streaming_output.StreamingOutput.__init__","title":"<code>__init__(output_topic, kafka_servers)</code>","text":"<p>Initialize a new streaming output configuration.</p> <p>Parameters:</p> Name Type Description Default <code>output_topic</code> <code>str</code> <p>Kafka topic to ingest data.</p> required <code>kafka_servers</code> <code>str</code> <p>Kafka bootstrap servers.</p> required"},{"location":"core/core_data_streaming_output/#core.data.streaming_output.StreamingOutput.close","title":"<code>close()</code>","text":"<p>\ud83d\udeaa Close the Kafka producer.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If no Kafka producer is available.</p>"},{"location":"core/core_data_streaming_output/#core.data.streaming_output.StreamingOutput.flush","title":"<code>flush()</code>","text":"<p>\ud83d\udd04 Flush the output by flushing the Kafka producer.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If no Kafka producer is available.</p>"},{"location":"core/core_data_streaming_output/#core.data.streaming_output.StreamingOutput.partition_available","title":"<code>partition_available(partition)</code>","text":"<p>\ud83e\uddd0 Check if a partition is available in the Kafka topic.</p> <p>Parameters:</p> Name Type Description Default <code>partition</code> <code>int</code> <p>The partition to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the partition is available, False otherwise.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If no Kafka producer is available.</p>"},{"location":"core/core_data_streaming_output/#core.data.streaming_output.StreamingOutput.save","title":"<code>save(data, filename=None)</code>","text":"<p>\ud83d\udce4 Ingest data into the Kafka topic.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The data to ingest.</p> required <code>filename</code> <code>str</code> <p>This argument is ignored for streaming outputs.</p> <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If no Kafka producer is available or an error occurs.</p>"},{"location":"core/core_data_streaming_output/#core.data.streaming_output.StreamingOutput.save_bulk","title":"<code>save_bulk(messages)</code>","text":"<p>\ud83d\udce6 Send multiple messages at once to the Kafka topic.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list</code> <p>The messages to send.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If no Kafka producer is available or an error occurs.</p>"},{"location":"core/core_data_streaming_output/#core.data.streaming_output.StreamingOutput.save_to_partition","title":"<code>save_to_partition(value, partition)</code>","text":"<p>\ud83c\udfaf Send a message to a specific partition in the Kafka topic.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The value of the message.</p> required <code>partition</code> <code>int</code> <p>The partition to send the message to.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If no Kafka producer is available or an error occurs.</p>"},{"location":"core/core_data_streaming_output/#core.data.streaming_output.StreamingOutput.send_key_value","title":"<code>send_key_value(key, value)</code>","text":"<p>\ud83d\udd11 Send a message with a key to the Kafka topic.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Any</code> <p>The key of the message.</p> required <code>value</code> <code>Any</code> <p>The value of the message.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If no Kafka producer is available or an error occurs.</p>"},{"location":"core/core_spout/","title":"Spout","text":"<p>Core Spout class</p>"},{"location":"core/core_spout/#core.spout.Spout","title":"<code>Spout</code>","text":"<p>             Bases: <code>Task</code></p> <p>Base class for all spouts.</p>"},{"location":"core/core_spout/#core.spout.Spout.__call__","title":"<code>__call__(method_name, *args, **kwargs)</code>","text":"<p>Execute a method locally and manage the state.</p> <p>Parameters:</p> Name Type Description Default <code>method_name</code> <code>str</code> <p>The name of the method to execute.</p> required <code>*args</code> <p>Positional arguments to pass to the method.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments to pass to the method. Keyword Arguments:     - Additional keyword arguments specific to the method.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The result of the method.</p>"},{"location":"core/core_spout/#core.spout.Spout.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>The <code>Spout</code> class is a base class for all spouts in the given context. It inherits from the <code>Task</code> class and provides methods for executing tasks both locally and remotely, as well as managing their state, with state management options including in-memory, Redis, PostgreSQL, and DynamoDB, and output configurations for batch or streaming data.</p> <p>The <code>Spout</code> class uses the <code>Output</code> and <code>State</code> classes, which are abstract base  classes for managing output configurations and states, respectively. The <code>Output</code> class  has two subclasses: <code>StreamingOutput</code> and <code>BatchOutput</code>, which manage streaming and  batch output configurations, respectively. The <code>State</code> class is used to get and set state,  and it has several subclasses for different types of state managers.</p> <p>The <code>Spout</code> class also uses the <code>ECSManager</code> and <code>K8sManager</code> classes in the <code>execute_remote</code> method, which are used to manage tasks on Amazon ECS and Kubernetes, respectively.</p> Usage <ul> <li>Create an instance of the Spout class by providing an Output object and a State object.</li> <li>The Output object specifies the output configuration for the spout.</li> <li>The State object handles the management of the spout's state.</li> </ul> Example <p>output = Output(...) state = State(...) spout = Spout(output, state)</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Output</code> <p>The output configuration.</p> required <code>state</code> <code>State</code> <p>The state manager.</p> required"},{"location":"core/core_spout/#core.spout.Spout.create","title":"<code>create(klass, output_type, state_type, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a spout of a specific type.</p> <p>Parameters:</p> Name Type Description Default <code>klass</code> <code>type</code> <p>The Spout class to create.</p> required <code>output_type</code> <code>str</code> <p>The type of output config (\"batch\" or \"streaming\").</p> required <code>state_type</code> <code>str</code> <p>The type of state manager (\"in_memory\", \"redis\", \"postgres\", or \"dynamodb\").</p> required <code>**kwargs</code> <p>Additional keyword arguments for initializing the spout. <pre><code>Keyword Arguments:\n    Batch output config:\n    - output_folder (str): The directory where output files should be stored temporarily.\n    - output_s3_bucket (str): The name of the S3 bucket for output storage.\n    - output_s3_folder (str): The S3 folder for output storage.\n    Streaming output config:\n    - output_kafka_topic (str): Kafka output topic for streaming spouts.\n    - output_kafka_cluster_connection_string (str): Kafka connection string for streaming spouts.\n    Redis state manager config:\n    - redis_host (str): The host address for the Redis server.\n    - redis_port (int): The port number for the Redis server.\n    - redis_db (int): The Redis database to be used.\n    Postgres state manager config:\n    - postgres_host (str): The host address for the PostgreSQL server.\n    - postgres_port (int): The port number for the PostgreSQL server.\n    - postgres_user (str): The username for the PostgreSQL server.\n    - postgres_password (str): The password for the PostgreSQL server.\n    - postgres_database (str): The PostgreSQL database to be used.\n    - postgres_table (str): The PostgreSQL table to be used.\n    DynamoDB state manager config:\n    - dynamodb_table_name (str): The name of the DynamoDB table.\n    - dynamodb_region_name (str): The AWS region for DynamoDB.\n</code></pre></p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Spout</code> <code>Spout</code> <p>The created spout.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid output type or state type is provided.</p>"},{"location":"core/core_state_base/","title":"State","text":"<p>Base class for task state mnager</p>"},{"location":"core/core_state_base/#core.state.base.State","title":"<code>State</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for a state manager.</p> <p>A state manager is responsible for getting and setting state.</p>"},{"location":"core/core_state_base/#core.state.base.State.get_state","title":"<code>get_state(key)</code>  <code>abstractmethod</code>","text":"<p>Get the state associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to get the state for.</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>Optional[Dict]</code> <p>The state associated with the key.</p>"},{"location":"core/core_state_base/#core.state.base.State.set_state","title":"<code>set_state(key, value)</code>  <code>abstractmethod</code>","text":"<p>Set the state associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to set the state for.</p> required <code>value</code> <code>Dict</code> <p>The state to set.</p> required"},{"location":"core/core_state_dynamo/","title":"DynamoDB State","text":"<p>State manager using dynamoDB</p>"},{"location":"core/core_state_dynamo/#core.state.dynamo.DynamoDBState","title":"<code>DynamoDBState</code>","text":"<p>             Bases: <code>State</code></p> <p>\ud83d\uddc4\ufe0f DynamoDBState: A state manager that stores state in DynamoDB.</p> <p>Attributes:</p> Name Type Description <code>dynamodb</code> <code>boto3.resources.factory.dynamodb.ServiceResource</code> <p>The DynamoDB service resource.</p> <code>table</code> <code>boto3.resources.factory.dynamodb.Table</code> <p>The DynamoDB table.</p> <p>Usage: <pre><code>manager = DynamoDBState(\"my_table\", \"us-west-1\")\nmanager.set_state(\"key123\", {\"status\": \"active\"})\nstate = manager.get_state(\"key123\")\nprint(state)  # Outputs: {\"status\": \"active\"}\n</code></pre></p> <p>Note: - Ensure DynamoDB is accessible and the table exists.</p>"},{"location":"core/core_state_dynamo/#core.state.dynamo.DynamoDBState.__init__","title":"<code>__init__(table_name, region_name)</code>","text":"<p>Initialize a new DynamoDB state manager.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>The name of the DynamoDB table.</p> required <code>region_name</code> <code>str</code> <p>The name of the AWS region.</p> required"},{"location":"core/core_state_dynamo/#core.state.dynamo.DynamoDBState.get_state","title":"<code>get_state(key)</code>","text":"<p>\ud83d\udcd6 Get the state associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to get the state for.</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>Optional[Dict]</code> <p>The state associated with the key, or None if not found.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there's an error accessing DynamoDB.</p>"},{"location":"core/core_state_dynamo/#core.state.dynamo.DynamoDBState.set_state","title":"<code>set_state(key, value)</code>","text":"<p>\ud83d\udcdd Set the state associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to set the state for.</p> required <code>value</code> <code>Dict</code> <p>The state to set.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If there's an error accessing DynamoDB.</p>"},{"location":"core/core_state_memory/","title":"In-memory State","text":"<p>State manager using local memory</p>"},{"location":"core/core_state_memory/#core.state.memory.InMemoryState","title":"<code>InMemoryState</code>","text":"<p>             Bases: <code>State</code></p> <p>\ud83e\udde0 InMemoryState: A state manager that stores state in memory.</p> <p>This manager is useful for temporary storage or testing purposes. Since it's in-memory, the data will be lost once the application stops.</p>"},{"location":"core/core_state_memory/#core.state.memory.InMemoryState--attributes","title":"Attributes:","text":"<ul> <li><code>store</code> (Dict[str, Dict]): The in-memory store for states.</li> </ul>"},{"location":"core/core_state_memory/#core.state.memory.InMemoryState--usage","title":"Usage:","text":"<pre><code>manager = InMemoryState()\nmanager.set_state(\"user123\", {\"status\": \"active\"})\nstate = manager.get_state(\"user123\")\nprint(state)  # Outputs: {\"status\": \"active\"}\n</code></pre> <p>!!! warning     Remember, this is an in-memory store. Do not use it for persistent storage!</p>"},{"location":"core/core_state_memory/#core.state.memory.InMemoryState.__init__","title":"<code>__init__()</code>","text":"<p>Initialize a new in-memory state manager.</p>"},{"location":"core/core_state_memory/#core.state.memory.InMemoryState.get_state","title":"<code>get_state(key)</code>","text":"<p>\ud83d\udcd6 Get the state associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to get the state for.</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>Optional[Dict]</code> <p>The state associated with the key, or None if not found.</p>"},{"location":"core/core_state_memory/#core.state.memory.InMemoryState.set_state","title":"<code>set_state(key, value)</code>","text":"<p>\ud83d\udcdd Set the state associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to set the state for.</p> required <code>value</code> <code>Dict</code> <p>The state to set.</p> required <p>Example: <pre><code>manager.set_state(\"user123\", {\"status\": \"active\"})\n</code></pre></p>"},{"location":"core/core_state_postgres/","title":"Postgres State","text":"<p>State manager using postgres database</p>"},{"location":"core/core_state_postgres/#core.state.postgres.PostgresState","title":"<code>PostgresState</code>","text":"<p>             Bases: <code>State</code></p> <p>\ud83d\uddc4\ufe0f PostgresState: A state manager that stores state in a PostgreSQL database.</p> <p>This manager provides a persistent storage solution using a PostgreSQL database.</p>"},{"location":"core/core_state_postgres/#core.state.postgres.PostgresState--attributes","title":"Attributes:","text":"<ul> <li><code>conn</code> (psycopg2.extensions.connection): The PostgreSQL connection.</li> </ul>"},{"location":"core/core_state_postgres/#core.state.postgres.PostgresState--usage","title":"Usage:","text":"<pre><code>manager = PostgresState(host=\"localhost\", port=5432, user=\"admin\", password=\"password\", database=\"mydb\")\nmanager.set_state(\"user123\", {\"status\": \"active\"})\nstate = manager.get_state(\"user123\")\nprint(state)  # Outputs: {\"status\": \"active\"}\n</code></pre> <p>!!! warning     Ensure PostgreSQL is accessible and the table exists.</p>"},{"location":"core/core_state_postgres/#core.state.postgres.PostgresState.__init__","title":"<code>__init__(host, port, user, password, database, table='geniusrise_state')</code>","text":"<p>Initialize a new PostgreSQL state manager.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The host of the PostgreSQL server.</p> required <code>port</code> <code>int</code> <p>The port of the PostgreSQL server.</p> required <code>user</code> <code>str</code> <p>The user to connect as.</p> required <code>password</code> <code>str</code> <p>The user's password.</p> required <code>database</code> <code>str</code> <p>The database to connect to.</p> required <code>table</code> <code>str</code> <p>The table to use. Defaults to \"geniusrise_state\".</p> <code>'geniusrise_state'</code>"},{"location":"core/core_state_postgres/#core.state.postgres.PostgresState.get_state","title":"<code>get_state(key)</code>","text":"<p>\ud83d\udcd6 Get the state associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to get the state for.</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>Optional[Dict]</code> <p>The state associated with the key, or None if not found.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there's an error accessing PostgreSQL.</p>"},{"location":"core/core_state_postgres/#core.state.postgres.PostgresState.set_state","title":"<code>set_state(key, value)</code>","text":"<p>\ud83d\udcdd Set the state associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to set the state for.</p> required <code>value</code> <code>Dict</code> <p>The state to set.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If there's an error accessing PostgreSQL.</p>"},{"location":"core/core_state_redis/","title":"Redis State","text":"<p>State manager using redis</p>"},{"location":"core/core_state_redis/#core.state.redis.RedisState","title":"<code>RedisState</code>","text":"<p>             Bases: <code>State</code></p> <p>\ud83d\uddc4\ufe0f RedisState: A state manager that stores state in Redis.</p> <p>This manager provides a fast, in-memory storage solution using Redis.</p>"},{"location":"core/core_state_redis/#core.state.redis.RedisState--attributes","title":"Attributes:","text":"<ul> <li><code>redis</code> (redis.Redis): The Redis connection.</li> </ul>"},{"location":"core/core_state_redis/#core.state.redis.RedisState--usage","title":"Usage:","text":"<pre><code>manager = RedisState(host=\"localhost\", port=6379, db=0)\nmanager.set_state(\"user123\", {\"status\": \"active\"})\nstate = manager.get_state(\"user123\")\nprint(state)  # Outputs: {\"status\": \"active\"}\n</code></pre> <p>!!! warning     Ensure Redis is accessible and running.</p>"},{"location":"core/core_state_redis/#core.state.redis.RedisState.__init__","title":"<code>__init__(host, port, db)</code>","text":"<p>Initialize a new Redis state manager.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The host of the Redis server.</p> required <code>port</code> <code>int</code> <p>The port of the Redis server.</p> required <code>db</code> <code>int</code> <p>The database number to connect to.</p> required"},{"location":"core/core_state_redis/#core.state.redis.RedisState.get_state","title":"<code>get_state(key)</code>","text":"<p>\ud83d\udcd6 Get the state associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to get the state for.</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>Optional[Dict]</code> <p>The state associated with the key, or None if not found.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there's an error accessing Redis.</p>"},{"location":"core/core_state_redis/#core.state.redis.RedisState.set_state","title":"<code>set_state(key, value)</code>","text":"<p>\ud83d\udcdd Set the state associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to set the state for.</p> required <code>value</code> <code>Dict</code> <p>The state to set.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If there's an error accessing Redis.</p>"},{"location":"core/core_task_base/","title":"Task","text":"<p>Base class for Task</p>"},{"location":"core/core_task_base/#core.task.base.Task","title":"<code>Task</code>","text":"<p>             Bases: <code>ABC</code></p> <p>\ud83d\udee0\ufe0f Task: Class for managing tasks.</p> <p>This class provides a foundation for creating and managing tasks. Each task has a unique identifier and can be associated with specific input and output configurations.</p>"},{"location":"core/core_task_base/#core.task.base.Task--attributes","title":"Attributes:","text":"<ul> <li><code>id</code> (uuid.UUID): Unique identifier for the task.</li> <li><code>input</code> (Input): Configuration for input data.</li> <li><code>output</code> (Output): Configuration for output data.</li> </ul>"},{"location":"core/core_task_base/#core.task.base.Task--usage","title":"Usage:","text":"<pre><code>task = Task()\ntask.execute(\"fetch_data\")\n</code></pre> <p>!!! note     Extend this class to implement specific task functionalities.</p>"},{"location":"core/core_task_base/#core.task.base.Task.__init__","title":"<code>__init__()</code>","text":"<p>Initialize a new task.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Input</code> <p>Configuration for input data.</p> required <code>output</code> <code>Output</code> <p>Configuration for output data.</p> required"},{"location":"core/core_task_base/#core.task.base.Task.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a string representation of the task.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string representation of the task.</p>"},{"location":"core/core_task_base/#core.task.base.Task.execute","title":"<code>execute(method_name, *args, **kwargs)</code>","text":"<p>\ud83d\ude80 Execute a given fetch_* method if it exists.</p> <p>Parameters:</p> Name Type Description Default <code>method_name</code> <code>str</code> <p>The name of the fetch_* method to execute.</p> required <code>*args</code> <p>Positional arguments to pass to the method.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments to pass to the method.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The result of the fetch_* method, or None if the method does not exist.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If the specified method doesn't exist.</p>"},{"location":"core/core_task_base/#core.task.base.Task.get_methods","title":"<code>get_methods()</code>  <code>staticmethod</code>","text":"<p>\ud83d\udcdc Get all the fetch_* methods and their parameters along with their default values and docstrings.</p> <p>Returns:</p> Type Description <code>List[Tuple[str, List[str], Optional[str]]]</code> <p>List[Tuple[str, List[str], str]]: A list of tuples, where each tuple contains the name of a fetch_* method,</p> <code>List[Tuple[str, List[str], Optional[str]]]</code> <p>a list of its parameters along with their default values, and its docstring.</p>"},{"location":"core/core_task_base/#core.task.base.Task.print_help","title":"<code>print_help()</code>  <code>staticmethod</code>","text":"<p>\ud83d\udda8\ufe0f Pretty print the fetch_* methods and their parameters along with their default values and docstrings. Also prints the class's docstring and init parameters.</p>"},{"location":"core/core_task_ecs/","title":"ECS Runner","text":""},{"location":"core/core_task_ecs/#core.task.ecs.ECSManager","title":"<code>ECSManager</code>","text":"<p>A class used to manage the lifecycle of an ECS container.</p> <p>...</p>"},{"location":"core/core_task_ecs/#core.task.ecs.ECSManager--attributes","title":"Attributes","text":"str <p>the name of the ECS task or service</p> List[str] <p>the command that the container runs</p> str <p>the name of the ECS cluster</p> List[str] <p>the subnet IDs for the task or service</p> List[str] <p>the security group IDs for the task or service</p> str <p>the Docker image for the task</p> int <p>the number of task replicas</p> int <p>the port that the container listens on</p> str <p>the CloudWatch log group for the task logs</p> int <p>the CPU value for the task</p> int <p>the memory value for the task</p>"},{"location":"core/core_task_ecs/#core.task.ecs.ECSManager--methods","title":"Methods","text":"<p>create_task_definition()     Registers a new task definition from the attributes of this class run_task(task_definition_arn: str)     Runs a new task using the specified task definition ARN describe_task(task_definition_arn: str)     Describes a task using the specified task definition ARN stop_task(task_definition_arn: str)     Stops a running task using the specified task definition ARN update_task(new_image: str, new_command: list)     Updates a task with a new Docker image and command create_service(task_definition_arn: str)     Creates a new service using the specified task definition ARN update_service(task_definition_arn: str)     Updates a service with a new task definition ARN delete_service()     Deletes the service</p>"},{"location":"core/core_task_ecs/#core.task.ecs.ECSManager.__init__","title":"<code>__init__(name, account_id, cluster, command=[], subnet_ids=[], security_group_ids=[], image='geniusrise/geniusrise', replicas=1, port=80, log_group='/ecs/geniusrise', cpu=256, memory=512)</code>","text":"<p>Constructs all the necessary attributes for the ECSManager object.</p>"},{"location":"core/core_task_ecs/#core.task.ecs.ECSManager.__init__--parameters","title":"Parameters","text":"<pre><code>name : str\n    the name of the ECS task or service\naccount_id : str\n    the id of the AWS account\ncommand : List[str]\n    the command that the container runs\ncluster : str\n    the name of the ECS cluster\nsubnet_ids : List[str]\n    the subnet IDs for the task or service\nsecurity_group_ids : List[str]\n    the security group IDs for the task or service\nimage : str, optional\n    the Docker image for the task (default is \"geniusrise/geniusrise\")\nreplicas : int, optional\n    the number of task replicas (default is 1)\nport : int, optional\n    the port that the container listens on (default is 80)\nlog_group : str, optional\n    the CloudWatch log group for the task logs (default is \"/ecs/geniusrise\")\ncpu : int, optional\n    the CPU value for the task (default is 256)\nmemory : int, optional\n    the memory value for the task (default is 512)\n</code></pre>"},{"location":"core/core_task_ecs/#core.task.ecs.ECSManager.create_service","title":"<code>create_service(task_definition_arn)</code>","text":"<p>Creates a new service using the specified task definition ARN.</p>"},{"location":"core/core_task_ecs/#core.task.ecs.ECSManager.create_service--parameters","title":"Parameters","text":"str <p>The ARN of the task definition to use for the service.</p>"},{"location":"core/core_task_ecs/#core.task.ecs.ECSManager.create_service--returns","title":"Returns","text":"<p>dict     The response from the ECS API, or None if an error occurred.</p>"},{"location":"core/core_task_ecs/#core.task.ecs.ECSManager.create_task_definition","title":"<code>create_task_definition()</code>","text":"<p>Registers a new task definition from the attributes of this class.</p>"},{"location":"core/core_task_ecs/#core.task.ecs.ECSManager.create_task_definition--returns","title":"Returns","text":"<p>str     The ARN of the task definition, or None if an error occurred.</p>"},{"location":"core/core_task_ecs/#core.task.ecs.ECSManager.delete_service","title":"<code>delete_service()</code>","text":"<p>Deletes the service.</p>"},{"location":"core/core_task_ecs/#core.task.ecs.ECSManager.delete_service--returns","title":"Returns","text":"<p>dict     The response from the ECS API, or None if an error occurred.</p>"},{"location":"core/core_task_ecs/#core.task.ecs.ECSManager.describe_task","title":"<code>describe_task(task_definition_arn)</code>","text":"<p>Describes a task using the specified task definition ARN.</p>"},{"location":"core/core_task_ecs/#core.task.ecs.ECSManager.describe_task--parameters","title":"Parameters","text":"str <p>The ARN of the task definition to describe.</p>"},{"location":"core/core_task_ecs/#core.task.ecs.ECSManager.describe_task--returns","title":"Returns","text":"<p>dict     The response from the ECS API, or None if an error occurred.</p>"},{"location":"core/core_task_ecs/#core.task.ecs.ECSManager.run_task","title":"<code>run_task(task_definition_arn)</code>","text":"<p>Runs a new task using the specified task definition ARN.</p>"},{"location":"core/core_task_ecs/#core.task.ecs.ECSManager.run_task--parameters","title":"Parameters","text":"str <p>The ARN of the task definition to run.</p>"},{"location":"core/core_task_ecs/#core.task.ecs.ECSManager.run_task--returns","title":"Returns","text":"<p>dict     The response from the ECS API, or None if an error occurred.</p>"},{"location":"core/core_task_ecs/#core.task.ecs.ECSManager.stop_task","title":"<code>stop_task(task_definition_arn)</code>","text":"<p>Stops a running task using the specified task definition ARN.</p>"},{"location":"core/core_task_ecs/#core.task.ecs.ECSManager.stop_task--parameters","title":"Parameters","text":"str <p>The ARN of the task definition to stop.</p>"},{"location":"core/core_task_ecs/#core.task.ecs.ECSManager.stop_task--returns","title":"Returns","text":"<p>dict     The response from the ECS API, or None if an error occurred.</p>"},{"location":"core/core_task_ecs/#core.task.ecs.ECSManager.update_service","title":"<code>update_service(task_definition_arn)</code>","text":"<p>Updates a service with a new task definition ARN.</p>"},{"location":"core/core_task_ecs/#core.task.ecs.ECSManager.update_service--parameters","title":"Parameters","text":"str <p>The new ARN of the task definition to use for the service.</p>"},{"location":"core/core_task_ecs/#core.task.ecs.ECSManager.update_service--returns","title":"Returns","text":"<p>dict     The response from the ECS API, or None if an error occurred.</p>"},{"location":"core/core_task_ecs/#core.task.ecs.ECSManager.update_task","title":"<code>update_task(new_image, new_command)</code>","text":"<p>Updates a task with a new Docker image and command.</p>"},{"location":"core/core_task_ecs/#core.task.ecs.ECSManager.update_task--parameters","title":"Parameters","text":"str <p>The new Docker image for the task.</p> list <p>The new command for the task.</p>"},{"location":"core/core_task_k8s/","title":"Kubernetes Runner","text":""},{"location":"core/core_task_k8s/#core.task.k8s.K8sManager","title":"<code>K8sManager</code>","text":"<p>A class used to manage Kubernetes deployments and services.</p>"},{"location":"core/core_task_k8s/#core.task.k8s.K8sManager--attributes","title":"Attributes","text":"str <p>The name of the deployment and service.</p> str <p>The namespace to create the deployment and service in.</p> str <p>The Docker image to use for the deployment.</p> list <p>The command to run in the Docker container.</p> int <p>The number of replicas to create for the deployment.</p> int <p>The port to expose on the service.</p>"},{"location":"core/core_task_k8s/#core.task.k8s.K8sManager--methods","title":"Methods","text":"<p>create_deployment()     Creates a new deployment. update_deployment(replicas)     Updates the number of replicas in the deployment. scale_deployment(replicas)     Scales the deployment to a new number of replicas. delete_deployment()     Deletes the deployment. create_service()     Creates a new service. delete_service()     Deletes the service. run()     Creates the deployment and service. destroy()     Deletes the deployment and service. get_status()     Returns the status of the deployment. get_statistics()     Returns the details of the deployment and the pods in the deployment. get_logs()     Returns the logs of the pods in the deployment.</p>"},{"location":"core/core_task_k8s/#core.task.k8s.K8sManager.__init__","title":"<code>__init__(name, command=[], namespace='default', image='geniusrise/geniusrise', replicas=1, port=80)</code>","text":"<p>Constructs all the necessary attributes for the K8sManager object.</p>"},{"location":"core/core_task_k8s/#core.task.k8s.K8sManager.__init__--parameters","title":"Parameters","text":"str <p>The name of the deployment and service.</p> list <p>The command to run in the Docker container.</p> str, optional <p>The namespace to create the deployment and service in (default is \"default\").</p> str, optional <p>The Docker image to use for the deployment (default is \"geniusrise/geniusrise\").</p> int, optional <p>The number of replicas to create for the deployment (default is 1).</p> int, optional <p>The port to expose on the service (default is 80).</p>"},{"location":"core/core_task_k8s/#core.task.k8s.K8sManager.create_deployment","title":"<code>create_deployment()</code>","text":"<p>Creates a new deployment.</p> <p>The deployment is created in the namespace specified in the constructor. The deployment uses the Docker image and command specified in the constructor, and creates the number of replicas specified in the constructor.</p> <p>If an error occurs while creating the deployment, an error message is logged and the method returns None.</p>"},{"location":"core/core_task_k8s/#core.task.k8s.K8sManager.create_service","title":"<code>create_service()</code>","text":"<p>Deletes the deployment.</p> <p>If an error occurs while deleting the deployment, an error message is logged and the method returns None.</p>"},{"location":"core/core_task_k8s/#core.task.k8s.K8sManager.delete_service","title":"<code>delete_service()</code>","text":"<p>Creates a new service.</p> <p>The service is created in the namespace specified in the constructor. The service exposes the port specified in the constructor.</p> <p>If an error occurs while creating the service, an error message is logged and the method returns None.</p>"},{"location":"core/core_task_k8s/#core.task.k8s.K8sManager.destroy","title":"<code>destroy()</code>","text":"<p>Deletes the deployment and service.</p> <p>If an error occurs while deleting the deployment or service, an error message is logged and the method returns None.</p>"},{"location":"core/core_task_k8s/#core.task.k8s.K8sManager.get_logs","title":"<code>get_logs()</code>","text":"<p>Get the logs of the pods in the deployment</p> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict[str, str]: The logs of the pods</p>"},{"location":"core/core_task_k8s/#core.task.k8s.K8sManager.get_statistics","title":"<code>get_statistics()</code>","text":"<p>Get the details of the deployment and the pods in the deployment</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: The details of the deployment and the pods</p>"},{"location":"core/core_task_k8s/#core.task.k8s.K8sManager.get_status","title":"<code>get_status()</code>","text":"<p>Get the status of the deployment</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: The status of the deployment</p>"},{"location":"core/core_task_k8s/#core.task.k8s.K8sManager.run","title":"<code>run()</code>","text":"<p>Creates the deployment and service.</p> <p>If an error occurs while creating the deployment or service, an error message is logged and the method returns None.</p>"},{"location":"core/core_task_k8s/#core.task.k8s.K8sManager.scale_deployment","title":"<code>scale_deployment(replicas)</code>","text":"<p>Scales the deployment to a new number of replicas.</p>"},{"location":"core/core_task_k8s/#core.task.k8s.K8sManager.scale_deployment--parameters","title":"Parameters","text":"int <p>The new number of replicas for the deployment.</p> <p>If an error occurs while scaling the deployment, an error message is logged and the method returns None.</p>"},{"location":"core/core_task_k8s/#core.task.k8s.K8sManager.update_deployment","title":"<code>update_deployment(replicas)</code>","text":"<p>Updates the number of replicas in the deployment.</p>"},{"location":"core/core_task_k8s/#core.task.k8s.K8sManager.update_deployment--parameters","title":"Parameters","text":"int <p>The new number of replicas for the deployment.</p> <p>If an error occurs while updating the deployment, an error message is logged and the method returns None.</p>"},{"location":"core/logging/","title":"Logging Configuration","text":""},{"location":"core/logging/#logging.setup_logger","title":"<code>setup_logger()</code>","text":"<p>\ud83d\udee0\ufe0f Setup Logger: Configure and return a logger with a default ColoredFormatter.</p> <p>This function sets up a logger for the <code>geniusrise-cli</code> with colorful logging outputs. The log level is determined by the <code>LOGLEVEL</code> from the configuration.</p>"},{"location":"core/logging/#logging.setup_logger--usage","title":"Usage:","text":"<pre><code>logger = setup_logger()\nlogger.info(\"This is a fancy info log!\")\n</code></pre> <p>Returns:</p> Type Description <code>logging.Logger</code> <p>logging.Logger: Configured logger with colorful outputs.</p>"},{"location":"guides/architecture/","title":"Architecture","text":""},{"location":"guides/architecture/#introduction","title":"Introduction","text":"<p>The Geniusrise framework is designed to provide a modular, scalable, and interoperable system for orchestrating machine learning workflows, particularly in the context of Large Language Models (LLMs). The architecture is built around the core concept of a <code>Task</code>, which represents a discrete unit of work. This document provides an overview of the architecture, detailing the primary components and their interactions.</p>"},{"location":"guides/architecture/#system-overview","title":"System Overview","text":"<p>The Geniusrise framework is composed of several key components:</p> <ol> <li>Tasks: The fundamental units of work.</li> <li>State Managers: Responsible for monitoring and managing the state of tasks.</li> <li>Data Managers: Oversee the input and output data associated with tasks.</li> <li>Model Managers: Handle model operations, ensuring efficient management.</li> <li>Runners: Wrappers for executing tasks on various platforms.</li> <li>Spouts and Bolts: Specialized tasks for data ingestion and processing.</li> </ol>"},{"location":"guides/architecture/#tasks","title":"Tasks","text":"<p>A task is the fundamental unit of work in the Geniusrise framework. It represents a specific operation or computation and can run for an arbitrary amount of time, performing any amount of work.</p> <pre>a9bd4816-b70a-4fc1-aafc-279a357e6ecc</pre>"},{"location":"guides/architecture/#state-managers","title":"State Managers","text":"<p>State Managers play a pivotal role in maintaining the state of tasks. They ensure that the progress and status of tasks are tracked, especially in distributed environments. Geniusrise offers various types of State Managers:</p> <ol> <li>DynamoDBStateManager: Interfaces with Amazon DynamoDB.</li> <li>InMemoryStateManager: Maintains state within the application's memory.</li> <li>PostgresStateManager: Interfaces with PostgreSQL databases.</li> <li>RedisStateManager: Interfaces with Redis in-memory data structure store.</li> </ol> <p>State Managers store data in various locations, allowing organizations to connect dashboards to these storage systems for real-time monitoring and analytics. This centralized storage and reporting mechanism ensures that stakeholders have a unified view of task states.</p> <pre>ff197cc9-e01b-4727-ba91-de9cb6df0bad</pre>"},{"location":"guides/architecture/#data-managers","title":"Data Managers","text":"<p>Data Managers are responsible for handling the input and output data for tasks. They implement various data operations methods that tasks can leverage to ingest or save data during their runs. Data Managers can be categorized based on their function and data processing type:</p> <ol> <li>BatchInputConfig: Manages batch input data.</li> <li>BatchOutputConfig: Manages batch output data.</li> <li>StreamingInputConfig: Manages streaming input data.</li> <li>StreamingOutputConfig: Manages streaming output data.</li> </ol> <p>Data Managers manage data partitioning for both batch and streaming data. By adhering to common data patterns, they enable the system's components to operate independently, fostering the creation of intricate networks of tasks. This independence, while allowing for flexibility and scalability, ensures that cascading failures in one component don't necessarily compromise the entire system.</p> <pre>81daf310-59ee-4509-ae7f-d2b8a2a9bdba</pre>"},{"location":"guides/architecture/#model-managers","title":"Model Managers","text":"<p>Model Managers oversee model operations, ensuring that models are saved, loaded, and managed. They can be of two primary types:</p> <ol> <li>S3ModelManager: Interfaces with Amazon S3 for model storage.</li> <li>WANDBModelManager: Interfaces with Weights &amp; Biases for model versioning.</li> <li>GitModelManager: Interfaces with Git repositories for versioning of models.</li> </ol> <pre>ab055385-459c-487f-ad84-7b76c3752e4c</pre>"},{"location":"guides/architecture/#spouts-and-bolts","title":"Spouts and Bolts","text":"<p>At the heart of the Geniusrise framework are two primary component types: spouts and bolts.</p> <ol> <li> <p>Spouts: These are tasks responsible for ingesting data from various sources. Depending on the output type, spouts can either produce streaming output or batch output.</p> <ol> <li>Batch: Runs periodically, Produces data as a batch output.</li> <li>Stream: Runs forever, produces data into a streaming output.</li> </ol> </li> <li> <p>Bolts: Bolts are tasks that take in data, process it, and produce output. They can be categorized based on their input and output types:</p> <ol> <li>Stream-Stream: Reads streaming data and produces streaming output.</li> <li>Stream-Batch: Reads streaming data and produces batch output.</li> <li>Batch-Stream: Reads batch data and produces streaming output.</li> <li>Batch-Batch: Reads batch data and produces batch output.</li> </ol> </li> </ol> <pre>64a686d5-8346-4ba4-8c55-dc0be00c0165</pre>"},{"location":"guides/architecture/#runners","title":"Runners","text":"<p>Runners are the backbone of the Geniusrise framework, ensuring that tasks are executed seamlessly across various platforms. They encapsulate the environment and resources required for task execution, abstracting away the underlying complexities. Geniusrise offers the following runners:</p> <ol> <li>Local Runner: Executes tasks directly on a local machine, ideal for development and testing.</li> <li>Docker Runner: Runs tasks within Docker containers, ensuring a consistent and isolated environment.</li> <li>Kubernetes Runner: Deploys tasks on Kubernetes clusters, leveraging its scalability and orchestration capabilities.</li> <li>Airflow Runner: Integrates with Apache Airflow, allowing for complex workflow orchestration and scheduling.</li> <li>ECS Runner: Executes tasks on AWS ECS, providing a managed container service.</li> <li>Batch Runner: Optimized for batch computing workloads on platforms like AWS Batch.</li> </ol>"},{"location":"guides/concepts/","title":"Concepts","text":""},{"location":"guides/concepts/#need","title":"Need","text":"<p>The landscape of machine learning and data processing has been rapidly evolving. While there are numerous solutions available for MLOps and DAG orchestration, most of them cater primarily to data engineering and data science teams. However, the rise of Large Language Models (LLMs) is reshaping this landscape, necessitating a more inclusive approach to MLOps.</p> <p>LLMs have democratized the use of machine learning models, enabling a broader spectrum of users within an organization to engage with them. This means that even organizations without a traditional data science data management or model management functions are now venturing into this domain.</p> <p>This shift brings forth several challenges:</p> <ol> <li> <p>Infrastructure Complexity: The world of MLOps is vast, with a plethora of options available for different use cases. Organizations often grapple with questions like:</p> <ul> <li>Which infrastructure is best suited for their needs?</li> <li>How can they efficiently reuse their existing infrastructure?</li> <li>How can they ensure scalability and performance while managing costs?</li> </ul> </li> <li> <p>Diverse Competence Levels: As LLM workflows become more prevalent, their volume is set to surpass that of traditional ML workflows in many organizations. This surge means that individuals without a formal engineering background or core ML expertise will be involved in the MLOps process. Ensuring that these individuals can contribute effectively without compromising the quality or integrity of the workflows is crucial.</p> </li> <li> <p>Standardization and Productionization: While many aspects of building LLM workflows can be managed without deep engineering expertise, there's a critical need for standardization. Engineers play a pivotal role in defining the processes for productionizing these workflows. Without standardized practices:</p> <ul> <li>How can organizations ensure consistency across workflows?</li> <li>How can they maintain the reliability and robustness of deployed models?</li> <li>How can they ensure that best practices are adhered to, regardless of who is building or deploying the workflow?</li> </ul> </li> </ol> <p>The advent of LLMs underscores the need for an MLOps framework that caters to a diverse audience. Such a framework should be flexible enough to accommodate the varied infrastructure needs of organizations, inclusive enough to empower contributors regardless of their technical expertise, and robust enough to ensure standardized, reliable workflows.</p>"},{"location":"guides/concepts/#introduction","title":"Introduction","text":"<p>The Geniusrise framework is built around loosely-coupled modules acting as a cohesive adhesive between distinct, modular components, much like how one would piece together Lego blocks. This design approach not only promotes flexibility but also ensures that each module or \"Lego block\" remains sufficiently independent. Such independence is crucial for diverse teams, each with its own unique infrastructure and requirements, to seamlessly build and manage their respective components.</p> <p>Geniusrise comes with a sizable set of plugins which implement various features and integrations. The independence and modularity of the design enable sharing of these building blocks in the community.</p>"},{"location":"guides/concepts/#concepts_1","title":"Concepts","text":"<ol> <li> <p>Task: At its core, a task represents a discrete unit of work within the Geniusrise framework. Think of it as a singular action or operation that the system needs to execute. A task further manifests itself into a Bolt or a Spout as stated below.</p> </li> <li> <p>Components of a Task: Each task is equipped with four components:</p> <ol> <li>State Manager: This component is responsible for continuously monitoring and managing the task's state, ensuring that it progresses smoothly from initiation to completion and to report errors and ship logs into a central location.</li> <li>Data Manager: As the name suggests, the Data Manager oversees the input and output data associated with a task, ensuring data integrity and efficient data flow. It also ensures data sanity follows partition semantics and isolation.</li> <li>Model Manager: In the realm of machine learning, model versioning and management are paramount. The Model Manager serves as a GitOps tool for ML models, ensuring that they are versioned, tracked, and managed effectively.</li> <li>Runner: These are wrappers for executing a task on various platforms. Depending on the platform, the runner ensures that the task is executed seamlessly.</li> </ol> </li> <li> <p>Task Classification: Tasks within the Geniusrise framework can be broadly classified into two categories:</p> <ul> <li>Spout: If a task's primary function is to ingest or bring in data, it's termed as a 'spout'.</li> <li>Bolt: For tasks that don't primarily ingest data but perform other operations, they are termed 'bolts'.</li> </ul> </li> </ol> <p>The beauty of the Geniusrise framework lies in its adaptability. Developers can script their workflow components once and have the freedom to deploy them across various platforms. To facilitate this, Geniusrise offers:</p> <ol> <li> <p>Runners for Task Execution: Geniusrise is equipped with a diverse set of runners, each tailored for different platforms, ensuring that tasks can be executed almost anywhere:</p> <ol> <li>On your local machine for quick testing and development.</li> <li>Within Docker containers for isolated, reproducible environments.</li> <li>On Kubernetes clusters for scalable, cloud-native deployments.</li> <li>Using Apache Airflow for complex workflow orchestration.</li> <li>On AWS ECS for containerized application management.</li> <li>With AWS Batch for efficient batch computing workloads.</li> </ol> </li> <li> <p>Library Wrappers: To ensure that tasks can interface with a variety of frameworks, Geniusrise provides integrations with:</p> <ol> <li>Jupyter/ipython for interactive computing.</li> <li>Apache PySpark for large-scale data processing.</li> <li>Apache PyFlink for stream and batch processing.</li> <li>Apache Beam for unified stream and batch data processing.</li> <li>Apache Storm for real-time computation.</li> </ol> </li> <li> <p>Genius hub - Where developers can share and sell their components. Coming soon geniusrise.com.</p> </li> </ol> <p>The framework aims to support multiple languages:</p> <ol> <li>Python</li> <li>Scala / JVM (WIP)</li> <li>Golang (WIP)</li> </ol> <p>This document delves into the core components and concepts that make up the Geniusrise framework.</p>"},{"location":"guides/concepts/#tradeoffs","title":"Tradeoffs","text":"<p>Because of the very loose coupling of the components, though the framework can be used to build very complex networks with independently running nodes, it provides limited orchestration capability, like synchronous pipelines. An external orchestrator like airflow can be used in such cases to orchestrate geniusrise components.</p>"},{"location":"guides/dev_cycle/","title":"Dev Cycle","text":"<p>This document describes one full local development cycle.</p>"},{"location":"guides/installation/","title":"Installation","text":"<p>Geniusrise is composed of the core framework and various plugins that implement specific tasks. The core has to be installed first, and after that selected plugins can be installed as and when required.</p>"},{"location":"guides/installation/#installing-geniusrise","title":"Installing Geniusrise","text":""},{"location":"guides/installation/#using-pip","title":"Using pip","text":"<p>To install the core framework using pip in local env, simply run:</p> <pre><code>pip install geniusrise\n</code></pre> <p>Or if you wish to install at user level:</p> <pre><code>pip install generiusrise --user\n</code></pre> <p>Or on a global level (might conflict with your OS package manager):</p> <pre><code>sudo pip install geniusrise\n</code></pre> <p>To verify the installation, you can check whether the geniusrise binary exists in PATH:</p> <pre><code>which geniusrise\n\ngeniusrise --help\n</code></pre>"},{"location":"guides/installation/#using-package-managers","title":"Using package managers","text":"<p>Geniusrise is also available as native packages for some Linux distributions.</p>"},{"location":"guides/installation/#aur","title":"AUR","text":"<p>Geniusrise is available on the AUR for arch and derived distros.</p> <pre><code>yay -S geniusrise\n</code></pre> <p>or directly from git master:</p> <pre><code>yay -S geniusrise-git\n</code></pre>"},{"location":"guides/installation/#ppa","title":"PPA","text":"<p>Geniusrise is also available on the PPA for debian-based distros.</p> <pre><code>sudo add-apt-repository ppa:ixaxaar/geniusrise\nsudo apt-get update\nsudo apt-get install -y geniusrise\n</code></pre>"},{"location":"guides/installation/#brew-cask","title":"Brew (cask)","text":"<pre><code>brew cask install geniusrise\n</code></pre>"},{"location":"guides/installation/#docker","title":"Docker","text":"<p>Geniusrise containers are available on Docker hub.</p> <pre><code>docker run -it --rm geniusrise/geniusrise:latest\n</code></pre>"},{"location":"guides/installation/#nix","title":"Nix","text":"<p>Coming soon \ud83d\ude22</p>"},{"location":"guides/installation/#installing-plugins","title":"Installing Plugins","text":"<p>Geniusrise offers a variety of plugins that act as composable lego blocks. To install a specific plugin, use the following format:</p> <pre><code>pip install geniusrise[plugin-name]\n</code></pre> <p>Replace <code>plugin-name</code> with the name of the desired plugin.</p>"},{"location":"guides/installation/#alternative-methods","title":"Alternative Methods","text":""},{"location":"guides/installation/#using-conda","title":"Using Conda","text":"<ol> <li>Activate the environment:</li> </ol> <pre><code>conda activate your-env\n</code></pre> <ol> <li>Install Geniusrise:</li> </ol> <pre><code>pip install geniusrise\n</code></pre> <p>For plugins:</p> <pre><code>pip install geniusrise[plugin-name]\n</code></pre>"},{"location":"guides/installation/#using-poetry","title":"Using Poetry","text":"<ol> <li>Add Geniusrise as a dependency:</li> </ol> <pre><code>poetry add geniusrise\n</code></pre> <p>For plugins:</p> <pre><code>poetry add geniusrise[plugin-name]\n</code></pre>"},{"location":"guides/installation/#development","title":"Development","text":"<p>For development, you may want to install from the repo:</p> <pre><code>git clone git@github.com:geniusrise/geniusrise.git\ncd geniusrise\nvirtualenv venv -p `which python3.10`\nsource venv/bin/activate\npip install -r ./requirements.txt\n\nmake install # installs in your local venv directory\n</code></pre> <p>That's it! You've successfully installed Geniusrise and its plugins. \ud83c\udf89</p>"},{"location":"guides/local/","title":"Local setup","text":"<p>This is probably the first thing that anyone would want to do.</p>"},{"location":"guides/local/#boilerplate","title":"Boilerplate","text":"<p>To setup a local geniusrise project, simply use the geniusrise project creator script:</p> <pre><code>curl -L https://cum.gdn/OfeQir | bash\n</code></pre> <p>or</p> <pre><code>curl -L https://raw.githubusercontent.com/geniusrise/geniusrise/master/scripts/install.sh | bash\n</code></pre>"},{"location":"guides/local/#existing-project","title":"Existing project","text":"<p>If you wish to add geniusrise to an existing project:</p> <pre><code>pip install geniusrise\npip freeze &gt; requirements.txt\n</code></pre>"},{"location":"guides/local/#from-scratch","title":"From scratch","text":"<p>Here is how to set up from scratch:</p> <pre><code>#!/bin/bash\n# Prompt for project details\nread -p \"Enter your project name: \" project_name\nread -p \"Enter your name: \" author_name\nread -p \"Enter your email: \" author_email\nread -p \"Enter your GitHub username: \" github_username\nread -p \"Enter a brief description of your project: \" project_description\n# Create project structure\nmkdir $project_name\ncd $project_name\nmkdir $project_name tests\n# Create basic files\ntouch README.md\ntouch requirements.txt\ntouch setup.py\ntouch Makefile\ntouch $project_name/__init__.py\ntouch tests/__init__.py\n# Populate README.md\necho \"# $project_name\" &gt; README.md\necho \"\\n$project_description\" &gt;&gt; README.md\n# Populate setup.py\ncat &lt;&lt;EOL &gt; setup.py\nfrom setuptools import setup, find_packages\nwith open(\"README.md\", \"r\", encoding=\"utf-8\") as fh:\nlong_description = fh.read()\nsetup(\nname='$project_name',\nversion='0.1.0',\npackages=find_packages(exclude=[\"tests\", \"tests.*\"]),\ninstall_requires=[],\npython_requires='&gt;=3.10',\nauthor='$author_name',\nauthor_email='$author_email',\ndescription='$project_description',\nlong_description=long_description,\nlong_description_content_type='text/markdown',\nurl='https://github.com/$github_username/$project_name',\nclassifiers=[\n'Programming Language :: Python :: 3',\n'License :: OSI Approved :: MIT License',\n'Operating System :: OS Independent',\n],\n)\nEOL\n# Populate Makefile\ncat &lt;&lt;EOL &gt; Makefile\nsetup:\n@pip install -r ./requirements.txt\ntest:\n@coverage run -m pytest -v ./tests\npublish:\n@python setup.py sdist bdist_wheel\n@twine upload dist/$project_name-\\$${VERSION}-* --verbose\nEOL\n# Set up the virtual environment and install necessary packages\nvirtualenv venv -p `which python3.10`\nsource venv/bin/activate\npip install twine setuptools pytest coverage\npip freeze &gt; requirements.txt\n# Fetch .pre-commit-config.yaml and .gitignore from geniusrise/geniusrise\ncurl -O https://raw.githubusercontent.com/geniusrise/geniusrise/master/.pre-commit-config.yaml\ncurl -O https://raw.githubusercontent.com/geniusrise/geniusrise/master/.gitignore\necho \"Project $project_name initialized!\"\n</code></pre> <p>Create a install script out of this and execute it:</p> <pre><code>touch install.sh\nchmod +x ./install.sh\n./install.sh\n</code></pre>"}]}