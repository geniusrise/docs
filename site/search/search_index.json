{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#geniusrise-agent-framework","title":"Geniusrise Agent Framework","text":""},{"location":"#about","title":"About","text":"<p>Geniusrise is a modular, loosely-coupled AgentOps / MLOps framework designed for the era of Large Language Models, offering flexibility, inclusivity, and standardization in designing networks of AI agents.</p> <p>It seamlessly integrates tasks, state management, data handling, and model versioning, all while supporting diverse infrastructures and user expertise levels. With its plug-and-play architecture, Geniusrise empowers teams to build, share, and deploy AI agent workflows across various platforms efficiently.</p>"},{"location":"#guides","title":"Guides","text":""},{"location":"#getting-started","title":"\ud83d\ude80 Getting Started","text":"<ol> <li>\ud83d\udcd8 Concepts - Concepts of the framework, start here.</li> <li>\ud83c\udfd7\ufe0f Architecture - Design and architecture of the framework.</li> <li>\ud83d\udee0\ufe0f Installation - Installation and setup.</li> </ol>"},{"location":"#development","title":"\ud83d\udcbb Development","text":"<ol> <li>\ud83c\udfe0 Local Experimentation - Local setup and project creation.</li> <li>\ud83d\udd04 Dev Cycle - Describes one full local development cycle.</li> <li>\ud83d\udce6 Packaging - Packaging your application.</li> <li>\ud83d\ude80 Deployment - Deploying parts or whole of your application.</li> <li>\u2699\ufe0f Workflow Ops - Operations and management of workflows.</li> <li>\ud83d\udcca Data Ops - Operations and management of data.</li> <li>\ud83e\udd16 Model Ops - Operations and management of models.</li> </ol>"},{"location":"#reference","title":"\ud83d\udcda Reference","text":"<ol> <li>\ud83d\udcc4 YAML Structure - Geniusfile structure and configuration.</li> <li>\ud83c\udf10 Community Plugins - Building and shipping community plugins (spouts and bolts).</li> <li>\ud83c\udfa8 Project Templates - Project templates for community plugins.</li> </ol>"},{"location":"#deployment","title":"\ud83d\ude80 Deployment","text":"\ud83c\udf10 Runners \ud83d\udfe2 Kubernetes \ud83d\udfe3 Apache Airflow \ud83d\udfe1 Apache Spark \ud83d\udfe0 Apache Flink \ud83d\udfe4 Apache Beam \ud83d\udd35 Apache Storm \ud83d\udfe5 AWS ECS \ud83d\udfe9 AWS Batch"},{"location":"#spouts","title":"\ud83c\udf2a\ufe0f Spouts","text":"\ud83c\udf10 Streaming \ud83d\udfe2 Http Polling \ud83d\udfe3 Socket.io \ud83d\udfe1 gRPC \ud83d\udfe0 QUIC \ud83d\udfe4 UDP \ud83d\udd35 Webhook \ud83d\udfe5 Websocket \ud83d\udfe9 SNS \ud83d\udfe7 SQS \ud83d\udfe8 AMQP \ud83d\udfeb Kafka \ud83d\udfea Kinesis Streams \ud83d\udfe9 MQTT \ud83d\udfe8 ActiveMQ \ud83d\udfeb ZeroMQ \ud83d\udfea Redis Pubsub \ud83d\udfe7 Redis Streams"},{"location":"#bolts","title":"\u26a1 Bolts","text":"\ud83c\udf10 Huggingface - Fine tuning \ud83d\udfe2 Language Model \ud83d\udfe3 Named Entity Recognition \ud83d\udfe1 Question Answering \ud83d\udfe0 Sentiment Analysis \ud83d\udfe4 Summarization \ud83d\udfe6 Translation \ud83d\udd35 Classification \ud83d\udd34 Commonsense Reasoning \ud83d\udfe7 Instruction Tuning \ud83c\udf10 OpenAI - Fine tuning \ud83d\udfe2 Classification \ud83d\udfe3 Commonsense Reasoning \ud83d\udfe1 Instruction Tuning \ud83d\udfe0 Language Model \ud83d\udfe4 Named Entity Recognition \ud83d\udfe6 Question Answering \ud83d\udd35 Sentiment Analysis \ud83d\udd34 Summarization \ud83d\udfe7 Translation"},{"location":"#library","title":"\ud83d\udcda Library","text":"\ud83d\udce6 geniusrise.cli \ud83d\udce6 geniusrise.core \ud83d\udce6 geniusrise.core.data \ud83d\udce6 geniusrise.core.state \ud83d\udce6 geniusrise.core.task \ud83d\udce6 geniusrise.runners \ud83d\udfe0 geniusctl \ud83d\udfe2 bolt \ud83d\udfe3 input \ud83d\udd34 base \ud83d\udfe4 base \ud83d\udd35 ecs \ud83d\udfe0 yamlctl \ud83d\udfe2 spout \ud83d\udfe3 output \ud83d\udd34 dynamo \ud83d\udd35 k8s \ud83d\udfe0 boltctl \ud83d\udfe3 batch_input \ud83d\udd34 memory \ud83d\udfe0 spoutctl \ud83d\udfe3 batch_output \ud83d\udd34 postgres \ud83d\udfe0 schema \ud83d\udfe3 streaming_input \ud83d\udd34 redis \ud83d\udfe0 discover \ud83d\udfe3 streaming_output \ud83d\udd34 prometheus \ud83d\udfe3 stream_to_batch_input \ud83d\udfe3 stream_to_batch_output \ud83d\udfe3 batch_to_stream_input"},{"location":"bolts/huggingface/classification/","title":"Classification","text":"<p>             Bases: <code>HuggingFaceFineTuner</code></p> <p>A bolt for fine-tuning Hugging Face models for text classification tasks.</p> <pre><code>Args:\n    model: The pre-trained model to fine-tune.\n    tokenizer: The tokenizer associated with the model.\n    input (BatchInput): The batch input data.\n    output (OutputConfig): The output data.\n    state (State): The state manager.\n</code></pre>"},{"location":"bolts/huggingface/classification/#classification.HuggingFaceClassificationFineTuner.load_dataset","title":"<code>load_dataset(dataset_path, **kwargs)</code>","text":"<p>Load a classification dataset from a directory.</p> <pre><code>The directory can contain any of the following file types:\n- Dataset files saved by the Hugging Face datasets library.\n- JSONL files: Each line is a JSON object representing an example. Structure:\n    {\n        \"text\": \"The text content\",\n        \"label\": \"The label\"\n    }\n- XML files: Each 'record' element should contain 'text' and 'label' child elements.\n- YAML files: Each document should be a dictionary with 'text' and 'label' keys.\n- TSV files: Should contain 'text' and 'label' columns separated by tabs.\n- Excel files (.xls, .xlsx): Should contain 'text' and 'label' columns.\n- SQLite files (.db): Should contain a table with 'text' and 'label' columns.\n- Feather files: Should contain 'text' and 'label' columns.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the dataset directory.</p> required <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Optional[Dataset]</code> <p>The loaded dataset.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there was an error loading the dataset.</p>"},{"location":"bolts/huggingface/commonsense_reasoning/","title":"Commonsense Reasoning","text":"<p>             Bases: <code>HuggingFaceFineTuner</code></p> <p>A bolt for fine-tuning Hugging Face models on commonsense reasoning tasks.</p> <pre><code>Args:\n    model: The pre-trained model to fine-tune.\n    tokenizer: The tokenizer associated with the model.\n    input (BatchInput): The batch input data.\n    output (OutputConfig): The output data.\n    state (State): The state manager.\n</code></pre>"},{"location":"bolts/huggingface/commonsense_reasoning/#commonsense_reasoning.HuggingFaceCommonsenseReasoningFineTuner.data_collator","title":"<code>data_collator(examples)</code>","text":"<p>Customize the data collator.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>Dict</code> <p>The examples to collate.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict</code> <p>The collated data.</p>"},{"location":"bolts/huggingface/commonsense_reasoning/#commonsense_reasoning.HuggingFaceCommonsenseReasoningFineTuner.load_dataset","title":"<code>load_dataset(dataset_path, **kwargs)</code>","text":"<p>Load a commonsense reasoning dataset from a directory.</p> <pre><code>The directory can contain any of the following file types:\n- Dataset files saved by the Hugging Face datasets library.\n- JSONL files: Each line is a JSON object representing an example. Structure:\n    {\n        \"premise\": \"The premise text\",\n        \"hypothesis\": \"The hypothesis text\",\n        \"label\": 0 or 1 or 2\n    }\n- CSV files: Should contain 'premise', 'hypothesis', and 'label' columns.\n- Parquet files: Should contain 'premise', 'hypothesis', and 'label' columns.\n- JSON files: Should be an array of objects with 'premise', 'hypothesis', and 'label' keys.\n- XML files: Each 'record' element should contain 'premise', 'hypothesis', and 'label' child elements.\n- YAML/YML files: Each document should be a dictionary with 'premise', 'hypothesis', and 'label' keys.\n- TSV files: Should contain 'premise', 'hypothesis', and 'label' columns separated by tabs.\n- Excel files (.xls, .xlsx): Should contain 'premise', 'hypothesis', and 'label' columns.\n- Feather files: Should contain 'premise', 'hypothesis', and 'label' columns.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the dataset directory.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Union[Dataset, DatasetDict, None]</code> <p>The loaded dataset.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there was an error loading the dataset.</p>"},{"location":"bolts/huggingface/commonsense_reasoning/#commonsense_reasoning.HuggingFaceCommonsenseReasoningFineTuner.prepare_train_features","title":"<code>prepare_train_features(examples)</code>","text":"<p>Tokenize the examples and prepare the features for training.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>dict</code> <p>A dictionary of examples.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict</code> <p>The processed features.</p>"},{"location":"bolts/huggingface/instruction_tuning/","title":"Instruction Tuning","text":"<p>             Bases: <code>HuggingFaceFineTuner</code></p> <p>A bolt for fine-tuning Hugging Face models on instruction tuning tasks.</p> <pre><code>Args:\n    model: The pre-trained model to fine-tune.\n    tokenizer: The tokenizer associated with the model.\n    input (BatchInput): The batch input data.\n    output (OutputConfig): The output data.\n    state (State): The state manager.\n</code></pre>"},{"location":"bolts/huggingface/instruction_tuning/#instruction_tuning.HuggingFaceInstructionTuningFineTuner.load_dataset","title":"<code>load_dataset(dataset_path, **kwargs)</code>","text":"<p>Load an instruction tuning dataset from a directory.</p> <pre><code>The directory can contain any of the following file types:\n- Dataset files saved by the Hugging Face datasets library (.arrow).\n- JSONL files: Each line is a JSON object representing an example. Structure:\n    {\n        \"instruction\": \"The instruction\",\n        \"output\": \"The output\"\n    }\n- CSV files: Should contain 'instruction' and 'output' columns.\n- Parquet files: Should contain 'instruction' and 'output' columns.\n- JSON files: Should be an array of objects with 'instruction' and 'output' keys.\n- XML files: Each 'record' element should contain 'instruction' and 'output' child elements.\n- YAML/YML files: Each document should be a dictionary with 'instruction' and 'output' keys.\n- TSV files: Should contain 'instruction' and 'output' columns separated by tabs.\n- Excel files (.xls, .xlsx): Should contain 'instruction' and 'output' columns.\n- SQLite files (.db): Should contain a table with 'instruction' and 'output' columns.\n- Feather files: Should contain 'instruction' and 'output' columns.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the dataset directory.</p> required <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Union[HFDataset, Dict]</code> <p>The loaded dataset.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there was an error loading the dataset.</p>"},{"location":"bolts/huggingface/language_model/","title":"Language Model","text":"<p>             Bases: <code>HuggingFaceFineTuner</code></p> <p>A bolt for fine-tuning Hugging Face models on language modeling tasks.</p> <pre><code>Args:\n    model: The pre-trained model to fine-tune.\n    tokenizer: The tokenizer associated with the model.\n    input (BatchInput): The batch input data.\n    output (OutputConfig): The output data.\n    state (State): The state manager.\n</code></pre>"},{"location":"bolts/huggingface/language_model/#language_model.HuggingFaceLanguageModelingFineTuner.data_collator","title":"<code>data_collator(examples)</code>","text":"<p>Customize the data collator.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <p>The examples to collate.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>The collated data.</p>"},{"location":"bolts/huggingface/language_model/#language_model.HuggingFaceLanguageModelingFineTuner.load_dataset","title":"<code>load_dataset(dataset_path, **kwargs)</code>","text":"<p>Load a language modeling dataset from a directory.</p> <pre><code>The directory can contain any of the following file types:\n- Dataset files saved by the Hugging Face datasets library.\n- JSONL files: Each line is a JSON object representing an example. Structure:\n    {\n        \"text\": \"The text content\"\n    }\n- CSV files: Should contain a 'text' column.\n- Parquet files: Should contain a 'text' column.\n- JSON files: Should be an array of objects with a 'text' key.\n- XML files: Each 'record' element should contain a 'text' child element.\n- YAML/YML files: Each document should be a dictionary with a 'text' key.\n- TSV files: Should contain a 'text' column separated by tabs.\n- Excel files (.xls, .xlsx): Should contain a 'text' column.\n- SQLite files (.db): Should contain a table with a 'text' column.\n- Feather files: Should contain a 'text' column.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the dataset directory.</p> required <p>Returns:</p> Name Type Description <code>Dataset</code> <p>The loaded dataset.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there was an error loading the dataset.</p>"},{"location":"bolts/huggingface/language_model/#language_model.HuggingFaceLanguageModelingFineTuner.prepare_train_features","title":"<code>prepare_train_features(examples)</code>","text":"<p>Tokenize the examples and prepare the features for training.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>dict</code> <p>A dictionary of examples.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>The processed features.</p>"},{"location":"bolts/huggingface/ner/","title":"Named Entity Recognition","text":"<p>             Bases: <code>HuggingFaceFineTuner</code></p> <p>A bolt for fine-tuning Hugging Face models on named entity recognition tasks.</p> <pre><code>Args:\n    model: The pre-trained model to fine-tune.\n    tokenizer: The tokenizer associated with the model.\n    input (BatchInput): The batch input data.\n    output (OutputConfig): The output data.\n    state (State): The state manager.\n</code></pre>"},{"location":"bolts/huggingface/ner/#ner.HuggingFaceNamedEntityRecognitionFineTuner.__init__","title":"<code>__init__(model, tokenizer, input, output, state, label_list, **kwargs)</code>","text":"<p>Initialize the NamedEntityRecognitionFineTuner.</p> <pre><code>Args:\n    model: The pre-trained model to fine-tune.\n    tokenizer: The tokenizer associated with the model.\n    input (BatchInput): The batch input data.\n    output (BatchOutput): The batch output data.\n    state (State): The state manager.\n    label_list (List[str]): The list of labels for the NER task.\n    **kwargs: Additional arguments for the superclass.\n</code></pre>"},{"location":"bolts/huggingface/ner/#ner.HuggingFaceNamedEntityRecognitionFineTuner.data_collator","title":"<code>data_collator(examples)</code>","text":"<p>Customize the data collator.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>List[Dict[str, torch.Tensor]]</code> <p>The examples to collate.</p> required <p>Returns:</p> Type Description <code>Dict[str, torch.Tensor]</code> <p>Dict[str, torch.Tensor]: The collated data.</p>"},{"location":"bolts/huggingface/ner/#ner.HuggingFaceNamedEntityRecognitionFineTuner.load_dataset","title":"<code>load_dataset(dataset_path, **kwargs)</code>","text":"<p>Load a named entity recognition dataset from a directory.</p> <pre><code>The directory can contain any of the following file types:\n- Dataset files saved by the Hugging Face datasets library.\n- JSONL files: Each line is a JSON object representing an example. Structure:\n    {\n        \"tokens\": [\"token1\", \"token2\", ...],\n        \"ner_tags\": [0, 1, ...]\n    }\n- CSV files: Should contain 'tokens' and 'ner_tags' columns.\n- Parquet files: Should contain 'tokens' and 'ner_tags' columns.\n- JSON files: Should be an array of objects with 'tokens' and 'ner_tags' keys.\n- XML files: Each 'record' element should contain 'tokens' and 'ner_tags' child elements.\n- YAML/YML files: Each document should be a dictionary with 'tokens' and 'ner_tags' keys.\n- TSV files: Should contain 'tokens' and 'ner_tags' columns separated by tabs.\n- Excel files (.xls, .xlsx): Should contain 'tokens' and 'ner_tags' columns.\n- SQLite files (.db): Should contain a table with 'tokens' and 'ner_tags' columns.\n- Feather files: Should contain 'tokens' and 'ner_tags' columns.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the dataset directory.</p> required <p>Returns:</p> Name Type Description <code>DatasetDict</code> <code>DatasetDict</code> <p>The loaded dataset.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there was an error loading the dataset.</p>"},{"location":"bolts/huggingface/ner/#ner.HuggingFaceNamedEntityRecognitionFineTuner.prepare_train_features","title":"<code>prepare_train_features(examples)</code>","text":"<p>Tokenize the examples and prepare the features for training.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>Dict[str, Union[List[str], List[int]]]</code> <p>A dictionary of examples.</p> required <p>Returns:</p> Type Description <code>Dict[str, Union[List[str], List[int]]]</code> <p>Dict[str, Union[List[str], List[int]]]: The processed features.</p>"},{"location":"bolts/huggingface/question_answering/","title":"Question Answering","text":"<p>             Bases: <code>HuggingFaceFineTuner</code></p> <p>A bolt for fine-tuning Hugging Face models on question answering tasks.</p> <pre><code>Args:\n    model: The pre-trained model to fine-tune.\n    tokenizer: The tokenizer associated with the model.\n    input (BatchInput): The batch input data.\n    output (OutputConfig): The output data.\n    state (State): The state manager.\n</code></pre>"},{"location":"bolts/huggingface/question_answering/#question_answering.HuggingFaceQuestionAnsweringFineTuner.__init__","title":"<code>__init__(model, tokenizer, input, output, state, pad_on_right, max_length, doc_stride, eval=False, **kwargs)</code>","text":"<p>Initialize the bolt.</p> <pre><code>Args:\n    model (PreTrainedModel): The pre-trained model to fine-tune.\n    tokenizer (PreTrainedTokenizer): The tokenizer associated with the model.\n    input (BatchInput): The batch input data.\n    output (OutputConfig): The output data.\n    state (State): The state manager.\n    pad_on_right (bool): Whether to pad on the right.\n    max_length (int): The maximum length of the sequences.\n    doc_stride (int): The document stride.\n    eval (bool, optional): Whether to evaluate the model after training. Defaults to False.\n    **kwargs: Additional keyword arguments.\n</code></pre>"},{"location":"bolts/huggingface/question_answering/#question_answering.HuggingFaceQuestionAnsweringFineTuner.compute_metrics","title":"<code>compute_metrics(eval_pred)</code>","text":"<p>Compute the accuracy of the model's predictions.</p> <p>Parameters:</p> Name Type Description Default <code>eval_pred</code> <code>tuple</code> <p>A tuple containing two elements: - predictions (np.ndarray): The model's predictions. - label_ids (np.ndarray): The true labels.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[Dict[str, float]]</code> <p>A dictionary mapping metric names to computed values.</p>"},{"location":"bolts/huggingface/question_answering/#question_answering.HuggingFaceQuestionAnsweringFineTuner.load_dataset","title":"<code>load_dataset(dataset_path, pad_on_right=None, max_length=None, doc_stride=None, **kwargs)</code>","text":"<p>Load a dataset from a directory.</p> <pre><code>The directory can contain any of the following file types:\n- Dataset files saved by the Hugging Face datasets library.\n- JSONL files: Each line is a JSON object representing an example. Structure:\n    {\n        \"context\": \"The context content\",\n        \"question\": \"The question\",\n        \"answers\": {\n            \"answer_start\": [int],\n            \"text\": [str]\n        }\n    }\n- CSV files: Should contain 'context', 'question', and 'answers' columns.\n- Parquet files: Should contain 'context', 'question', and 'answers' columns.\n- JSON files: Should contain an array of objects with 'context', 'question', and 'answers' keys.\n- XML files: Each 'record' element should contain 'context', 'question', and 'answers' child elements.\n- YAML files: Each document should be a dictionary with 'context', 'question', and 'answers' keys.\n- TSV files: Should contain 'context', 'question', and 'answers' columns separated by tabs.\n- Excel files (.xls, .xlsx): Should contain 'context', 'question', and 'answers' columns.\n- SQLite files (.db): Should contain a table with 'context', 'question', and 'answers' columns.\n- Feather files: Should contain 'context', 'question', and 'answers' columns.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the directory containing the dataset files.</p> required <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Optional[Dataset]</code> <p>The loaded dataset.</p>"},{"location":"bolts/huggingface/question_answering/#question_answering.HuggingFaceQuestionAnsweringFineTuner.prepare_train_features","title":"<code>prepare_train_features(examples)</code>","text":"<p>Tokenize our examples with truncation and padding, but keep the overflows using a stride.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>Dict[str, Union[str, List[str]]]</code> <p>The examples to be tokenized.</p> required <p>Returns:</p> Type Description <code>Optional[Dict[str, Union[List[int], List[List[int]]]]]</code> <p>The tokenized examples.</p>"},{"location":"bolts/huggingface/sentiment_analysis/","title":"Sentiment Analysis","text":"<p>             Bases: <code>HuggingFaceFineTuner</code></p> <p>A bolt for fine-tuning Hugging Face models on sentiment analysis tasks.</p> <pre><code>Args:\n    model: The pre-trained model to fine-tune.\n    tokenizer: The tokenizer associated with the model.\n    input (BatchInput): The batch input data.\n    output (OutputConfig): The output data.\n    state (State): The state manager.\n</code></pre>"},{"location":"bolts/huggingface/sentiment_analysis/#sentiment_analysis.HuggingFaceSentimentAnalysisFineTuner.data_collator","title":"<code>data_collator(examples)</code>","text":"<p>Customize the data collator.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>List[Dict[str, Union[List[int], int]]]</code> <p>The examples to collate.</p> required <p>Returns:</p> Type Description <code>Dict[str, Union[torch.Tensor, List[torch.Tensor]]]</code> <p>Dict[str, Union[torch.Tensor, List[torch.Tensor]]]: The collated data.</p>"},{"location":"bolts/huggingface/sentiment_analysis/#sentiment_analysis.HuggingFaceSentimentAnalysisFineTuner.load_dataset","title":"<code>load_dataset(dataset_path, **kwargs)</code>","text":"<pre><code>Load a dataset from a directory.\n\nArgs:\n    dataset_path (str): The path to the directory containing the dataset files.\n\nReturns:\n    DatasetDict: The loaded dataset.\n</code></pre> <pre><code>The directory can contain any of the following file types:\n- Dataset files saved by the Hugging Face datasets library.\n- JSONL files: Each line is a JSON object representing an example. Structure:\n    {\n        \"text\": \"The text content\",\n        \"label\": \"The label\"\n    }\n- CSV files: Should contain 'text' and 'label' columns.\n- Parquet files: Should contain 'text' and 'label' columns.\n- JSON files: Should contain an array of objects with 'text' and 'label' keys.\n- XML files: Each 'record' element should contain 'text' and 'label' child elements.\n- YAML files: Each document should be a dictionary with 'text' and 'label' keys.\n- TSV files: Should contain 'text' and 'label' columns separated by tabs.\n- Excel files (.xls, .xlsx): Should contain 'text' and 'label' columns.\n- SQLite files (.db): Should contain a table with 'text' and 'label' columns.\n- Feather files: Should contain 'text' and 'label' columns.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The pre-trained model to fine-tune.</p> required <code>tokenizer</code> <p>The tokenizer associated with the model.</p> required <code>input</code> <code>BatchInput</code> <p>The batch input data.</p> required <code>output</code> <code>OutputConfig</code> <p>The output data.</p> required <code>state</code> <code>State</code> <p>The state manager.</p> required"},{"location":"bolts/huggingface/sentiment_analysis/#sentiment_analysis.HuggingFaceSentimentAnalysisFineTuner.prepare_train_features","title":"<code>prepare_train_features(examples)</code>","text":"<p>Tokenize the examples and prepare the features for training.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>Dict[str, Union[str, int]]</code> <p>A dictionary of examples.</p> required <p>Returns:</p> Type Description <code>Dict[str, Union[List[int], int]]</code> <p>Dict[str, Union[List[int], int]]: The processed features.</p>"},{"location":"bolts/huggingface/summarization/","title":"Summarization","text":"<p>             Bases: <code>HuggingFaceFineTuner</code></p> <p>A bolt for fine-tuning Hugging Face models on summarization tasks.</p> <pre><code>Args:\n    model: The pre-trained model to fine-tune.\n    tokenizer: The tokenizer associated with the model.\n    input (BatchInput): The batch input data.\n    output (OutputConfig): The output data.\n    state (State): The state manager.\n</code></pre>"},{"location":"bolts/huggingface/summarization/#summarization.HuggingFaceSummarizationFineTuner.compute_metrics","title":"<code>compute_metrics(pred)</code>","text":"<p>Compute ROUGE metrics.</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <code>EvalPrediction</code> <p>The predicted results.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, float]</code> <p>A dictionary with ROUGE-1, ROUGE-2, and ROUGE-L scores.</p>"},{"location":"bolts/huggingface/summarization/#summarization.HuggingFaceSummarizationFineTuner.create_optimizer_and_scheduler","title":"<code>create_optimizer_and_scheduler(num_training_steps)</code>","text":"<p>Create an optimizer and a learning rate scheduler.</p> <p>Parameters:</p> Name Type Description Default <code>num_training_steps</code> <code>int</code> <p>The total number of training steps.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[AdamW, Any]</code> <p>The optimizer and the scheduler.</p>"},{"location":"bolts/huggingface/summarization/#summarization.HuggingFaceSummarizationFineTuner.data_collator","title":"<code>data_collator(examples)</code>","text":"<p>Customize the data collator.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>List[Dict[str, Union[str, List[int]]]]</code> <p>The examples to collate.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Union[List[int], List[List[int]]]]</code> <p>The collated data.</p>"},{"location":"bolts/huggingface/summarization/#summarization.HuggingFaceSummarizationFineTuner.load_dataset","title":"<code>load_dataset(dataset_path, **kwargs)</code>","text":"<p>Load a dataset from a directory.</p> <pre><code>The directory can contain any of the following file types:\n- Dataset files saved by the Hugging Face datasets library.\n- JSONL files: Each line is a JSON object representing an example. Structure:\n    {\n        \"document\": \"The document content\",\n        \"summary\": \"The summary\"\n    }\n- CSV files: Should contain 'document' and 'summary' columns.\n- Parquet files: Should contain 'document' and 'summary' columns.\n- JSON files: Should contain an array of objects with 'document' and 'summary' keys.\n- XML files: Each 'record' element should contain 'document' and 'summary' child elements.\n- YAML files: Each document should be a dictionary with 'document' and 'summary' keys.\n- TSV files: Should contain 'document' and 'summary' columns separated by tabs.\n- Excel files (.xls, .xlsx): Should contain 'document' and 'summary' columns.\n- SQLite files (.db): Should contain a table with 'document' and 'summary' columns.\n- Feather files: Should contain 'document' and 'summary' columns.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the directory containing the dataset files.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>DatasetDict</code> <code>Optional[DatasetDict]</code> <p>The loaded dataset.</p>"},{"location":"bolts/huggingface/summarization/#summarization.HuggingFaceSummarizationFineTuner.prepare_train_features","title":"<code>prepare_train_features(examples)</code>","text":"<p>Tokenize the examples and prepare the features for training.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>dict</code> <p>A dictionary of examples.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[Dict[str, List[int]]]</code> <p>The processed features.</p>"},{"location":"bolts/huggingface/translation/","title":"Translation","text":"<p>             Bases: <code>HuggingFaceFineTuner</code></p> <p>A bolt for fine-tuning Hugging Face models on translation tasks.</p> <pre><code>Args:\n    model: The pre-trained model to fine-tune.\n    tokenizer: The tokenizer associated with the model.\n    input (BatchInput): The batch input data.\n    output (OutputConfig): The output data.\n    state (State): The state manager.\n</code></pre>"},{"location":"bolts/huggingface/translation/#translation.HuggingFaceTranslationFineTuner.data_collator","title":"<code>data_collator(examples)</code>","text":"<p>Customize the data collator.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <p>The examples to collate.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>The collated data.</p>"},{"location":"bolts/huggingface/translation/#translation.HuggingFaceTranslationFineTuner.load_dataset","title":"<code>load_dataset(dataset_path, **kwargs)</code>","text":"<p>Load a dataset from a directory.</p> <pre><code>The directory can contain any of the following file types:\n- Dataset files saved by the Hugging Face datasets library.\n- JSONL files: Each line is a JSON object representing an example. Structure:\n    {\n        \"translation\": {\n            \"en\": \"English text\",\n            \"fr\": \"French text\"\n        }\n    }\n- CSV files: Should contain 'en' and 'fr' columns.\n- Parquet files: Should contain 'en' and 'fr' columns.\n- JSON files: Should contain an array of objects with 'en' and 'fr' keys.\n- XML files: Each 'record' element should contain 'en' and 'fr' child elements.\n- YAML files: Each document should be a dictionary with 'en' and 'fr' keys.\n- TSV files: Should contain 'en' and 'fr' columns separated by tabs.\n- Excel files (.xls, .xlsx): Should contain 'en' and 'fr' columns.\n- SQLite files (.db): Should contain a table with 'en' and 'fr' columns.\n- Feather files: Should contain 'en' and 'fr' columns.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the directory containing the dataset files.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>DatasetDict</code> <code>Optional[DatasetDict]</code> <p>The loaded dataset.</p>"},{"location":"bolts/huggingface/translation/#translation.HuggingFaceTranslationFineTuner.prepare_train_features","title":"<code>prepare_train_features(examples)</code>","text":"<p>Tokenize the examples and prepare the features for training.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>dict</code> <p>A dictionary of examples.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>The processed features.</p>"},{"location":"core/cli_boltctl/","title":"Boltctl","text":"<p>The main bolt controller</p>"},{"location":"core/cli_boltctl/#cli.boltctl.BoltCtl","title":"<code>BoltCtl</code>","text":"<p>Class for managing bolts end-to-end from the command line.</p>"},{"location":"core/cli_boltctl/#cli.boltctl.BoltCtl.__init__","title":"<code>__init__(discovered_bolt)</code>","text":"<p>Initialize BoltCtl with a DiscoveredBolt object.</p> <p>Parameters:</p> Name Type Description Default <code>discovered_bolt</code> <code>DiscoveredBolt</code> <p>DiscoveredBolt object used to create and manage bolts.</p> required"},{"location":"core/cli_boltctl/#cli.boltctl.BoltCtl.create_bolt","title":"<code>create_bolt(input_type, output_type, state_type, **kwargs)</code>","text":"<p>Create a bolt of a specific type.</p> <p>Parameters:</p> Name Type Description Default <code>input_type</code> <code>str</code> <p>The type of input (\"batch\" or \"streaming\").</p> required <code>output_type</code> <code>str</code> <p>The type of output (\"batch\" or \"streaming\").</p> required <code>state_type</code> <code>str</code> <p>The type of state manager (\"in_memory\", \"redis\", \"postgres\", or \"dynamodb\").</p> required <code>**kwargs</code> <p>Additional keyword arguments for initializing the bolt. <pre><code>Keyword Arguments:\n    Batch input:\n    - input_folder (str): The input folder argument.\n    - input_s3_bucket (str): The input bucket argument.\n    - input_s3_folder (str): The input S3 folder argument.\n    Batch outupt config:\n    - output_folder (str): The output folder argument.\n    - output_s3_bucket (str): The output bucket argument.\n    - output_s3_folder (str): The output S3 folder argument.\n    Streaming input:\n    - input_kafka_cluster_connection_string (str): The input Kafka servers argument.\n    - input_kafka_topic (str): The input kafka topic argument.\n    - input_kafka_consumer_group_id (str): The Kafka consumer group id.\n    Streaming output:\n    - output_kafka_cluster_connection_string (str): The output Kafka servers argument.\n    - output_kafka_topic (str): The output kafka topic argument.\n    Stream-to-Batch input:\n    - buffer_size (int): Number of messages to buffer.\n    - input_kafka_cluster_connection_string (str): The input Kafka servers argument.\n    - input_kafka_topic (str): The input kafka topic argument.\n    - input_kafka_consumer_group_id (str): The Kafka consumer group id.\n    Batch-to-Streaming input:\n    - buffer_size (int): Number of messages to buffer.\n    - input_folder (str): The input folder argument.\n    - input_s3_bucket (str): The input bucket argument.\n    - input_s3_folder (str): The input S3 folder argument.\n    Stream-to-Batch output:\n    - buffer_size (int): Number of messages to buffer.\n    - output_folder (str): The output folder argument.\n    - output_s3_bucket (str): The output bucket argument.\n    - output_s3_folder (str): The output S3 folder argument.\n    Redis state manager config:\n    - redis_host (str): The Redis host argument.\n    - redis_port (str): The Redis port argument.\n    - redis_db (str): The Redis database argument.\n    Postgres state manager config:\n    - postgres_host (str): The PostgreSQL host argument.\n    - postgres_port (str): The PostgreSQL port argument.\n    - postgres_user (str): The PostgreSQL user argument.\n    - postgres_password (str): The PostgreSQL password argument.\n    - postgres_database (str): The PostgreSQL database argument.\n    - postgres_table (str): The PostgreSQL table argument.\n    DynamoDB state manager config:\n    - dynamodb_table_name (str): The DynamoDB table name argument.\n    - dynamodb_region_name (str): The DynamoDB region name argument.\n    Prometheus state manager config:\n    - prometheus_gateway (str): The push gateway for Prometheus metrics.\n</code></pre></p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Bolt</code> <code>Bolt</code> <p>The created bolt.</p>"},{"location":"core/cli_boltctl/#cli.boltctl.BoltCtl.create_parser","title":"<code>create_parser(parser)</code>","text":"<p>Add arguments to the command-line parser for managing the bolt.</p> <p>Parameters:</p> Name Type Description Default <code>parser</code> <code>argparse.ArgumentParser</code> <p>Command-line parser.</p> required"},{"location":"core/cli_boltctl/#cli.boltctl.BoltCtl.execute_bolt","title":"<code>execute_bolt(bolt, method_name, *args, **kwargs)</code>","text":"<p>Execute a method of a bolt.</p> <p>Parameters:</p> Name Type Description Default <code>bolt</code> <code>Bolt</code> <p>The bolt to execute.</p> required <code>method_name</code> <code>str</code> <p>The name of the method to execute.</p> required <code>*args</code> <p>Positional arguments to pass to the method.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments to pass to the method.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <p>The result of the method.</p>"},{"location":"core/cli_boltctl/#cli.boltctl.BoltCtl.run","title":"<code>run(args)</code>","text":"<p>Run the command-line interface.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>argparse.Namespace</code> <p>Parsed command-line arguments.</p> required"},{"location":"core/cli_discover/","title":"Discover","text":"<p>Module discovery</p>"},{"location":"core/cli_discover/#cli.discover.Discover","title":"<code>Discover</code>","text":""},{"location":"core/cli_discover/#cli.discover.Discover.__init__","title":"<code>__init__(directory=None)</code>","text":"<p>Initialize the Discover class.</p>"},{"location":"core/cli_discover/#cli.discover.Discover.discover_installed_extensions","title":"<code>discover_installed_extensions()</code>","text":"<p>Discover installed geniusrise extensions.</p>"},{"location":"core/cli_discover/#cli.discover.Discover.find_classes","title":"<code>find_classes(module)</code>","text":"<p>Discover spout/bolt classes in a module.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Any</code> <p>Module to scan for spout/bolt classes.</p> required"},{"location":"core/cli_discover/#cli.discover.Discover.get_geniusignore_patterns","title":"<code>get_geniusignore_patterns(directory)</code>  <code>staticmethod</code>","text":"<p>Read the .geniusignore file and return a list of patterns to ignore.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Directory containing the .geniusignore file.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of patterns to ignore.</p>"},{"location":"core/cli_discover/#cli.discover.Discover.get_init_args","title":"<code>get_init_args(cls)</code>","text":"<p>Extract initialization arguments of a class.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>type</code> <p>Class to extract initialization arguments from.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Initialization arguments.</p>"},{"location":"core/cli_discover/#cli.discover.Discover.import_module","title":"<code>import_module(path)</code>","text":"<p>Import a module given its path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the module.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <p>Imported module.</p>"},{"location":"core/cli_discover/#cli.discover.Discover.scan_directory","title":"<code>scan_directory(directory=None)</code>","text":"<p>Scan for spouts/bolts in installed extensions and user's codebase.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Optional[str]</code> <p>Directory to scan for user-defined spouts/bolts.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Discovered spouts/bolts.</p>"},{"location":"core/cli_geniusctl/","title":"Geniusctl","text":"<p>The main command line application</p>"},{"location":"core/cli_geniusctl/#cli.geniusctl.GeniusCtl","title":"<code>GeniusCtl</code>","text":"<p>Main class for managing the geniusrise CLI application.</p>"},{"location":"core/cli_geniusctl/#cli.geniusctl.GeniusCtl.__init__","title":"<code>__init__()</code>","text":"<p>Initialize GeniusCtl.v</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>The directory to scan for spouts and bolts.</p> required"},{"location":"core/cli_geniusctl/#cli.geniusctl.GeniusCtl.cli","title":"<code>cli()</code>","text":"<p>Main function to be called when geniusrise is run from the command line.</p>"},{"location":"core/cli_geniusctl/#cli.geniusctl.GeniusCtl.create_parser","title":"<code>create_parser()</code>","text":"<p>Create a command-line parser with arguments for managing the application.</p> <p>Returns:</p> Type Description <p>argparse.ArgumentParser: Command-line parser.</p>"},{"location":"core/cli_geniusctl/#cli.geniusctl.GeniusCtl.list_spouts_and_bolts","title":"<code>list_spouts_and_bolts(verbose=False)</code>","text":"<p>List all discovered spouts and bolts in a table.</p>"},{"location":"core/cli_geniusctl/#cli.geniusctl.GeniusCtl.run","title":"<code>run(args)</code>","text":"<p>Run the command-line interface.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>argparse.Namespace</code> <p>Parsed command-line arguments.</p> required"},{"location":"core/cli_schema/","title":"YAML schema","text":"<p>YAML schema definition as pydantic</p>"},{"location":"core/cli_schema/#cli.schema.Bolt","title":"<code>Bolt</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This class defines a bolt. A bolt has a name, method, optional arguments, input, output, state, and deployment.</p>"},{"location":"core/cli_schema/#cli.schema.Deploy","title":"<code>Deploy</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This class defines the deployment of the spout or bolt. The deployment can be of type k8s or ecs.</p>"},{"location":"core/cli_schema/#cli.schema.DeployArgs","title":"<code>DeployArgs</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This class defines the arguments for the deployment. Depending on the type of deployment (k8s, ecs), different arguments are required.</p>"},{"location":"core/cli_schema/#cli.schema.ExtraKwargs","title":"<code>ExtraKwargs</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This class is used to handle any extra arguments that are not explicitly defined in the schema.</p>"},{"location":"core/cli_schema/#cli.schema.Geniusfile","title":"<code>Geniusfile</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This class defines the overall structure of the YAML file. It includes a version, spouts, and bolts.</p>"},{"location":"core/cli_schema/#cli.schema.Input","title":"<code>Input</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This class defines the input of the bolt. The input can be of type batch, streaming, spout, or bolt.</p>"},{"location":"core/cli_schema/#cli.schema.InputArgs","title":"<code>InputArgs</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This class defines the arguments for the input. Depending on the type of input (batch, streaming, spout, bolt), different arguments are required.</p>"},{"location":"core/cli_schema/#cli.schema.Output","title":"<code>Output</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This class defines the output of the spout or bolt. The output can be of type batch or streaming.</p>"},{"location":"core/cli_schema/#cli.schema.OutputArgs","title":"<code>OutputArgs</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This class defines the arguments for the output. Depending on the type of output (batch, streaming), different arguments are required.</p>"},{"location":"core/cli_schema/#cli.schema.Spout","title":"<code>Spout</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This class defines a spout. A spout has a name, method, optional arguments, output, state, and deployment.</p>"},{"location":"core/cli_schema/#cli.schema.State","title":"<code>State</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This class defines the state of the spout or bolt. The state can be of type in_memory, redis, postgres, or dynamodb.</p>"},{"location":"core/cli_schema/#cli.schema.StateArgs","title":"<code>StateArgs</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This class defines the arguments for the state. Depending on the type of state (in_memory, redis, postgres, dynamodb), different arguments are required.</p>"},{"location":"core/cli_spoutctl/","title":"Spoutctl","text":"<p>The main spout controller</p>"},{"location":"core/cli_spoutctl/#cli.spoutctl.SpoutCtl","title":"<code>SpoutCtl</code>","text":"<p>Class for managing spouts end-to-end from the command line.</p>"},{"location":"core/cli_spoutctl/#cli.spoutctl.SpoutCtl.__init__","title":"<code>__init__(discovered_spout)</code>","text":"<p>Initialize SpoutCtl with a DiscoveredSpout object.</p> <p>Parameters:</p> Name Type Description Default <code>discovered_spout</code> <code>DiscoveredSpout</code> <p>DiscoveredSpout object used to create and manage spouts.</p> required"},{"location":"core/cli_spoutctl/#cli.spoutctl.SpoutCtl.create_parser","title":"<code>create_parser(parser)</code>","text":"<p>Add arguments to the command-line parser for managing the spout.</p> <p>Parameters:</p> Name Type Description Default <code>parser</code> <code>argparse.ArgumentParser</code> <p>Command-line parser.</p> required"},{"location":"core/cli_spoutctl/#cli.spoutctl.SpoutCtl.create_spout","title":"<code>create_spout(output_type, state_type, **kwargs)</code>","text":"<p>Create a spout of a specific type.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>str</code> <p>The type of output (\"batch\" or \"streaming\").</p> required <code>state_type</code> <code>str</code> <p>The type of state manager (\"in_memory\", \"redis\", \"postgres\", or \"dynamodb\").</p> required <code>**kwargs</code> <p>Additional keyword arguments for initializing the spout. <pre><code>Keyword Arguments:\n    Batch output:\n    - output_folder (str): The directory where output files should be stored temporarily.\n    - output_s3_bucket (str): The name of the S3 bucket for output storage.\n    - output_s3_folder (str): The S3 folder for output storage.\n    Streaming output:\n    - output_kafka_topic (str): Kafka output topic for streaming spouts.\n    - output_kafka_cluster_connection_string (str): Kafka connection string for streaming spouts.\n    Stream to Batch output:\n    - output_folder (str): The directory where output files should be stored temporarily.\n    - output_s3_bucket (str): The name of the S3 bucket for output storage.\n    - output_s3_folder (str): The S3 folder for output storage.\n    - buffer_size (int): Number of messages to buffer.\n    Redis state manager config:\n    - redis_host (str): The host address for the Redis server.\n    - redis_port (int): The port number for the Redis server.\n    - redis_db (int): The Redis database to be used.\n    Postgres state manager config:\n    - postgres_host (str): The host address for the PostgreSQL server.\n    - postgres_port (int): The port number for the PostgreSQL server.\n    - postgres_user (str): The username for the PostgreSQL server.\n    - postgres_password (str): The password for the PostgreSQL server.\n    - postgres_database (str): The PostgreSQL database to be used.\n    - postgres_table (str): The PostgreSQL table to be used.\n    DynamoDB state manager config:\n    - dynamodb_table_name (str): The name of the DynamoDB table.\n    - dynamodb_region_name (str): The AWS region for DynamoDB.\n    Prometheus state manager config:\n    - prometheus_gateway (str): The push gateway for Prometheus metrics.\n</code></pre></p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Spout</code> <code>Spout</code> <p>The created spout.</p>"},{"location":"core/cli_spoutctl/#cli.spoutctl.SpoutCtl.execute_spout","title":"<code>execute_spout(spout, method_name, *args, **kwargs)</code>","text":"<p>Execute a method of a spout.</p> <p>Parameters:</p> Name Type Description Default <code>spout</code> <code>Spout</code> <p>The spout to execute.</p> required <code>method_name</code> <code>str</code> <p>The name of the method to execute.</p> required <code>*args</code> <p>Positional arguments to pass to the method.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments to pass to the method.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <p>The result of the method.</p>"},{"location":"core/cli_spoutctl/#cli.spoutctl.SpoutCtl.run","title":"<code>run(args)</code>","text":"<p>Run the command-line interface.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>argparse.Namespace</code> <p>Parsed command-line arguments.</p> required"},{"location":"core/cli_yamlctl/","title":"YamlCtl","text":"<p>Control spouts and bolts defined in a YAML file</p>"},{"location":"core/cli_yamlctl/#cli.yamlctl.YamlCtl","title":"<code>YamlCtl</code>","text":"<p>Command-line interface for managing spouts and bolts based on a YAML configuration.</p> <p>The YamlCtl class provides methods to run specific or all spouts and bolts defined in a YAML file. The YAML file's structure is defined by the Geniusfile schema.</p> <p>Example YAML structure: <pre><code>version: \"1\"\nspouts:\n  spout_name1:\n    name: \"spout1\"\n    method: \"method_name\"\n    ...\nbolts:\n  bolt_name1:\n    name: \"bolt1\"\n    method: \"method_name\"\n    ...\n</code></pre></p> <p>Attributes:</p> Name Type Description <code>geniusfile</code> <code>Geniusfile</code> <p>Parsed YAML configuration.</p> <code>spout_ctls</code> <code>Dict[str, SpoutCtl]</code> <p>Dictionary of SpoutCtl instances.</p> <code>bolt_ctls</code> <code>Dict[str, BoltCtl]</code> <p>Dictionary of BoltCtl instances.</p>"},{"location":"core/cli_yamlctl/#cli.yamlctl.YamlCtl.__init__","title":"<code>__init__(spout_ctls, bolt_ctls)</code>","text":"<p>Initialize YamlCtl with the path to the YAML file and control instances for spouts and bolts.</p> <p>Parameters:</p> Name Type Description Default <code>spout_ctls</code> <code>Dict[str, SpoutCtl]</code> <p>Dictionary of SpoutCtl instances.</p> required <code>bolt_ctls</code> <code>Dict[str, BoltCtl]</code> <p>Dictionary of BoltCtl instances.</p> required"},{"location":"core/cli_yamlctl/#cli.yamlctl.YamlCtl.create_parser","title":"<code>create_parser(parser)</code>","text":"<p>Create and return the command-line parser for managing spouts and bolts.</p>"},{"location":"core/cli_yamlctl/#cli.yamlctl.YamlCtl.resolve_reference","title":"<code>resolve_reference(input_type, ref_name)</code>","text":"<p>Resolve the reference of a bolt's input based on the input type (spout or bolt).</p> <p>Parameters:</p> Name Type Description Default <code>input_type</code> <code>str</code> <p>Type of the input (\"spout\" or \"bolt\").</p> required <code>ref_name</code> <code>str</code> <p>Name of the spout or bolt to refer to.</p> required <p>Returns:</p> Name Type Description <code>Output</code> <p>The output data of the referred spout or bolt.</p>"},{"location":"core/cli_yamlctl/#cli.yamlctl.YamlCtl.run","title":"<code>run(args)</code>","text":"<p>Run the command-line interface for managing spouts and bolts based on provided arguments. Please note that there is no ordering of the spouts and bolts in the YAML configuration. Each spout and bolt is an independent entity even when connected together.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>argparse.Namespace</code> <p>Parsed command-line arguments.</p> required"},{"location":"core/cli_yamlctl/#cli.yamlctl.YamlCtl.run_bolt","title":"<code>run_bolt(bolt_name)</code>","text":"<p>Run a specific bolt based on its name.</p> <p>Parameters:</p> Name Type Description Default <code>bolt_name</code> <code>str</code> <p>Name of the bolt to run.</p> required"},{"location":"core/cli_yamlctl/#cli.yamlctl.YamlCtl.run_bolts","title":"<code>run_bolts()</code>","text":"<p>Run all bolts defined in the YAML configuration.</p>"},{"location":"core/cli_yamlctl/#cli.yamlctl.YamlCtl.run_spout","title":"<code>run_spout(spout_name)</code>","text":"<p>Run a specific spout based on its name.</p> <p>Parameters:</p> Name Type Description Default <code>spout_name</code> <code>str</code> <p>Name of the spout to run.</p> required"},{"location":"core/cli_yamlctl/#cli.yamlctl.YamlCtl.run_spouts","title":"<code>run_spouts()</code>","text":"<p>Run all spouts defined in the YAML configuration.</p>"},{"location":"core/config/","title":"Encironment Configuration","text":""},{"location":"core/core_bolt/","title":"Bolt","text":"<p>Core Bolt class</p>"},{"location":"core/core_bolt/#core.bolt.Bolt","title":"<code>Bolt</code>","text":"<p>             Bases: <code>Task</code></p> <p>Base class for all bolts.</p> <p>A bolt is a component that consumes streams of data, processes them, and possibly emits new data streams.</p>"},{"location":"core/core_bolt/#core.bolt.Bolt.__call__","title":"<code>__call__(method_name, *args, **kwargs)</code>","text":"<p>Execute a method locally and manage the state.</p> <p>Parameters:</p> Name Type Description Default <code>method_name</code> <code>str</code> <p>The name of the method to execute.</p> required <code>*args</code> <p>Positional arguments to pass to the method.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments to pass to the method. Keyword Arguments:     - Additional keyword arguments specific to the method.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The result of the method.</p>"},{"location":"core/core_bolt/#core.bolt.Bolt.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>The <code>Bolt</code> class is a base class for all bolts in the given context. It inherits from the <code>Task</code> class and provides methods for executing tasks both locally and remotely, as well as managing their state, with state management options including in-memory, Redis, PostgreSQL, and DynamoDB, and input and output data for  batch, streaming, stream-to-batch, and batch-to-streaming.</p> <p>The <code>Bolt</code> class uses the <code>Input</code>, <code>Output</code> and <code>State</code> classes, which are abstract base classes for managing input data, output data and states, respectively. The <code>Input</code> and <code>Output</code> classes each have two subclasses: <code>StreamingInput</code>, <code>BatchInput</code>, <code>StreamingOutput</code> and <code>BatchOutput</code>, which manage streaming and batch input and output data, respectively. The <code>State</code> class is used to get and set state, and it has several subclasses for different types of state managers.</p> <p>The <code>Bolt</code> class also uses the <code>ECSManager</code> and <code>K8sManager</code> classes in the <code>execute_remote</code> method, which are used to manage tasks on Amazon ECS and Kubernetes, respectively.</p> Usage <ul> <li>Create an instance of the Bolt class by providing an Input object, an Output object and a State object.</li> <li>The Input object specifies the input data for the bolt.</li> <li>The Output object specifies the output data for the bolt.</li> <li>The State object handles the management of the bolt's state.</li> </ul> Example <p>input = Input(...) output = Output(...) state = State(...) bolt = Bolt(input, output, state)</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Input</code> <p>The input data.</p> required <code>output</code> <code>Output</code> <p>The output data.</p> required <code>state</code> <code>State</code> <p>The state manager.</p> required"},{"location":"core/core_bolt/#core.bolt.Bolt.create","title":"<code>create(klass, input_type, output_type, state_type, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a bolt of a specific type.</p> <p>This static method is used to create a bolt of a specific type. It takes in an input type, an output type, a state type, and additional keyword arguments for initializing the bolt.</p> <p>The method creates the input, output, and state manager based on the provided types, and then creates and returns a bolt using these configurations.</p> <p>Parameters:</p> Name Type Description Default <code>klass</code> <code>type</code> <p>The Bolt class to create.</p> required <code>input_type</code> <code>str</code> <p>The type of input (\"batch\" or \"streaming\").</p> required <code>output_type</code> <code>str</code> <p>The type of output (\"batch\" or \"streaming\").</p> required <code>state_type</code> <code>str</code> <p>The type of state manager (\"in_memory\", \"redis\", \"postgres\", or \"dynamodb\").</p> required <code>**kwargs</code> <p>Additional keyword arguments for initializing the bolt. <pre><code>Keyword Arguments:\n    Batch input:\n    - input_folder (str): The input folder argument.\n    - input_s3_bucket (str): The input bucket argument.\n    - input_s3_folder (str): The input S3 folder argument.\n    Batch outupt config:\n    - output_folder (str): The output folder argument.\n    - output_s3_bucket (str): The output bucket argument.\n    - output_s3_folder (str): The output S3 folder argument.\n    Streaming input:\n    - input_kafka_cluster_connection_string (str): The input Kafka servers argument.\n    - input_kafka_topic (str): The input kafka topic argument.\n    - input_kafka_consumer_group_id (str): The Kafka consumer group id.\n    Streaming output:\n    - output_kafka_cluster_connection_string (str): The output Kafka servers argument.\n    - output_kafka_topic (str): The output kafka topic argument.\n    Stream-to-Batch input:\n    - buffer_size (int): Number of messages to buffer.\n    - input_kafka_cluster_connection_string (str): The input Kafka servers argument.\n    - input_kafka_topic (str): The input kafka topic argument.\n    - input_kafka_consumer_group_id (str): The Kafka consumer group id.\n    Batch-to-Streaming input:\n    - buffer_size (int): Number of messages to buffer.\n    - input_folder (str): The input folder argument.\n    - input_s3_bucket (str): The input bucket argument.\n    - input_s3_folder (str): The input S3 folder argument.\n    Stream-to-Batch output:\n    - buffer_size (int): Number of messages to buffer.\n    - output_folder (str): The output folder argument.\n    - output_s3_bucket (str): The output bucket argument.\n    - output_s3_folder (str): The output S3 folder argument.\n    Redis state manager config:\n    - redis_host (str): The Redis host argument.\n    - redis_port (str): The Redis port argument.\n    - redis_db (str): The Redis database argument.\n    Postgres state manager config:\n    - postgres_host (str): The PostgreSQL host argument.\n    - postgres_port (str): The PostgreSQL port argument.\n    - postgres_user (str): The PostgreSQL user argument.\n    - postgres_password (str): The PostgreSQL password argument.\n    - postgres_database (str): The PostgreSQL database argument.\n    - postgres_table (str): The PostgreSQL table argument.\n    DynamoDB state manager config:\n    - dynamodb_table_name (str): The DynamoDB table name argument.\n    - dynamodb_region_name (str): The DynamoDB region name argument.\n    Prometheus state manager config:\n    - prometheus_gateway (str): The push gateway for Prometheus metrics.\n</code></pre></p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Bolt</code> <code>Bolt</code> <p>The created bolt.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid input type, output type, or state type is provided.</p>"},{"location":"core/core_data_batch_input/","title":"Batch data input","text":"<p>Batch input manager</p>"},{"location":"core/core_data_batch_input/#core.data.batch_input.BatchInput","title":"<code>BatchInput</code>","text":"<p>             Bases: <code>Input</code></p> <p>\ud83d\udcc1 BatchInput: Manages batch input data.</p> <p>Attributes:</p> Name Type Description <code>input_folder</code> <code>str</code> <p>Folder to read input files.</p> <code>bucket</code> <code>str</code> <p>S3 bucket name.</p> <code>s3_folder</code> <code>str</code> <p>Folder within the S3 bucket.</p> <p>Usage: <pre><code>config = BatchInput(\"/path/to/input\", \"my_bucket\", \"s3/folder\")\nfiles = list(config.list_files())\ncontent = config.read_file(\"example.txt\")\n</code></pre></p> <p>Raises:</p> Type Description <code>FileNotExistError</code> <p>If the file does not exist.</p>"},{"location":"core/core_data_batch_input/#core.data.batch_input.BatchInput.__init__","title":"<code>__init__(input_folder, bucket, s3_folder)</code>","text":"<p>\ud83d\udee0 Initialize a new batch input data.</p> <p>Parameters:</p> Name Type Description Default <code>input_folder</code> <code>str</code> <p>Folder to read input files from.</p> required <code>bucket</code> <code>str</code> <p>S3 bucket name.</p> required <code>s3_folder</code> <code>str</code> <p>Folder within the S3 bucket.</p> required"},{"location":"core/core_data_batch_input/#core.data.batch_input.BatchInput.copy_from_remote","title":"<code>copy_from_remote()</code>","text":"<p>\ud83d\udd04 Copy contents from a given S3 bucket and location to the input folder.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If no input folder is specified.</p>"},{"location":"core/core_data_batch_input/#core.data.batch_input.BatchInput.copy_to_remote","title":"<code>copy_to_remote(filename, bucket, s3_folder)</code>","text":"<p>\ud83d\udce4 Copies a file to a remote S3 bucket.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The name of the file to copy.</p> required <code>bucket</code> <code>str</code> <p>The name of the S3 bucket.</p> required <code>s3_folder</code> <code>str</code> <p>The folder within the S3 bucket.</p> required <p>Raises:</p> Type Description <code>FileNotExistError</code> <p>If the file does not exist.</p>"},{"location":"core/core_data_batch_input/#core.data.batch_input.BatchInput.delete_file","title":"<code>delete_file(filename)</code>","text":"<p>\ud83d\uddd1 Deletes a file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The name of the file to delete.</p> required <p>Raises:</p> Type Description <code>FileNotExistError</code> <p>If the file does not exist.</p>"},{"location":"core/core_data_batch_input/#core.data.batch_input.BatchInput.get","title":"<code>get()</code>","text":"<p>\ud83d\udce5 Returns the input folder path.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The path to the input folder.</p>"},{"location":"core/core_data_batch_input/#core.data.batch_input.BatchInput.list_files","title":"<code>list_files(start=None, limit=None)</code>","text":"<p>\ud83d\udccb Lists all files in the input folder with optional pagination.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>Optional[int]</code> <p>The starting index for pagination.</p> <code>None</code> <code>limit</code> <code>Optional[int]</code> <p>The maximum number of files to return.</p> <code>None</code> <p>Yields:</p> Name Type Description <code>str</code> <code>str</code> <p>The next file path in the input folder.</p>"},{"location":"core/core_data_batch_input/#core.data.batch_input.BatchInput.read_file","title":"<code>read_file(filename)</code>","text":"<p>\ud83d\udcd6 Reads the content of a file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The name of the file to read.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The content of the file.</p> <p>Raises:</p> Type Description <code>FileNotExistError</code> <p>If the file does not exist.</p>"},{"location":"core/core_data_batch_input/#core.data.batch_input.BatchInput.validate_file","title":"<code>validate_file(filename)</code>","text":"<p>\u2705 Validates if the file exists and is a file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The name of the file to validate.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the file is valid, False otherwise.</p>"},{"location":"core/core_data_batch_input/#core.data.batch_input.FileNotExistError","title":"<code>FileNotExistError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>\u274c Custom exception for file not existing.</p>"},{"location":"core/core_data_batch_output/","title":"Batch data output","text":"<p>Batch output manager</p>"},{"location":"core/core_data_batch_output/#core.data.batch_output.BatchOutput","title":"<code>BatchOutput</code>","text":"<p>             Bases: <code>Output</code></p> <p>\ud83d\udcc1 BatchOutput: Manages batch output data.</p> <p>Attributes:</p> Name Type Description <code>output_folder</code> <code>str</code> <p>Folder to save output files.</p> <code>bucket</code> <code>str</code> <p>S3 bucket name.</p> <code>s3_folder</code> <code>str</code> <p>Folder within the S3 bucket.</p> <p>Usage: <pre><code>config = BatchOutput(\"/path/to/output\", \"my_bucket\", \"s3/folder\")\nconfig.save({\"key\": \"value\"}, \"example.json\")\nfiles = config.list_files()\ncontent = config.read_file(\"example.json\")\n</code></pre></p>"},{"location":"core/core_data_batch_output/#core.data.batch_output.BatchOutput.__init__","title":"<code>__init__(output_folder, bucket, s3_folder)</code>","text":"<p>Initialize a new batch output data.</p> <p>Parameters:</p> Name Type Description Default <code>output_folder</code> <code>str</code> <p>Folder to save output files.</p> required <code>bucket</code> <code>str</code> <p>S3 bucket name.</p> required <code>s3_folder</code> <code>str</code> <p>Folder within the S3 bucket.</p> required"},{"location":"core/core_data_batch_output/#core.data.batch_output.BatchOutput.copy_file_to_remote","title":"<code>copy_file_to_remote(filename)</code>","text":"<p>\u2601\ufe0f Copy a specific file from the output folder to the S3 bucket.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The name of the file to copy.</p> required"},{"location":"core/core_data_batch_output/#core.data.batch_output.BatchOutput.copy_to_remote","title":"<code>copy_to_remote()</code>","text":"<p>\u2601\ufe0f Recursively copy all files and directories from the output folder to a given S3 bucket and folder.</p>"},{"location":"core/core_data_batch_output/#core.data.batch_output.BatchOutput.delete_file","title":"<code>delete_file(filename)</code>","text":"<p>\ud83d\uddd1\ufe0f Delete a file from the output folder.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The name of the file to delete.</p> required"},{"location":"core/core_data_batch_output/#core.data.batch_output.BatchOutput.flush","title":"<code>flush()</code>","text":"<p>\ud83d\udd04 Flush the output by copying all files and directories from the output folder to a given S3 bucket and folder.</p>"},{"location":"core/core_data_batch_output/#core.data.batch_output.BatchOutput.list_files","title":"<code>list_files()</code>","text":"<p>\ud83d\udcdc List all files in the output folder.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>List[str]</code> <p>The list of files in the output folder.</p>"},{"location":"core/core_data_batch_output/#core.data.batch_output.BatchOutput.read_file","title":"<code>read_file(filename)</code>","text":"<p>\ud83d\udcd6 Read a file from the output folder.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The name of the file to read.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The contents of the file.</p>"},{"location":"core/core_data_batch_output/#core.data.batch_output.BatchOutput.save","title":"<code>save(data, filename=None)</code>","text":"<p>\ud83d\udcbe Save data to a file in the output folder.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The data to save.</p> required <code>filename</code> <code>str</code> <p>The filename to use when saving the data to a file.</p> <code>None</code>"},{"location":"core/core_data_batch_to_stream_input/","title":"Batch data input behaving as streaming input","text":""},{"location":"core/core_data_batch_to_stream_input/#core.data.batch_to_stream_input.BatchToStreamingInput","title":"<code>BatchToStreamingInput</code>","text":"<p>             Bases: <code>StreamingInput</code>, <code>BatchInput</code></p> <p>\ud83d\udd04 BatchToStreamingInput: Manages converting batch data to streaming input.</p> Inherits <p>StreamingInput: For Kafka streaming capabilities. BatchInput: For batch-like file operations.</p> <p>Usage: <pre><code>config = BatchToStreamingInput(\"my_topic\", \"localhost:9094\", \"/path/to/input\", \"my_bucket\", \"s3/folder\")\niterator = config.stream_batch(\"example.json\")\nfor message in iterator:\nprint(message)\n</code></pre></p> <p>Note: - Ensure the Kafka cluster is running and accessible.</p>"},{"location":"core/core_data_batch_to_stream_input/#core.data.batch_to_stream_input.BatchToStreamingInput.__init__","title":"<code>__init__(input_folder, bucket, s3_folder, input_topic='', kafka_cluster_connection_string='', group_id='geniusrise')</code>","text":"<p>Initialize a new batch to streaming input data.</p> <p>Parameters:</p> Name Type Description Default <code>input_topic</code> <code>str</code> <p>Kafka topic to consume data.</p> <code>''</code> <code>kafka_cluster_connection_string</code> <code>str</code> <p>Kafka cluster connection string.</p> <code>''</code> <code>input_folder</code> <code>str</code> <p>Folder to read input files.</p> required <code>bucket</code> <code>str</code> <p>S3 bucket name.</p> required <code>s3_folder</code> <code>str</code> <p>Folder within the S3 bucket.</p> required <code>group_id</code> <code>str</code> <p>Kafka consumer group id. Defaults to \"geniusrise\".</p> <code>'geniusrise'</code>"},{"location":"core/core_data_batch_to_stream_input/#core.data.batch_to_stream_input.BatchToStreamingInput.ack","title":"<code>ack()</code>","text":"<p>\u2705 Acknowledge the processing of a Kafka message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>KafkaMessage</code> <p>The Kafka message to acknowledge.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If an error occurs while acknowledging the message.</p>"},{"location":"core/core_data_batch_to_stream_input/#core.data.batch_to_stream_input.BatchToStreamingInput.async_iterator","title":"<code>async_iterator()</code>  <code>async</code>","text":"<p>\ud83d\udd04 Asynchronous iterator method for yielding data from the Kafka consumer.</p> <p>Yields:</p> Name Type Description <code>KafkaMessage</code> <code>AsyncIterator[KafkaMessage]</code> <p>The next message from the Kafka consumer.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If no Kafka consumer is available.</p>"},{"location":"core/core_data_batch_to_stream_input/#core.data.batch_to_stream_input.BatchToStreamingInput.close","title":"<code>close()</code>","text":"<p>\ud83d\udeaa Close the Kafka consumer.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If an error occurs while closing the consumer.</p>"},{"location":"core/core_data_batch_to_stream_input/#core.data.batch_to_stream_input.BatchToStreamingInput.collect_metrics","title":"<code>collect_metrics()</code>","text":"<p>\ud83d\udcca Collect metrics related to the Kafka consumer.</p> <p>Returns:</p> Type Description <code>Dict[str, Union[int, float]]</code> <p>Dict[str, Union[int, float]]: A dictionary containing metrics like latency.</p>"},{"location":"core/core_data_batch_to_stream_input/#core.data.batch_to_stream_input.BatchToStreamingInput.filter_messages","title":"<code>filter_messages(filter_func)</code>","text":"<p>\ud83d\udd0d Filter messages from the Kafka consumer based on a filter function.</p> <p>Parameters:</p> Name Type Description Default <code>filter_func</code> <code>callable</code> <p>A function that takes a Kafka message and returns a boolean.</p> required <p>Yields:</p> Type Description <code>Iterator</code> <p>Kafka message: The next message from the Kafka consumer that passes the filter.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If no Kafka consumer is available or an error occurs.</p>"},{"location":"core/core_data_batch_to_stream_input/#core.data.batch_to_stream_input.BatchToStreamingInput.get","title":"<code>get()</code>","text":"<p>\ud83d\udd04 Convert batch data from a file to a streaming iterator.</p> <p>Yields:</p> Name Type Description <code>Any</code> <p>The next item from the batch data.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If no Kafka consumer is available or an error occurs.</p>"},{"location":"core/core_data_batch_to_stream_input/#core.data.batch_to_stream_input.BatchToStreamingInput.iterator","title":"<code>iterator()</code>","text":"<p>\ud83d\udd04 Iterator method for yielding data from the Kafka consumer.</p> <p>Yields:</p> Type Description <code>Iterator</code> <p>Kafka message: The next message from the Kafka consumer.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If no Kafka consumer is available.</p>"},{"location":"core/core_data_input/","title":"Data input","text":"<p>Input manager base class</p>"},{"location":"core/core_data_input/#core.data.input.Input","title":"<code>Input</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract class for managing input data.</p> <p>Attributes:</p> Name Type Description <code>log</code> <code>logging.Logger</code> <p>Logger instance.</p> <code>start_time</code> <code>float</code> <p>Start time for metrics.</p> <code>end_time</code> <code>float</code> <p>End time for metrics.</p>"},{"location":"core/core_data_input/#core.data.input.Input.__add__","title":"<code>__add__(*inputs)</code>","text":"<p>Compose multiple inputs.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Input</code> <p>Variable number of Input instances.</p> <code>()</code> <p>Returns:</p> Type Description <code>Union[bool, str]</code> <p>Union[bool, str]: True if successful, error message otherwise.</p>"},{"location":"core/core_data_input/#core.data.input.Input.collect_metrics","title":"<code>collect_metrics()</code>","text":"<p>Collect metrics like latency.</p> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: A dictionary containing metrics.</p>"},{"location":"core/core_data_input/#core.data.input.Input.compose","title":"<code>compose(*inputs)</code>","text":"<p>Compose multiple inputs.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Input</code> <p>Variable number of Input instances.</p> <code>()</code> <p>Returns:</p> Type Description <code>Union[bool, str]</code> <p>Union[bool, str]: True if successful, error message otherwise.</p>"},{"location":"core/core_data_input/#core.data.input.Input.get","title":"<code>get()</code>  <code>abstractmethod</code>","text":"<p>Abstract method to get data from the input source.</p> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The data from the input source.</p>"},{"location":"core/core_data_input/#core.data.input.Input.retryable_get","title":"<code>retryable_get()</code>","text":"<p>Retryable get method.</p> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The data from the input source.</p>"},{"location":"core/core_data_input/#core.data.input.Input.timeout","title":"<code>timeout(seconds)</code>","text":"<p>Timeout context manager.</p> <p>Parameters:</p> Name Type Description Default <code>seconds</code> <code>int</code> <p>Timeout in seconds.</p> required"},{"location":"core/core_data_input/#core.data.input.Input.validate_data","title":"<code>validate_data(data)</code>","text":"<p>Validate the incoming data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The data to validate.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if data is valid, False otherwise.</p>"},{"location":"core/core_data_output/","title":"Data output","text":"<p>Output manager base class</p>"},{"location":"core/core_data_output/#core.data.output.Output","title":"<code>Output</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for managing output data.</p>"},{"location":"core/core_data_output/#core.data.output.Output.flush","title":"<code>flush()</code>  <code>abstractmethod</code>","text":"<p>Flush the output. This method should be implemented by subclasses.</p>"},{"location":"core/core_data_output/#core.data.output.Output.save","title":"<code>save(data, filename=None)</code>  <code>abstractmethod</code>","text":"<p>Save data to a file or ingest it into a Kafka topic.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The data to save or ingest.</p> required <code>filename</code> <code>str</code> <p>The filename to use when saving the data to a file.</p> <code>None</code>"},{"location":"core/core_data_stream_to_batch_input/","title":"Streaming data input behaving as batch input","text":""},{"location":"core/core_data_stream_to_batch_input/#core.data.stream_to_batch_input.StreamToBatchInput","title":"<code>StreamToBatchInput</code>","text":"<p>             Bases: <code>StreamingInput</code>, <code>BatchInput</code></p> <p>\ud83d\udce6 StreamToBatchInput: Manages buffered streaming input data.</p> Inherits <p>StreamingInput: For Kafka streaming capabilities. BatchInput: For batch-like file operations.</p> <p>Attributes:</p> Name Type Description <code>buffer_size</code> <code>int</code> <p>Number of messages to buffer.</p> <code>temp_folder</code> <code>str</code> <p>Temporary folder to store buffered messages.</p> <p>Usage: <pre><code>config = StreamToBatchInput(\"my_topic\", \"localhost:9094\", buffer_size=100)\ntemp_folder = config.get()\n</code></pre></p> <p>Note: - Ensure the Kafka cluster is running and accessible. - Adjust the <code>group_id</code> if needed.</p>"},{"location":"core/core_data_stream_to_batch_input/#core.data.stream_to_batch_input.StreamToBatchInput.__init__","title":"<code>__init__(input_topic, kafka_cluster_connection_string, input_folder='', bucket='', s3_folder='', buffer_size=1000, group_id='geniusrise')</code>","text":"<p>\ud83d\udca5 Initialize a new buffered streaming input data.</p> <p>Parameters:</p> Name Type Description Default <code>input_topic</code> <code>str</code> <p>Kafka topic to consume data.</p> required <code>kafka_cluster_connection_string</code> <code>str</code> <p>Kafka cluster connection string.</p> required <code>buffer_size</code> <code>int</code> <p>Number of messages to buffer.</p> <code>1000</code> <code>group_id</code> <code>str</code> <p>Kafka consumer group id. Defaults to \"geniusrise\".</p> <code>'geniusrise'</code>"},{"location":"core/core_data_stream_to_batch_input/#core.data.stream_to_batch_input.StreamToBatchInput.buffer_messages","title":"<code>buffer_messages()</code>","text":"<p>\ud83d\udce5 Buffer messages from Kafka into local memory. ...</p>"},{"location":"core/core_data_stream_to_batch_input/#core.data.stream_to_batch_input.StreamToBatchInput.close","title":"<code>close()</code>","text":"<p>\ud83d\udeaa Close the Kafka consumer and clean up resources.</p>"},{"location":"core/core_data_stream_to_batch_input/#core.data.stream_to_batch_input.StreamToBatchInput.get","title":"<code>get()</code>","text":"<p>\ud83d\udce5 Get data from the input topic and buffer it into a temporary folder. ...</p>"},{"location":"core/core_data_stream_to_batch_input/#core.data.stream_to_batch_input.StreamToBatchInput.store_to_temp","title":"<code>store_to_temp(messages)</code>","text":"<p>\ud83d\udcbe Store buffered messages to temporary folder.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[KafkaMessage]</code> <p>List of buffered Kafka messages.</p> required"},{"location":"core/core_data_stream_to_batch_output/","title":"Streaming data output behaving as batch output","text":""},{"location":"core/core_data_stream_to_batch_output/#core.data.stream_to_batch_output.StreamToBatchOutput","title":"<code>StreamToBatchOutput</code>","text":"<p>             Bases: <code>StreamingOutput</code>, <code>BatchOutput</code></p> <p>\ud83d\udce6 StreamToBatchOutput: Manages buffered streaming output data.</p> Inherits <p>StreamingOutput: For Kafka streaming capabilities. BatchOutput: For batch-like file operations.</p> <p>Attributes:</p> Name Type Description <code>buffer_size</code> <code>int</code> <p>Number of messages to buffer.</p> <code>buffered_messages</code> <code>List[Any]</code> <p>List of buffered messages.</p> <p>Usage: <pre><code>config = StreamToBatchOutput(\"my_topic\", \"localhost:9094\", \"/path/to/output\", \"my_bucket\", \"s3/folder\", buffer_size=100)\nconfig.save({\"key\": \"value\"})\nconfig.flush()\n</code></pre></p>"},{"location":"core/core_data_stream_to_batch_output/#core.data.stream_to_batch_output.StreamToBatchOutput.__init__","title":"<code>__init__(output_folder, bucket, s3_folder, buffer_size, output_topic='', kafka_servers='')</code>","text":"<p>\ud83d\udca5 Initialize a new buffered streaming output data.</p> <p>Parameters:</p> Name Type Description Default <code>output_topic</code> <code>str</code> <p>Kafka topic to ingest data.</p> <code>''</code> <code>kafka_servers</code> <code>str</code> <p>Kafka bootstrap servers.</p> <code>''</code> <code>output_folder</code> <code>str</code> <p>Folder to save output files.</p> required <code>bucket</code> <code>str</code> <p>S3 bucket name.</p> required <code>s3_folder</code> <code>str</code> <p>Folder within the S3 bucket.</p> required <code>buffer_size</code> <code>int</code> <p>Number of messages to buffer.</p> required"},{"location":"core/core_data_stream_to_batch_output/#core.data.stream_to_batch_output.StreamToBatchOutput.flush","title":"<code>flush()</code>","text":"<p>\ud83d\udd04 Flush the output by saving buffered messages to a file and copying it to S3.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If no Kafka producer is available or an error occurs.</p>"},{"location":"core/core_data_stream_to_batch_output/#core.data.stream_to_batch_output.StreamToBatchOutput.save","title":"<code>save(data, filename=None)</code>","text":"<p>\ud83d\udce4 Buffer data into local memory until buffer size is reached.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The data to buffer.</p> required <code>filename</code> <code>str</code> <p>This argument is ignored for buffered streaming outputs.</p> <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If no Kafka producer is available or an error occurs.</p>"},{"location":"core/core_data_streaming_input/","title":"Streaming data input","text":"<p>Streaming input manager</p>"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.KafkaConnectionError","title":"<code>KafkaConnectionError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>\u274c Custom exception for kafka connection problems.</p>"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.StreamingInput","title":"<code>StreamingInput</code>","text":"<p>             Bases: <code>Input</code></p> <p>\ud83d\udce1 StreamingInput: Manages streaming input data.</p> <p>Attributes:</p> Name Type Description <code>input_topic</code> <code>str</code> <p>Kafka topic to consume data.</p> <code>consumer</code> <code>KafkaConsumer</code> <p>Kafka consumer for consuming data.</p> <p>Usage: <pre><code>config = StreamingInput(\"my_topic\", \"localhost:9094\")\nfor message in config.iterator():\nprint(message.value)\n</code></pre></p> <p>Note: - Ensure the Kafka cluster is running and accessible. - Adjust the <code>group_id</code> if needed.</p>"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.StreamingInput.__init__","title":"<code>__init__(input_topic, kafka_cluster_connection_string, group_id='geniusrise', **kwargs)</code>","text":"<p>\ud83d\udca5 Initialize a new streaming input data.</p> <p>Parameters:</p> Name Type Description Default <code>input_topic</code> <code>str</code> <p>Kafka topic to consume data.</p> required <code>kafka_cluster_connection_string</code> <code>str</code> <p>Kafka cluster connection string.</p> required <code>group_id</code> <code>str</code> <p>Kafka consumer group id. Defaults to \"geniusrise\".</p> <code>'geniusrise'</code>"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.StreamingInput.__iter__","title":"<code>__iter__()</code>","text":"<p>\ud83d\udd04 Make the class iterable.</p>"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.StreamingInput.__next__","title":"<code>__next__()</code>","text":"<p>\ud83d\udd25 Get the next message from the Kafka consumer.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If no Kafka consumer is available or an error occurs.</p>"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.StreamingInput.ack","title":"<code>ack()</code>","text":"<p>\u2705 Acknowledge the processing of a Kafka message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>KafkaMessage</code> <p>The Kafka message to acknowledge.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If an error occurs while acknowledging the message.</p>"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.StreamingInput.async_iterator","title":"<code>async_iterator()</code>  <code>async</code>","text":"<p>\ud83d\udd04 Asynchronous iterator method for yielding data from the Kafka consumer.</p> <p>Yields:</p> Name Type Description <code>KafkaMessage</code> <code>AsyncIterator[KafkaMessage]</code> <p>The next message from the Kafka consumer.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If no Kafka consumer is available.</p>"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.StreamingInput.close","title":"<code>close()</code>","text":"<p>\ud83d\udeaa Close the Kafka consumer.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If an error occurs while closing the consumer.</p>"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.StreamingInput.collect_metrics","title":"<code>collect_metrics()</code>","text":"<p>\ud83d\udcca Collect metrics related to the Kafka consumer.</p> <p>Returns:</p> Type Description <code>Dict[str, Union[int, float]]</code> <p>Dict[str, Union[int, float]]: A dictionary containing metrics like latency.</p>"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.StreamingInput.commit","title":"<code>commit()</code>","text":"<p>\u2705 Manually commit offsets.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If an error occurs while committing offsets.</p>"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.StreamingInput.filter_messages","title":"<code>filter_messages(filter_func)</code>","text":"<p>\ud83d\udd0d Filter messages from the Kafka consumer based on a filter function.</p> <p>Parameters:</p> Name Type Description Default <code>filter_func</code> <code>callable</code> <p>A function that takes a Kafka message and returns a boolean.</p> required <p>Yields:</p> Type Description <code>Iterator</code> <p>Kafka message: The next message from the Kafka consumer that passes the filter.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If no Kafka consumer is available or an error occurs.</p>"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.StreamingInput.get","title":"<code>get()</code>","text":"<p>\ud83d\udce5 Get data from the input topic.</p> <p>Returns:</p> Name Type Description <code>KafkaConsumer</code> <code>KafkaConsumer</code> <p>The Kafka consumer.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If no input source or consumer is specified.</p>"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.StreamingInput.iterator","title":"<code>iterator()</code>","text":"<p>\ud83d\udd04 Iterator method for yielding data from the Kafka consumer.</p> <p>Yields:</p> Type Description <code>Iterator</code> <p>Kafka message: The next message from the Kafka consumer.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If no Kafka consumer is available.</p>"},{"location":"core/core_data_streaming_output/","title":"Streaming data output","text":"<p>Streaming output manager</p>"},{"location":"core/core_data_streaming_output/#core.data.streaming_output.StreamingOutput","title":"<code>StreamingOutput</code>","text":"<p>             Bases: <code>Output</code></p> <p>\ud83d\udce1 StreamingOutput: Manages streaming output data.</p> <p>Attributes:</p> Name Type Description <code>output_topic</code> <code>str</code> <p>Kafka topic to ingest data.</p> <code>producer</code> <code>KafkaProducer</code> <p>Kafka producer for ingesting data.</p> <p>Usage: <pre><code>config = StreamingOutput(\"my_topic\", \"localhost:9094\")\nconfig.save({\"key\": \"value\"}, \"ignored_filename\")\nconfig.flush()\n</code></pre></p> <p>Note: - Ensure the Kafka cluster is running and accessible.</p>"},{"location":"core/core_data_streaming_output/#core.data.streaming_output.StreamingOutput.__init__","title":"<code>__init__(output_topic, kafka_servers)</code>","text":"<p>Initialize a new streaming output data.</p> <p>Parameters:</p> Name Type Description Default <code>output_topic</code> <code>str</code> <p>Kafka topic to ingest data.</p> required <code>kafka_servers</code> <code>str</code> <p>Kafka bootstrap servers.</p> required"},{"location":"core/core_data_streaming_output/#core.data.streaming_output.StreamingOutput.close","title":"<code>close()</code>","text":"<p>\ud83d\udeaa Close the Kafka producer.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If no Kafka producer is available.</p>"},{"location":"core/core_data_streaming_output/#core.data.streaming_output.StreamingOutput.flush","title":"<code>flush()</code>","text":"<p>\ud83d\udd04 Flush the output by flushing the Kafka producer.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If no Kafka producer is available.</p>"},{"location":"core/core_data_streaming_output/#core.data.streaming_output.StreamingOutput.partition_available","title":"<code>partition_available(partition)</code>","text":"<p>\ud83e\uddd0 Check if a partition is available in the Kafka topic.</p> <p>Parameters:</p> Name Type Description Default <code>partition</code> <code>int</code> <p>The partition to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the partition is available, False otherwise.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If no Kafka producer is available.</p>"},{"location":"core/core_data_streaming_output/#core.data.streaming_output.StreamingOutput.save","title":"<code>save(data, filename=None)</code>","text":"<p>\ud83d\udce4 Ingest data into the Kafka topic.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The data to ingest.</p> required <code>filename</code> <code>str</code> <p>This argument is ignored for streaming outputs.</p> <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If no Kafka producer is available or an error occurs.</p>"},{"location":"core/core_data_streaming_output/#core.data.streaming_output.StreamingOutput.save_bulk","title":"<code>save_bulk(messages)</code>","text":"<p>\ud83d\udce6 Send multiple messages at once to the Kafka topic.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list</code> <p>The messages to send.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If no Kafka producer is available or an error occurs.</p>"},{"location":"core/core_data_streaming_output/#core.data.streaming_output.StreamingOutput.save_to_partition","title":"<code>save_to_partition(value, partition)</code>","text":"<p>\ud83c\udfaf Send a message to a specific partition in the Kafka topic.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The value of the message.</p> required <code>partition</code> <code>int</code> <p>The partition to send the message to.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If no Kafka producer is available or an error occurs.</p>"},{"location":"core/core_data_streaming_output/#core.data.streaming_output.StreamingOutput.send_key_value","title":"<code>send_key_value(key, value)</code>","text":"<p>\ud83d\udd11 Send a message with a key to the Kafka topic.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Any</code> <p>The key of the message.</p> required <code>value</code> <code>Any</code> <p>The value of the message.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If no Kafka producer is available or an error occurs.</p>"},{"location":"core/core_spout/","title":"Spout","text":"<p>Core Spout class</p>"},{"location":"core/core_spout/#core.spout.Spout","title":"<code>Spout</code>","text":"<p>             Bases: <code>Task</code></p> <p>Base class for all spouts.</p>"},{"location":"core/core_spout/#core.spout.Spout.__call__","title":"<code>__call__(method_name, *args, **kwargs)</code>","text":"<p>Execute a method locally and manage the state.</p> <p>Parameters:</p> Name Type Description Default <code>method_name</code> <code>str</code> <p>The name of the method to execute.</p> required <code>*args</code> <p>Positional arguments to pass to the method.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments to pass to the method. Keyword Arguments:     - Additional keyword arguments specific to the method.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The result of the method.</p>"},{"location":"core/core_spout/#core.spout.Spout.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>The <code>Spout</code> class is a base class for all spouts in the given context. It inherits from the <code>Task</code> class and provides methods for executing tasks both locally and remotely, as well as managing their state, with state management options including in-memory, Redis, PostgreSQL, and DynamoDB, and output data for batch or streaming data.</p> <p>The <code>Spout</code> class uses the <code>Output</code> and <code>State</code> classes, which are abstract base  classes for managing output data and states, respectively. The <code>Output</code> class  has two subclasses: <code>StreamingOutput</code> and <code>BatchOutput</code>, which manage streaming and  batch output data, respectively. The <code>State</code> class is used to get and set state,  and it has several subclasses for different types of state managers.</p> <p>The <code>Spout</code> class also uses the <code>ECSManager</code> and <code>K8sManager</code> classes in the <code>execute_remote</code> method, which are used to manage tasks on Amazon ECS and Kubernetes, respectively.</p> Usage <ul> <li>Create an instance of the Spout class by providing an Output object and a State object.</li> <li>The Output object specifies the output data for the spout.</li> <li>The State object handles the management of the spout's state.</li> </ul> Example <p>output = Output(...) state = State(...) spout = Spout(output, state)</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Output</code> <p>The output data.</p> required <code>state</code> <code>State</code> <p>The state manager.</p> required"},{"location":"core/core_spout/#core.spout.Spout.create","title":"<code>create(klass, output_type, state_type, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a spout of a specific type.</p> <p>Parameters:</p> Name Type Description Default <code>klass</code> <code>type</code> <p>The Spout class to create.</p> required <code>output_type</code> <code>str</code> <p>The type of output (\"batch\" or \"streaming\").</p> required <code>state_type</code> <code>str</code> <p>The type of state manager (\"in_memory\", \"redis\", \"postgres\", or \"dynamodb\").</p> required <code>**kwargs</code> <p>Additional keyword arguments for initializing the spout. <pre><code>Keyword Arguments:\n    Batch output:\n    - output_folder (str): The directory where output files should be stored temporarily.\n    - output_s3_bucket (str): The name of the S3 bucket for output storage.\n    - output_s3_folder (str): The S3 folder for output storage.\n    Streaming output:\n    - output_kafka_topic (str): Kafka output topic for streaming spouts.\n    - output_kafka_cluster_connection_string (str): Kafka connection string for streaming spouts.\n    Stream to Batch output:\n    - output_folder (str): The directory where output files should be stored temporarily.\n    - output_s3_bucket (str): The name of the S3 bucket for output storage.\n    - output_s3_folder (str): The S3 folder for output storage.\n    - buffer_size (int): Number of messages to buffer.\n    Redis state manager config:\n    - redis_host (str): The host address for the Redis server.\n    - redis_port (int): The port number for the Redis server.\n    - redis_db (int): The Redis database to be used.\n    Postgres state manager config:\n    - postgres_host (str): The host address for the PostgreSQL server.\n    - postgres_port (int): The port number for the PostgreSQL server.\n    - postgres_user (str): The username for the PostgreSQL server.\n    - postgres_password (str): The password for the PostgreSQL server.\n    - postgres_database (str): The PostgreSQL database to be used.\n    - postgres_table (str): The PostgreSQL table to be used.\n    DynamoDB state manager config:\n    - dynamodb_table_name (str): The name of the DynamoDB table.\n    - dynamodb_region_name (str): The AWS region for DynamoDB.\n    Prometheus state manager config:\n    - prometheus_gateway (str): The push gateway for Prometheus metrics.\n</code></pre></p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Spout</code> <code>Spout</code> <p>The created spout.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid output type or state type is provided.</p>"},{"location":"core/core_state_base/","title":"State","text":"<p>Base class for task state mnager</p>"},{"location":"core/core_state_base/#core.state.base.State","title":"<code>State</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for a state manager.</p> <p>A state manager is responsible for getting and setting state, capturing metrics, and logging. It provides an interface for state management and also captures various system metrics.</p> <p>Attributes:</p> Name Type Description <code>read_ops</code> <code>Counter</code> <p>Counter for read operations.</p> <code>write_ops</code> <code>Counter</code> <p>Counter for write operations.</p> <code>process_time</code> <code>Summary</code> <p>Summary for time spent in processing.</p> <code>cpu_usage</code> <code>Gauge</code> <p>Gauge for CPU usage.</p> <code>memory_usage</code> <code>Gauge</code> <p>Gauge for memory usage.</p> <code>python_version</code> <code>str</code> <p>Python version.</p> <code>cpu_count</code> <code>str</code> <p>Count of CPUs visible.</p> <code>virtual_memory</code> <code>str</code> <p>virtual memory available.</p> <code>gpu_count</code> <code>str</code> <p>GPU count visible.</p> <code>gpu_memory</code> <code>str</code> <p>GPU memory available.</p> <code>buffer</code> <code>Dict[str, Any]</code> <p>Buffer for state data.</p> <code>log</code> <code>logging.Logger</code> <p>Logger for capturing logs.</p>"},{"location":"core/core_state_base/#core.state.base.State.__del__","title":"<code>__del__()</code>","text":"<p>Destructor to flush the buffer before object deletion.</p> <p>This ensures that any buffered state data is not lost when the object is deleted.</p>"},{"location":"core/core_state_base/#core.state.base.State.capture_log","title":"<code>capture_log(log_entry)</code>","text":"<p>Capture log entries.</p> <p>This method captures log entries and timestamps, storing them in a buffer for later use.</p> <p>Parameters:</p> Name Type Description Default <code>log_entry</code> <code>str</code> <p>The log entry to capture.</p> required"},{"location":"core/core_state_base/#core.state.base.State.capture_metrics","title":"<code>capture_metrics()</code>","text":"<p>Capture system metrics.</p> <p>This method captures various system metrics like CPU usage, memory usage, etc., and stores them in a buffer.</p>"},{"location":"core/core_state_base/#core.state.base.State.capture_metrics_periodically","title":"<code>capture_metrics_periodically(interval=1)</code>","text":"<p>Periodically capture metrics.</p> <p>This method runs in a separate thread and captures system metrics at regular intervals.</p> <p>Parameters:</p> Name Type Description Default <code>interval</code> <code>int</code> <p>Time interval in seconds.</p> <code>1</code>"},{"location":"core/core_state_base/#core.state.base.State.flush_buffer","title":"<code>flush_buffer()</code>","text":"<p>Flush the buffer to the state storage.</p> <p>This method is responsible for writing the buffered state data to the underlying storage mechanism.</p>"},{"location":"core/core_state_base/#core.state.base.State.flush_metrics","title":"<code>flush_metrics()</code>","text":"<p>Flush the metrics buffer to the state storage.</p> <p>This method is responsible for writing the buffered metrics data to the underlying storage mechanism.</p>"},{"location":"core/core_state_base/#core.state.base.State.get","title":"<code>get(key)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to get the state associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to get the state for.</p> required <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>Optional[Dict[str, Any]]: The state associated with the key.</p>"},{"location":"core/core_state_base/#core.state.base.State.get_state","title":"<code>get_state(key)</code>","text":"<p>Get the state associated with a key and capture metrics.</p> <p>This method wraps the abstract <code>get</code> method to provide additional functionality like metrics capturing.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to get the state for.</p> required <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>Optional[Dict[str, Any]]: The state associated with the key.</p>"},{"location":"core/core_state_base/#core.state.base.State.set","title":"<code>set(key, value)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to set the state associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to set the state for.</p> required <code>value</code> <code>Dict[str, Any]</code> <p>The state to set.</p> required"},{"location":"core/core_state_base/#core.state.base.State.set_state","title":"<code>set_state(key, value)</code>","text":"<p>Set the state associated with a key and capture metrics.</p> <p>This method wraps the abstract <code>set</code> method to provide additional functionality like metrics capturing.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to set the state for.</p> required <code>value</code> <code>Dict[str, Any]</code> <p>The state to set.</p> required"},{"location":"core/core_state_base/#core.state.base.State.time_function","title":"<code>time_function(func, *args, **kwargs)</code>","text":"<p>Time the execution of a function and capture metrics.</p> <p>This method times the execution of a given function and captures the time spent in a Summary metric.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Any</code> <p>The function to time.</p> required <code>*args</code> <code>Any</code> <p>Positional arguments for the function.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments for the function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The result of the function execution.</p>"},{"location":"core/core_state_dynamo/","title":"DynamoDB State","text":"<p>State manager using dynamoDB</p>"},{"location":"core/core_state_dynamo/#core.state.dynamo.DynamoDBState","title":"<code>DynamoDBState</code>","text":"<p>             Bases: <code>State</code></p> <p>\ud83d\uddc4\ufe0f DynamoDBState: A state manager that stores state in DynamoDB.</p> <p>Attributes:</p> Name Type Description <code>dynamodb</code> <code>boto3.resources.factory.dynamodb.ServiceResource</code> <p>The DynamoDB service resource.</p> <code>table</code> <code>boto3.resources.factory.dynamodb.Table</code> <p>The DynamoDB table.</p> <p>Usage: <pre><code>manager = DynamoDBState(\"my_table\", \"us-west-1\")\nmanager.set_state(\"key123\", {\"status\": \"active\"})\nstate = manager.get_state(\"key123\")\nprint(state)  # Outputs: {\"status\": \"active\"}\n</code></pre></p> <p>Note: - Ensure DynamoDB is accessible and the table exists.</p>"},{"location":"core/core_state_dynamo/#core.state.dynamo.DynamoDBState.__init__","title":"<code>__init__(table_name, region_name)</code>","text":"<p>\ud83d\udca5 Initialize a new DynamoDB state manager.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>The name of the DynamoDB table.</p> required <code>region_name</code> <code>str</code> <p>The name of the AWS region.</p> required"},{"location":"core/core_state_dynamo/#core.state.dynamo.DynamoDBState.get","title":"<code>get(key)</code>","text":"<p>\ud83d\udcd6 Get the state associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to get the state for.</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>Optional[Dict]</code> <p>The state associated with the key, or None if not found.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there's an error accessing DynamoDB.</p>"},{"location":"core/core_state_dynamo/#core.state.dynamo.DynamoDBState.set","title":"<code>set(key, value)</code>","text":"<p>\ud83d\udcdd Set the state associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to set the state for.</p> required <code>value</code> <code>Dict</code> <p>The state to set.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If there's an error accessing DynamoDB.</p>"},{"location":"core/core_state_memory/","title":"In-memory State","text":"<p>State manager using local memory</p>"},{"location":"core/core_state_memory/#core.state.memory.InMemoryState","title":"<code>InMemoryState</code>","text":"<p>             Bases: <code>State</code></p> <p>\ud83e\udde0 InMemoryState: A state manager that stores state in memory.</p> <p>This manager is useful for temporary storage or testing purposes. Since it's in-memory, the data will be lost once the application stops.</p>"},{"location":"core/core_state_memory/#core.state.memory.InMemoryState--attributes","title":"Attributes:","text":"<ul> <li><code>store</code> (Dict[str, Dict]): The in-memory store for states.</li> </ul>"},{"location":"core/core_state_memory/#core.state.memory.InMemoryState--usage","title":"Usage:","text":"<pre><code>manager = InMemoryState()\nmanager.set_state(\"user123\", {\"status\": \"active\"})\nstate = manager.get_state(\"user123\")\nprint(state)  # Outputs: {\"status\": \"active\"}\n</code></pre> <p>Remember, this is an in-memory store. Do not use it for persistent storage!</p>"},{"location":"core/core_state_memory/#core.state.memory.InMemoryState.__init__","title":"<code>__init__()</code>","text":"<p>\ud83d\udca5 Initialize a new in-memory state manager.</p>"},{"location":"core/core_state_memory/#core.state.memory.InMemoryState.get","title":"<code>get(key)</code>","text":"<p>\ud83d\udcd6 Get the state associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to get the state for.</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>Optional[Dict]</code> <p>The state associated with the key, or None if not found.</p>"},{"location":"core/core_state_memory/#core.state.memory.InMemoryState.set","title":"<code>set(key, value)</code>","text":"<p>\ud83d\udcdd Set the state associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to set the state for.</p> required <code>value</code> <code>Dict</code> <p>The state to set.</p> required <p>Example: <pre><code>manager.set_state(\"user123\", {\"status\": \"active\"})\n</code></pre></p>"},{"location":"core/core_state_postgres/","title":"Postgres State","text":"<p>State manager using postgres database</p>"},{"location":"core/core_state_postgres/#core.state.postgres.PostgresState","title":"<code>PostgresState</code>","text":"<p>             Bases: <code>State</code></p> <p>\ud83d\uddc4\ufe0f PostgresState: A state manager that stores state in a PostgreSQL database.</p> <p>This manager provides a persistent storage solution using a PostgreSQL database.</p>"},{"location":"core/core_state_postgres/#core.state.postgres.PostgresState--attributes","title":"Attributes:","text":"<ul> <li><code>conn</code> (psycopg2.extensions.connection): The PostgreSQL connection.</li> </ul>"},{"location":"core/core_state_postgres/#core.state.postgres.PostgresState--usage","title":"Usage:","text":"<pre><code>manager = PostgresState(host=\"localhost\", port=5432, user=\"admin\", password=\"password\", database=\"mydb\")\nmanager.set_state(\"user123\", {\"status\": \"active\"})\nstate = manager.get_state(\"user123\")\nprint(state)  # Outputs: {\"status\": \"active\"}\n</code></pre> <p>Ensure PostgreSQL is accessible and the table exists.</p>"},{"location":"core/core_state_postgres/#core.state.postgres.PostgresState.__init__","title":"<code>__init__(host, port, user, password, database, table='geniusrise_state')</code>","text":"<p>\ud83d\udca5 Initialize a new PostgreSQL state manager.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The host of the PostgreSQL server.</p> required <code>port</code> <code>int</code> <p>The port of the PostgreSQL server.</p> required <code>user</code> <code>str</code> <p>The user to connect as.</p> required <code>password</code> <code>str</code> <p>The user's password.</p> required <code>database</code> <code>str</code> <p>The database to connect to.</p> required <code>table</code> <code>str</code> <p>The table to use. Defaults to \"geniusrise_state\".</p> <code>'geniusrise_state'</code>"},{"location":"core/core_state_postgres/#core.state.postgres.PostgresState.get","title":"<code>get(key)</code>","text":"<p>\ud83d\udcd6 Get the state associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to get the state for.</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>Optional[Dict]</code> <p>The state associated with the key, or None if not found.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there's an error accessing PostgreSQL.</p>"},{"location":"core/core_state_postgres/#core.state.postgres.PostgresState.set","title":"<code>set(key, value)</code>","text":"<p>\ud83d\udcdd Set the state associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to set the state for.</p> required <code>value</code> <code>Dict</code> <p>The state to set.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If there's an error accessing PostgreSQL.</p>"},{"location":"core/core_state_redis/","title":"Redis State","text":"<p>State manager using redis</p>"},{"location":"core/core_state_redis/#core.state.redis.RedisState","title":"<code>RedisState</code>","text":"<p>             Bases: <code>State</code></p> <p>\ud83d\uddc4\ufe0f RedisState: A state manager that stores state in Redis.</p> <p>This manager provides a fast, in-memory storage solution using Redis.</p>"},{"location":"core/core_state_redis/#core.state.redis.RedisState--attributes","title":"Attributes:","text":"<ul> <li><code>redis</code> (redis.Redis): The Redis connection.</li> </ul>"},{"location":"core/core_state_redis/#core.state.redis.RedisState--usage","title":"Usage:","text":"<pre><code>manager = RedisState(host=\"localhost\", port=6379, db=0)\nmanager.set_state(\"user123\", {\"status\": \"active\"})\nstate = manager.get_state(\"user123\")\nprint(state)  # Outputs: {\"status\": \"active\"}\n</code></pre> <p>Ensure Redis is accessible and running.</p>"},{"location":"core/core_state_redis/#core.state.redis.RedisState.__init__","title":"<code>__init__(host, port, db)</code>","text":"<p>\ud83d\udca5 Initialize a new Redis state manager.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The host of the Redis server.</p> required <code>port</code> <code>int</code> <p>The port of the Redis server.</p> required <code>db</code> <code>int</code> <p>The database number to connect to.</p> required"},{"location":"core/core_state_redis/#core.state.redis.RedisState.get","title":"<code>get(key)</code>","text":"<p>\ud83d\udcd6 Get the state associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to get the state for.</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>Optional[Dict]</code> <p>The state associated with the key, or None if not found.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there's an error accessing Redis.</p>"},{"location":"core/core_state_redis/#core.state.redis.RedisState.set","title":"<code>set(key, value)</code>","text":"<p>\ud83d\udcdd Set the state associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to set the state for.</p> required <code>value</code> <code>Dict</code> <p>The state to set.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If there's an error accessing Redis.</p>"},{"location":"core/core_task_base/","title":"Task","text":"<p>Base class for Task</p>"},{"location":"core/core_task_base/#core.task.base.Task","title":"<code>Task</code>","text":"<p>             Bases: <code>ABC</code></p> <p>\ud83d\udee0\ufe0f Task: Class for managing tasks.</p> <p>This class provides a foundation for creating and managing tasks. Each task has a unique identifier and can be associated with specific input and output data.</p>"},{"location":"core/core_task_base/#core.task.base.Task--attributes","title":"Attributes:","text":"<ul> <li><code>id</code> (uuid.UUID): Unique identifier for the task.</li> <li><code>input</code> (Input): Configuration for input data.</li> <li><code>output</code> (Output): Configuration for output data.</li> </ul>"},{"location":"core/core_task_base/#core.task.base.Task--usage","title":"Usage:","text":"<pre><code>task = Task()\ntask.execute(\"fetch_data\")\n</code></pre> <p>!!! note     Extend this class to implement specific task functionalities.</p>"},{"location":"core/core_task_base/#core.task.base.Task.__init__","title":"<code>__init__()</code>","text":"<p>Initialize a new task.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Input</code> <p>Configuration for input data.</p> required <code>output</code> <code>Output</code> <p>Configuration for output data.</p> required"},{"location":"core/core_task_base/#core.task.base.Task.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a string representation of the task.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string representation of the task.</p>"},{"location":"core/core_task_base/#core.task.base.Task.execute","title":"<code>execute(method_name, *args, **kwargs)</code>","text":"<p>\ud83d\ude80 Execute a given fetch_* method if it exists.</p> <p>Parameters:</p> Name Type Description Default <code>method_name</code> <code>str</code> <p>The name of the fetch_* method to execute.</p> required <code>*args</code> <p>Positional arguments to pass to the method.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments to pass to the method.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The result of the fetch_* method, or None if the method does not exist.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If the specified method doesn't exist.</p>"},{"location":"core/core_task_base/#core.task.base.Task.get_methods","title":"<code>get_methods()</code>  <code>staticmethod</code>","text":"<p>\ud83d\udcdc Get all the fetch_* methods and their parameters along with their default values and docstrings.</p> <p>Returns:</p> Type Description <code>List[Tuple[str, List[str], Optional[str]]]</code> <p>List[Tuple[str, List[str], str]]: A list of tuples, where each tuple contains the name of a fetch_* method,</p> <code>List[Tuple[str, List[str], Optional[str]]]</code> <p>a list of its parameters along with their default values, and its docstring.</p>"},{"location":"core/core_task_base/#core.task.base.Task.print_help","title":"<code>print_help()</code>  <code>staticmethod</code>","text":"<p>\ud83d\udda8\ufe0f Pretty print the fetch_* methods and their parameters along with their default values and docstrings. Also prints the class's docstring and init parameters.</p>"},{"location":"guides/404/","title":"WIP","text":"<p>Working on it! \ud83d\ude05</p>"},{"location":"guides/architecture/","title":"Architecture","text":""},{"location":"guides/architecture/#introduction","title":"Introduction","text":"<p>The Geniusrise framework is designed to provide a modular, scalable, and interoperable system for orchestrating machine learning workflows, particularly in the context of Large Language Models (LLMs). The architecture is built around the core concept of a <code>Task</code>, which represents a discrete unit of work. This document provides an overview of the architecture, detailing the primary components and their interactions.</p>"},{"location":"guides/architecture/#system-overview","title":"System Overview","text":"<p>The Geniusrise framework is composed of several key components:</p> <ol> <li>Tasks: The fundamental units of work.</li> <li>State Managers: Responsible for monitoring and managing the state of tasks.</li> <li>Data Managers: Oversee the input and output data associated with tasks.</li> <li>Model Managers: Handle model operations, ensuring efficient management.</li> <li>Runners: Wrappers for executing tasks on various platforms.</li> <li>Spouts and Bolts: Specialized tasks for data ingestion and processing.</li> </ol>"},{"location":"guides/architecture/#tasks","title":"Tasks","text":"<p>A task is the fundamental unit of work in the Geniusrise framework. It represents a specific operation or computation and can run for an arbitrary amount of time, performing any amount of work.</p> <pre>1bb1b68e-1886-4d56-9ec5-e48b73671cce</pre>"},{"location":"guides/architecture/#state-managers","title":"State Managers","text":"<p>State Managers play a pivotal role in maintaining the state of tasks. They ensure that the progress and status of tasks are tracked, especially in distributed environments. Geniusrise offers various types of State Managers:</p> <ol> <li>DynamoDBStateManager: Interfaces with Amazon DynamoDB.</li> <li>InMemoryStateManager: Maintains state within the application's memory.</li> <li>PostgresStateManager: Interfaces with PostgreSQL databases.</li> <li>RedisStateManager: Interfaces with Redis in-memory data structure store.</li> </ol> <p>State Managers store data in various locations, allowing organizations to connect dashboards to these storage systems for real-time monitoring and analytics. This centralized storage and reporting mechanism ensures that stakeholders have a unified view of task states.</p> <pre>0bed59e9-19bf-42d2-b905-caaa971e1f67</pre>"},{"location":"guides/architecture/#data-managers","title":"Data Managers","text":"<p>Data Managers are responsible for handling the input and output data for tasks. They implement various data operations methods that tasks can leverage to ingest or save data during their runs. Data Managers can be categorized based on their function and data processing type:</p> <ol> <li>BatchInputConfig: Manages batch input data.</li> <li>BatchOutputConfig: Manages batch output data.</li> <li>StreamingInputConfig: Manages streaming input data.</li> <li>StreamingOutputConfig: Manages streaming output data.</li> </ol> <p>Data Managers manage data partitioning for both batch and streaming data. By adhering to common data patterns, they enable the system's components to operate independently, fostering the creation of intricate networks of tasks. This independence, while allowing for flexibility and scalability, ensures that cascading failures in one component don't necessarily compromise the entire system.</p> <pre>10300a0f-7b70-4e93-95ab-7a8b95a6bf03</pre>"},{"location":"guides/architecture/#model-managers","title":"Model Managers","text":"<p>Model Managers oversee model operations, ensuring that models are saved, loaded, and managed. They can be of two primary types:</p> <ol> <li>S3ModelManager: Interfaces with Amazon S3 for model storage.</li> <li>WANDBModelManager: Interfaces with Weights &amp; Biases for model versioning.</li> <li>GitModelManager: Interfaces with Git repositories for versioning of models.</li> </ol> <pre>5993f57a-a422-43cc-9fe2-b958da7374e7</pre>"},{"location":"guides/architecture/#spouts-and-bolts","title":"Spouts and Bolts","text":"<p>At the heart of the Geniusrise framework are two primary component types: spouts and bolts.</p> <ol> <li> <p>Spouts: These are tasks responsible for ingesting data from various sources. Depending on the output type, spouts can either produce streaming output or batch output.</p> <ol> <li>Batch: Runs periodically, Produces data as a batch output.</li> <li>Stream: Runs forever, produces data into a streaming output.</li> </ol> </li> <li> <p>Bolts: Bolts are tasks that take in data, process it, and produce output. They can be categorized based on their input and output types:</p> <ol> <li>Stream-Stream: Reads streaming data and produces streaming output.</li> <li>Stream-Batch: Reads streaming data and produces batch output.</li> <li>Batch-Stream: Reads batch data and produces streaming output.</li> <li>Batch-Batch: Reads batch data and produces batch output.</li> </ol> </li> </ol> <pre>4a9795a2-3c74-4394-8fc2-844a7c11c6e7</pre>"},{"location":"guides/architecture/#runners","title":"Runners","text":"<p>Runners are the backbone of the Geniusrise framework, ensuring that tasks are executed seamlessly across various platforms. They encapsulate the environment and resources required for task execution, abstracting away the underlying complexities. Geniusrise offers the following runners:</p> <ol> <li>Local Runner: Executes tasks directly on a local machine, ideal for development and testing.</li> <li>Docker Runner: Runs tasks within Docker containers, ensuring a consistent and isolated environment.</li> <li>Kubernetes Runner: Deploys tasks on Kubernetes clusters, leveraging its scalability and orchestration capabilities.</li> <li>Airflow Runner: Integrates with Apache Airflow, allowing for complex workflow orchestration and scheduling.</li> <li>ECS Runner: Executes tasks on AWS ECS, providing a managed container service.</li> <li>Batch Runner: Optimized for batch computing workloads on platforms like AWS Batch.</li> </ol>"},{"location":"guides/concepts/","title":"Concepts","text":""},{"location":"guides/concepts/#need","title":"Need","text":"<p>The landscape of machine learning and data processing has been rapidly evolving. While there are numerous solutions available for MLOps and DAG orchestration, most of them cater primarily to data engineering and data science teams. However, the rise of Large Language Models (LLMs) is reshaping this landscape, necessitating a more inclusive approach to MLOps.</p> <p>LLMs have democratized the use of machine learning models, enabling a broader spectrum of users within an organization to engage with them. This means that even organizations without a traditional data science data management or model management functions are now venturing into this domain.</p> <p>This shift brings forth several challenges:</p> <ol> <li> <p>Infrastructure Complexity: The world of MLOps is vast, with a plethora of options available for different use cases. Organizations often grapple with questions like:</p> <ul> <li>Which infrastructure is best suited for their needs?</li> <li>How can they efficiently reuse their existing infrastructure?</li> <li>How can they ensure scalability and performance while managing costs?</li> </ul> </li> <li> <p>Diverse Competence Levels: As LLM workflows become more prevalent, their volume is set to surpass that of traditional ML workflows in many organizations. This surge means that individuals without a formal engineering background or core ML expertise will be involved in the MLOps process. Ensuring that these individuals can contribute effectively without compromising the quality or integrity of the workflows is crucial.</p> </li> <li> <p>Standardization and Productionization: While many aspects of building LLM workflows can be managed without deep engineering expertise, there's a critical need for standardization. Engineers play a pivotal role in defining the processes for productionizing these workflows. Without standardized practices:</p> <ul> <li>How can organizations ensure consistency across workflows?</li> <li>How can they maintain the reliability and robustness of deployed models?</li> <li>How can they ensure that best practices are adhered to, regardless of who is building or deploying the workflow?</li> </ul> </li> </ol> <p>The advent of LLMs underscores the need for an MLOps framework that caters to a diverse audience. Such a framework should be flexible enough to accommodate the varied infrastructure needs of organizations, inclusive enough to empower contributors regardless of their technical expertise, and robust enough to ensure standardized, reliable workflows.</p>"},{"location":"guides/concepts/#introduction","title":"Introduction","text":"<p>The Geniusrise framework is built around loosely-coupled modules acting as a cohesive adhesive between distinct, modular components, much like how one would piece together Lego blocks. This design approach not only promotes flexibility but also ensures that each module or \"Lego block\" remains sufficiently independent. Such independence is crucial for diverse teams, each with its own unique infrastructure and requirements, to seamlessly build and manage their respective components.</p> <p>Geniusrise comes with a sizable set of plugins which implement various features and integrations. The independence and modularity of the design enable sharing of these building blocks in the community.</p>"},{"location":"guides/concepts/#concepts_1","title":"Concepts","text":"<ol> <li> <p>Task: At its core, a task represents a discrete unit of work within the Geniusrise framework. Think of it as a singular action or operation that the system needs to execute. A task further manifests itself into a Bolt or a Spout as stated below.</p> </li> <li> <p>Components of a Task: Each task is equipped with four components:</p> <ol> <li>State Manager: This component is responsible for continuously monitoring and managing the task's state, ensuring that it progresses smoothly from initiation to completion and to report errors and ship logs into a central location.</li> <li>Data Manager: As the name suggests, the Data Manager oversees the input and output data associated with a task, ensuring data integrity and efficient data flow. It also ensures data sanity follows partition semantics and isolation.</li> <li>Model Manager: In the realm of machine learning, model versioning and management are paramount. The Model Manager serves as a GitOps tool for ML models, ensuring that they are versioned, tracked, and managed effectively.</li> <li>Runner: These are wrappers for executing a task on various platforms. Depending on the platform, the runner ensures that the task is executed seamlessly.</li> </ol> </li> <li> <p>Task Classification: Tasks within the Geniusrise framework can be broadly classified into two categories:</p> <ul> <li>Spout: If a task's primary function is to ingest or bring in data, it's termed as a 'spout'.</li> <li>Bolt: For tasks that don't primarily ingest data but perform other operations, they are termed 'bolts'.</li> </ul> </li> </ol> <p>The beauty of the Geniusrise framework lies in its adaptability. Developers can script their workflow components once and have the freedom to deploy them across various platforms. To facilitate this, Geniusrise offers:</p> <ol> <li> <p>Runners for Task Execution: Geniusrise is equipped with a diverse set of runners, each tailored for different platforms, ensuring that tasks can be executed almost anywhere:</p> <ol> <li>On your local machine for quick testing and development.</li> <li>Within Docker containers for isolated, reproducible environments.</li> <li>On Kubernetes clusters for scalable, cloud-native deployments.</li> <li>Using Apache Airflow for complex workflow orchestration.</li> <li>On AWS ECS for containerized application management.</li> <li>With AWS Batch for efficient batch computing workloads.</li> </ol> </li> <li> <p>Library Wrappers: To ensure that tasks can interface with a variety of frameworks, Geniusrise provides integrations with:</p> <ol> <li>Jupyter/ipython for interactive computing.</li> <li>Apache PySpark for large-scale data processing.</li> <li>Apache PyFlink for stream and batch processing.</li> <li>Apache Beam for unified stream and batch data processing.</li> <li>Apache Storm for real-time computation.</li> </ol> </li> <li> <p>Genius hub - Where developers can share and sell their components. Coming soon geniusrise.com.</p> </li> </ol> <p>The framework aims to support multiple languages:</p> <ol> <li>Python</li> <li>Scala / JVM (WIP)</li> <li>Golang (WIP)</li> </ol> <p>This document delves into the core components and concepts that make up the Geniusrise framework.</p>"},{"location":"guides/concepts/#tradeoffs","title":"Tradeoffs","text":"<p>Because of the very loose coupling of the components, though the framework can be used to build very complex networks with independently running nodes, it provides limited orchestration capability, like synchronous pipelines. An external orchestrator like airflow can be used in such cases to orchestrate geniusrise components.</p>"},{"location":"guides/dev_cycle/","title":"Dev Cycle","text":"<p>This document describes one full local development cycle.</p>"},{"location":"guides/installation/","title":"Installation","text":"<p>Geniusrise is composed of the core framework and various plugins that implement specific tasks. The core has to be installed first, and after that selected plugins can be installed as and when required.</p>"},{"location":"guides/installation/#installing-geniusrise","title":"Installing Geniusrise","text":""},{"location":"guides/installation/#using-pip","title":"Using pip","text":"<p>To install the core framework using pip in local env, simply run:</p> <pre><code>pip install geniusrise\n</code></pre> <p>Or if you wish to install at user level:</p> <pre><code>pip install generiusrise --user\n</code></pre> <p>Or on a global level (might conflict with your OS package manager):</p> <pre><code>sudo pip install geniusrise\n</code></pre> <p>To verify the installation, you can check whether the geniusrise binary exists in PATH:</p> <pre><code>which geniusrise\n\ngeniusrise --help\n</code></pre>"},{"location":"guides/installation/#using-package-managers","title":"Using package managers","text":"<p>Geniusrise is also available as native packages for some Linux distributions.</p>"},{"location":"guides/installation/#aur","title":"AUR","text":"<p>Geniusrise is available on the AUR for arch and derived distros.</p> <pre><code>yay -S geniusrise\n</code></pre> <p>or directly from git master:</p> <pre><code>yay -S geniusrise-git\n</code></pre>"},{"location":"guides/installation/#ppa","title":"PPA","text":"<p>Geniusrise is also available on the PPA for debian-based distros.</p> <pre><code>sudo add-apt-repository ppa:ixaxaar/geniusrise\nsudo apt-get update\nsudo apt-get install -y geniusrise\n</code></pre>"},{"location":"guides/installation/#brew-cask","title":"Brew (cask)","text":"<pre><code>brew cask install geniusrise\n</code></pre>"},{"location":"guides/installation/#docker","title":"Docker","text":"<p>Geniusrise containers are available on Docker hub.</p> <pre><code>docker run -it --rm geniusrise/geniusrise:latest\n</code></pre>"},{"location":"guides/installation/#nix","title":"Nix","text":"<p>Coming soon \ud83d\ude22</p>"},{"location":"guides/installation/#installing-plugins","title":"Installing Plugins","text":"<p>Geniusrise offers a variety of plugins that act as composable lego blocks. To install a specific plugin, use the following format:</p> <pre><code>pip install geniusrise[plugin-name]\n</code></pre> <p>Replace <code>plugin-name</code> with the name of the desired plugin.</p>"},{"location":"guides/installation/#alternative-methods","title":"Alternative Methods","text":""},{"location":"guides/installation/#using-conda","title":"Using Conda","text":"<ol> <li>Activate the environment:</li> </ol> <pre><code>conda activate your-env\n</code></pre> <ol> <li>Install Geniusrise:</li> </ol> <pre><code>pip install geniusrise\n</code></pre> <p>For plugins:</p> <pre><code>pip install geniusrise[plugin-name]\n</code></pre>"},{"location":"guides/installation/#using-poetry","title":"Using Poetry","text":"<ol> <li>Add Geniusrise as a dependency:</li> </ol> <pre><code>poetry add geniusrise\n</code></pre> <p>For plugins:</p> <pre><code>poetry add geniusrise[plugin-name]\n</code></pre>"},{"location":"guides/installation/#development","title":"Development","text":"<p>For development, you may want to install from the repo:</p> <pre><code>git clone git@github.com:geniusrise/geniusrise.git\ncd geniusrise\nvirtualenv venv -p `which python3.10`\nsource venv/bin/activate\npip install -r ./requirements.txt\n\nmake install # installs in your local venv directory\n</code></pre> <p>That's it! You've successfully installed Geniusrise and its plugins. \ud83c\udf89</p>"},{"location":"guides/local/","title":"Local setup","text":"<p>This is probably the first thing that anyone would want to do.</p>"},{"location":"guides/local/#boilerplate","title":"Boilerplate","text":"<p>To setup a local geniusrise project, simply use the geniusrise project creator script:</p> <pre><code>curl -L https://cum.gdn/OfeQir | bash\n</code></pre> <p>or</p> <pre><code>curl -L https://raw.githubusercontent.com/geniusrise/geniusrise/master/scripts/install.sh | bash\n</code></pre>"},{"location":"guides/local/#existing-project","title":"Existing project","text":"<p>If you wish to add geniusrise to an existing project:</p> <pre><code>pip install geniusrise\npip freeze &gt; requirements.txt\n</code></pre>"},{"location":"guides/local/#from-scratch","title":"From scratch","text":"<p>Here is how to set up from scratch:</p> <pre><code>#!/bin/bash\n# Prompt for project details\nread -p \"Enter your project name: \" project_name\nread -p \"Enter your name: \" author_name\nread -p \"Enter your email: \" author_email\nread -p \"Enter your GitHub username: \" github_username\nread -p \"Enter a brief description of your project: \" project_description\n# Create project structure\nmkdir $project_name\ncd $project_name\nmkdir $project_name tests\n# Create basic files\ntouch README.md\ntouch requirements.txt\ntouch setup.py\ntouch Makefile\ntouch $project_name/__init__.py\ntouch tests/__init__.py\n# Populate README.md\necho \"# $project_name\" &gt; README.md\necho \"\\n$project_description\" &gt;&gt; README.md\n# Populate setup.py\ncat &lt;&lt;EOL &gt; setup.py\nfrom setuptools import setup, find_packages\nwith open(\"README.md\", \"r\", encoding=\"utf-8\") as fh:\nlong_description = fh.read()\nsetup(\nname='$project_name',\nversion='0.1.0',\npackages=find_packages(exclude=[\"tests\", \"tests.*\"]),\ninstall_requires=[],\npython_requires='&gt;=3.10',\nauthor='$author_name',\nauthor_email='$author_email',\ndescription='$project_description',\nlong_description=long_description,\nlong_description_content_type='text/markdown',\nurl='https://github.com/$github_username/$project_name',\nclassifiers=[\n'Programming Language :: Python :: 3',\n'License :: OSI Approved :: MIT License',\n'Operating System :: OS Independent',\n],\n)\nEOL\n# Populate Makefile\ncat &lt;&lt;EOL &gt; Makefile\nsetup:\n@pip install -r ./requirements.txt\ntest:\n@coverage run -m pytest -v ./tests\npublish:\n@python setup.py sdist bdist_wheel\n@twine upload dist/$project_name-\\$${VERSION}-* --verbose\nEOL\n# Set up the virtual environment and install necessary packages\nvirtualenv venv -p `which python3.10`\nsource venv/bin/activate\npip install twine setuptools pytest coverage\npip freeze &gt; requirements.txt\n# Fetch .pre-commit-config.yaml and .gitignore from geniusrise/geniusrise\ncurl -O https://raw.githubusercontent.com/geniusrise/geniusrise/master/.pre-commit-config.yaml\ncurl -O https://raw.githubusercontent.com/geniusrise/geniusrise/master/.gitignore\necho \"Project $project_name initialized!\"\n</code></pre> <p>Create a install script out of this and execute it:</p> <pre><code>touch install.sh\nchmod +x ./install.sh\n./install.sh\n</code></pre>"},{"location":"spouts/activemq/","title":"ActiveMQ","text":"<p>Spout for ActiveMQ</p> <p>             Bases: <code>Spout</code></p>"},{"location":"spouts/activemq/#activemq.ActiveMQ.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the ActiveMQ class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>StreamingOutput</code> <p>An instance of the StreamingOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"spouts/activemq/#activemq.ActiveMQ.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius ActiveMQ rise \\\nstreaming \\\n--output_kafka_topic activemq_test \\\n--output_kafka_cluster_connection_string localhost:9094 \\\npostgres \\\n--postgres_host 127.0.0.1 \\\n--postgres_port 5432 \\\n--postgres_user postgres \\\n--postgres_password postgres \\\n--postgres_database geniusrise \\\n--postgres_table state \\\nlisten \\\n--args host=localhost port=61613 destination=my_queue\n</code></pre>"},{"location":"spouts/activemq/#activemq.ActiveMQ.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_activemq_spout:\nname: \"ActiveMQ\"\nmethod: \"listen\"\nargs:\nhost: \"localhost\"\nport: 61613\ndestination: \"my_queue\"\noutput:\ntype: \"streaming\"\nargs:\noutput_topic: \"activemq_test\"\nkafka_servers: \"localhost:9094\"\nstate:\ntype: \"postgres\"\nargs:\npostgres_host: \"127.0.0.1\"\npostgres_port: 5432\npostgres_user: \"postgres\"\npostgres_password: \"postgres\"\npostgres_database: \"geniusrise\"\npostgres_table: \"state\"\ndeploy:\ntype: \"k8s\"\nargs:\nname: \"my_activemq_spout\"\nnamespace: \"default\"\nimage: \"my_activemq_spout_image\"\nreplicas: 1\n</code></pre>"},{"location":"spouts/activemq/#activemq.ActiveMQ.listen","title":"<code>listen(host, port, destination, username=None, password=None)</code>","text":"<p>\ud83d\udcd6 Start listening for data from the ActiveMQ server.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The ActiveMQ server host.</p> required <code>port</code> <code>int</code> <p>The ActiveMQ server port.</p> required <code>destination</code> <code>str</code> <p>The ActiveMQ destination (queue or topic).</p> required <code>username</code> <code>Optional[str]</code> <p>The username for authentication. Defaults to None.</p> <code>None</code> <code>password</code> <code>Optional[str]</code> <p>The password for authentication. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the ActiveMQ server.</p>"},{"location":"spouts/amqp/","title":"ActiveMQ","text":"<p>Spout for AMQP</p> <p>             Bases: <code>Spout</code></p>"},{"location":"spouts/amqp/#amqp.RabbitMQ.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the RabbitMQ class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>StreamingOutput</code> <p>An instance of the StreamingOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"spouts/amqp/#amqp.RabbitMQ.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius RabbitMQ rise \\\nstreaming \\\n--output_kafka_topic rabbitmq_test \\\n--output_kafka_cluster_connection_string localhost:9094 \\\npostgres \\\n--postgres_host 127.0.0.1 \\\n--postgres_port 5432 \\\n--postgres_user postgres \\\n--postgres_password postgres \\\n--postgres_database geniusrise \\\n--postgres_table state \\\nlisten \\\n--args queue_name=my_queue host=localhost\n</code></pre>"},{"location":"spouts/amqp/#amqp.RabbitMQ.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_rabbitmq_spout:\nname: \"RabbitMQ\"\nmethod: \"listen\"\nargs:\nqueue_name: \"my_queue\"\nhost: \"localhost\"\noutput:\ntype: \"streaming\"\nargs:\noutput_topic: \"rabbitmq_test\"\nkafka_servers: \"localhost:9094\"\nstate:\ntype: \"postgres\"\nargs:\npostgres_host: \"127.0.0.1\"\npostgres_port: 5432\npostgres_user: \"postgres\"\npostgres_password: \"postgres\"\npostgres_database: \"geniusrise\"\npostgres_table: \"state\"\ndeploy:\ntype: \"k8s\"\nargs:\nname: \"my_rabbitmq_spout\"\nnamespace: \"default\"\nimage: \"my_rabbitmq_spout_image\"\nreplicas: 1\n</code></pre>"},{"location":"spouts/amqp/#amqp.RabbitMQ.listen","title":"<code>listen(queue_name, host='localhost', username=None, password=None)</code>","text":"<p>\ud83d\udcd6 Start listening for data from the RabbitMQ server.</p> <p>Parameters:</p> Name Type Description Default <code>queue_name</code> <code>str</code> <p>The RabbitMQ queue name to listen to.</p> required <code>host</code> <code>str</code> <p>The RabbitMQ server host. Defaults to \"localhost\".</p> <code>'localhost'</code> <code>username</code> <code>Optional[str]</code> <p>The username for authentication. Defaults to None.</p> <code>None</code> <code>password</code> <code>Optional[str]</code> <p>The password for authentication. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the RabbitMQ server.</p>"},{"location":"spouts/grpc/","title":"GRPC","text":"<p>Spout for gRPC</p> <p>             Bases: <code>Spout</code></p>"},{"location":"spouts/grpc/#grpc.Grpc.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the Grpc class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>StreamingOutput</code> <p>An instance of the StreamingOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"spouts/grpc/#grpc.Grpc.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius Grpc rise \\\nstreaming \\\n--output_kafka_topic grpc_test \\\n--output_kafka_cluster_connection_string localhost:9094 \\\npostgres \\\n--postgres_host 127.0.0.1 \\\n--postgres_port 5432 \\\n--postgres_user postgres \\\n--postgres_password postgres \\\n--postgres_database geniusrise \\\n--postgres_table state \\\nlisten \\\n--args server_address=localhost:50051 request_data=my_request syntax=proto3\n</code></pre>"},{"location":"spouts/grpc/#grpc.Grpc.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_grpc_spout:\nname: \"Grpc\"\nmethod: \"listen\"\nargs:\nserver_address: \"localhost:50051\"\nrequest_data: \"my_request\"\nsyntax: \"proto3\"\noutput:\ntype: \"streaming\"\nargs:\noutput_topic: \"grpc_test\"\nkafka_servers: \"localhost:9094\"\nstate:\ntype: \"postgres\"\nargs:\npostgres_host: \"127.0.0.1\"\npostgres_port: 5432\npostgres_user: \"postgres\"\npostgres_password: \"postgres\"\npostgres_database: \"geniusrise\"\npostgres_table: \"state\"\ndeploy:\ntype: \"k8s\"\nargs:\nname: \"my_grpc_spout\"\nnamespace: \"default\"\nimage: \"my_grpc_spout_image\"\nreplicas: 1\n</code></pre>"},{"location":"spouts/grpc/#grpc.Grpc.listen","title":"<code>listen(server_address, request_data, syntax, certificate=None, client_key=None, client_cert=None)</code>","text":"<p>\ud83d\udcd6 Start listening for data from the gRPC server.</p> <p>Parameters:</p> Name Type Description Default <code>server_address</code> <code>str</code> <p>The address of the gRPC server.</p> required <code>request_data</code> <code>str</code> <p>Data to send in the request.</p> required <code>syntax</code> <code>str</code> <p>The syntax to be used (e.g., \"proto3\").</p> required <code>certificate</code> <code>Optional[str]</code> <p>Optional server certificate for SSL/TLS.</p> <code>None</code> <code>client_key</code> <code>Optional[str]</code> <p>Optional client key for SSL/TLS.</p> <code>None</code> <code>client_cert</code> <code>Optional[str]</code> <p>Optional client certificate for SSL/TLS.</p> <code>None</code> <p>Raises:</p> Type Description <code>grpc.RpcError</code> <p>If there is an error while processing gRPC messages.</p>"},{"location":"spouts/http_polling/","title":"HTTP polling","text":"<p>Spout for HTTP polling</p> <p>             Bases: <code>Spout</code></p>"},{"location":"spouts/http_polling/#http_polling.RESTAPIPoll.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the RESTAPIPoll class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>StreamingOutput</code> <p>An instance of the StreamingOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"spouts/http_polling/#http_polling.RESTAPIPoll.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius RESTAPIPoll rise \\\nstreaming \\\n--output_kafka_topic restapi_test \\\n--output_kafka_cluster_connection_string localhost:9094 \\\npostgres \\\n--postgres_host 127.0.0.1 \\\n--postgres_port 5432 \\\n--postgres_user postgres \\\n--postgres_password postgres \\\n--postgres_database geniusrise \\\n--postgres_table state \\\nlisten \\\n--args url=https://api.example.com method=GET interval=60\n</code></pre>"},{"location":"spouts/http_polling/#http_polling.RESTAPIPoll.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_restapi_poll:\nname: \"RESTAPIPoll\"\nmethod: \"listen\"\nargs:\nurl: \"https://api.example.com\"\nmethod: \"GET\"\ninterval: 60\noutput:\ntype: \"streaming\"\nargs:\noutput_topic: \"restapi_test\"\nkafka_servers: \"localhost:9094\"\nstate:\ntype: \"postgres\"\nargs:\npostgres_host: \"127.0.0.1\"\npostgres_port: 5432\npostgres_user: \"postgres\"\npostgres_password: \"postgres\"\npostgres_database: \"geniusrise\"\npostgres_table: \"state\"\ndeploy:\ntype: \"k8s\"\nargs:\nname: \"my_restapi_poll\"\nnamespace: \"default\"\nimage: \"my_restapi_poll_image\"\nreplicas: 1\n</code></pre>"},{"location":"spouts/http_polling/#http_polling.RESTAPIPoll.listen","title":"<code>listen(url, method, interval=60, body=None, headers=None, params=None)</code>","text":"<p>Start polling the REST API for data.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The API endpoint.</p> required <code>method</code> <code>str</code> <p>The HTTP method (GET, POST, etc.).</p> required <code>interval</code> <code>int</code> <p>The polling interval in seconds. Defaults to 60.</p> <code>60</code> <code>body</code> <code>Optional[Dict]</code> <p>The request body. Defaults to None.</p> <code>None</code> <code>headers</code> <code>Optional[Dict[str, str]]</code> <p>The request headers. Defaults to None.</p> <code>None</code> <code>params</code> <code>Optional[Dict[str, str]]</code> <p>The request query parameters. Defaults to None.</p> <code>None</code>"},{"location":"spouts/http_polling/#http_polling.RESTAPIPoll.poll_api","title":"<code>poll_api(url, method, body=None, headers=None, params=None)</code>","text":"<p>\ud83d\udcd6 Start polling the REST API for data.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The API endpoint.</p> required <code>method</code> <code>str</code> <p>The HTTP method (GET, POST, etc.).</p> required <code>interval</code> <code>int</code> <p>The polling interval in seconds.</p> required <code>body</code> <code>Optional[Dict]</code> <p>The request body. Defaults to None.</p> <code>None</code> <code>headers</code> <code>Optional[Dict[str, str]]</code> <p>The request headers. Defaults to None.</p> <code>None</code> <code>params</code> <code>Optional[Dict[str, str]]</code> <p>The request query parameters. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the REST API server.</p>"},{"location":"spouts/kafka/","title":"Kafka","text":"<p>Spout for Kafka</p> <p>             Bases: <code>Spout</code></p>"},{"location":"spouts/kafka/#kafka.Kafka.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the Kafka class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>StreamingOutput</code> <p>An instance of the StreamingOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"spouts/kafka/#kafka.Kafka.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius Kafka rise \\\nstreaming \\\n--output_kafka_topic kafka_test \\\n--output_kafka_cluster_connection_string localhost:9094 \\\npostgres \\\n--postgres_host 127.0.0.1 \\\n--postgres_port 5432 \\\n--postgres_user postgres \\\n--postgres_password postgres \\\n--postgres_database geniusrise \\\n--postgres_table state \\\nlisten \\\n--args topic=my_topic group_id=my_group\n</code></pre>"},{"location":"spouts/kafka/#kafka.Kafka.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_kafka_spout:\nname: \"Kafka\"\nmethod: \"listen\"\nargs:\ntopic: \"my_topic\"\ngroup_id: \"my_group\"\noutput:\ntype: \"streaming\"\nargs:\noutput_topic: \"kafka_test\"\nkafka_servers: \"localhost:9094\"\nstate:\ntype: \"postgres\"\nargs:\npostgres_host: \"127.0.0.1\"\npostgres_port: 5432\npostgres_user: \"postgres\"\npostgres_password: \"postgres\"\npostgres_database: \"geniusrise\"\npostgres_table: \"state\"\ndeploy:\ntype: \"k8s\"\nargs:\nname: \"my_kafka_spout\"\nnamespace: \"default\"\nimage: \"my_kafka_spout_image\"\nreplicas: 1\n</code></pre>"},{"location":"spouts/kafka/#kafka.Kafka.listen","title":"<code>listen(topic, group_id, bootstrap_servers='localhost:9092', username=None, password=None)</code>","text":"<p>\ud83d\udcd6 Start listening for data from the Kafka topic.</p> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>str</code> <p>The Kafka topic to listen to.</p> required <code>group_id</code> <code>str</code> <p>The Kafka consumer group ID.</p> required <code>bootstrap_servers</code> <code>str</code> <p>The Kafka bootstrap servers. Defaults to \"localhost:9092\".</p> <code>'localhost:9092'</code> <code>username</code> <code>Optional[str]</code> <p>The username for SASL/PLAIN authentication. Defaults to None.</p> <code>None</code> <code>password</code> <code>Optional[str]</code> <p>The password for SASL/PLAIN authentication. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the Kafka server.</p>"},{"location":"spouts/kinesis/","title":"Kinesis","text":"<p>Spout for Kinesis</p> <p>             Bases: <code>Spout</code></p>"},{"location":"spouts/kinesis/#kinesis.Kinesis.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the Kinesis class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>StreamingOutput</code> <p>An instance of the StreamingOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"spouts/kinesis/#kinesis.Kinesis.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius Kinesis rise \\\nstreaming \\\n--output_kafka_topic kinesis_test \\\n--output_kafka_cluster_connection_string localhost:9094 \\\npostgres \\\n--postgres_host 127.0.0.1 \\\n--postgres_port 5432 \\\n--postgres_user postgres \\\n--postgres_password postgres \\\n--postgres_database geniusrise \\\n--postgres_table state \\\nlisten \\\n--args stream_name=my_stream shard_id=shardId-000000000000\n</code></pre>"},{"location":"spouts/kinesis/#kinesis.Kinesis.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_kinesis_spout:\nname: \"Kinesis\"\nmethod: \"listen\"\nargs:\nstream_name: \"my_stream\"\nshard_id: \"shardId-000000000000\"\noutput:\ntype: \"streaming\"\nargs:\noutput_topic: \"kinesis_test\"\nkafka_servers: \"localhost:9094\"\nstate:\ntype: \"postgres\"\nargs:\npostgres_host: \"127.0.0.1\"\npostgres_port: 5432\npostgres_user: \"postgres\"\npostgres_password: \"postgres\"\npostgres_database: \"geniusrise\"\npostgres_table: \"state\"\ndeploy:\ntype: \"k8s\"\nargs:\nname: \"my_kinesis_spout\"\nnamespace: \"default\"\nimage: \"my_kinesis_spout_image\"\nreplicas: 1\n</code></pre>"},{"location":"spouts/kinesis/#kinesis.Kinesis.listen","title":"<code>listen(stream_name, shard_id='shardId-000000000000', region_name=None, aws_access_key_id=None, aws_secret_access_key=None)</code>","text":"<p>\ud83d\udcd6 Start listening for data from the Kinesis stream.</p> <p>Parameters:</p> Name Type Description Default <code>stream_name</code> <code>str</code> <p>The name of the Kinesis stream.</p> required <code>shard_id</code> <code>str</code> <p>The shard ID to read from. Defaults to \"shardId-000000000000\".</p> <code>'shardId-000000000000'</code> <code>region_name</code> <code>str</code> <p>The AWS region name.</p> <code>None</code> <code>aws_access_key_id</code> <code>str</code> <p>AWS access key ID for authentication.</p> <code>None</code> <code>aws_secret_access_key</code> <code>str</code> <p>AWS secret access key for authentication.</p> <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If there is an error while processing Kinesis records.</p>"},{"location":"spouts/mqtt/","title":"MQTT","text":"<p>Spout for MQTT</p> <p>             Bases: <code>Spout</code></p>"},{"location":"spouts/mqtt/#mqtt.MQTT.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the MQTT class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>StreamingOutput</code> <p>An instance of the StreamingOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"spouts/mqtt/#mqtt.MQTT.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius MQTT rise \\\nstreaming \\\n--output_kafka_topic mqtt_test \\\n--output_kafka_cluster_connection_string localhost:9094 \\\npostgres \\\n--postgres_host 127.0.0.1 \\\n--postgres_port 5432 \\\n--postgres_user postgres \\\n--postgres_password postgres \\\n--postgres_database geniusrise \\\n--postgres_table state \\\nlisten \\\n--args host=localhost port=1883 topic=my_topic\n</code></pre>"},{"location":"spouts/mqtt/#mqtt.MQTT.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_mqtt_spout:\nname: \"MQTT\"\nmethod: \"listen\"\nargs:\nhost: \"localhost\"\nport: 1883\ntopic: \"my_topic\"\noutput:\ntype: \"streaming\"\nargs:\noutput_topic: \"mqtt_test\"\nkafka_servers: \"localhost:9094\"\nstate:\ntype: \"postgres\"\nargs:\npostgres_host: \"127.0.0.1\"\npostgres_port: 5432\npostgres_user: \"postgres\"\npostgres_password: \"postgres\"\npostgres_database: \"geniusrise\"\npostgres_table: \"state\"\ndeploy:\ntype: \"k8s\"\nargs:\nname: \"my_mqtt_spout\"\nnamespace: \"default\"\nimage: \"my_mqtt_spout_image\"\nreplicas: 1\n</code></pre>"},{"location":"spouts/mqtt/#mqtt.MQTT.listen","title":"<code>listen(host='localhost', port=1883, topic='#', username=None, password=None)</code>","text":"<p>Start listening for data from the MQTT broker.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The MQTT broker host. Defaults to \"localhost\".</p> <code>'localhost'</code> <code>port</code> <code>int</code> <p>The MQTT broker port. Defaults to 1883.</p> <code>1883</code> <code>topic</code> <code>str</code> <p>The MQTT topic to subscribe to. Defaults to \"#\".</p> <code>'#'</code> <code>username</code> <code>Optional[str]</code> <p>The username for authentication. Defaults to None.</p> <code>None</code> <code>password</code> <code>Optional[str]</code> <p>The password for authentication. Defaults to None.</p> <code>None</code>"},{"location":"spouts/quic/","title":"Quic","text":"<p>Spout for Quic</p> <p>             Bases: <code>Spout</code></p>"},{"location":"spouts/quic/#quic.Quic.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the Quic class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>StreamingOutput</code> <p>An instance of the StreamingOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"spouts/quic/#quic.Quic.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius Quic rise \\\nstreaming \\\n--output_kafka_topic quic_test \\\n--output_kafka_cluster_connection_string localhost:9094 \\\npostgres \\\n--postgres_host 127.0.0.1 \\\n--postgres_port 5432 \\\n--postgres_user postgres \\\n--postgres_password postgres \\\n--postgres_database geniusrise \\\n--postgres_table state \\\nlisten \\\n--args cert_path=/path/to/cert.pem key_path=/path/to/key.pem host=localhost port=4433\n</code></pre>"},{"location":"spouts/quic/#quic.Quic.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_quic_spout:\nname: \"Quic\"\nmethod: \"listen\"\nargs:\ncert_path: \"/path/to/cert.pem\"\nkey_path: \"/path/to/key.pem\"\nhost: \"localhost\"\nport: 4433\noutput:\ntype: \"streaming\"\nargs:\noutput_topic: \"quic_test\"\nkafka_servers: \"localhost:9094\"\nstate:\ntype: \"postgres\"\nargs:\npostgres_host: \"127.0.0.1\"\npostgres_port: 5432\npostgres_user: \"postgres\"\npostgres_password: \"postgres\"\npostgres_database: \"geniusrise\"\npostgres_table: \"state\"\ndeploy:\ntype: \"k8s\"\nargs:\nname: \"my_quic_spout\"\nnamespace: \"default\"\nimage: \"my_quic_spout_image\"\nreplicas: 1\n</code></pre>"},{"location":"spouts/quic/#quic.Quic.handle_stream_data","title":"<code>handle_stream_data(data, stream_id)</code>  <code>async</code>","text":"<p>Handle incoming stream data.</p> <p>:param data: The incoming data. :param stream_id: The ID of the stream.</p>"},{"location":"spouts/quic/#quic.Quic.listen","title":"<code>listen(cert_path, key_path, host='localhost', port=4433)</code>","text":"<p>\ud83d\udcd6 Start listening for data from the QUIC server.</p> <p>Parameters:</p> Name Type Description Default <code>cert_path</code> <code>str</code> <p>Path to the certificate file.</p> required <code>key_path</code> <code>str</code> <p>Path to the private key file.</p> required <code>host</code> <code>str</code> <p>Hostname to listen on. Defaults to \"localhost\".</p> <code>'localhost'</code> <code>port</code> <code>int</code> <p>Port to listen on. Defaults to 4433.</p> <code>4433</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to start the QUIC server.</p>"},{"location":"spouts/redis_pubsub/","title":"Redis pubsub","text":"<p>Spout for Redis pubsub</p> <p>             Bases: <code>Spout</code></p>"},{"location":"spouts/redis_pubsub/#redis_pubsub.RedisPubSub.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the RedisPubSub class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>StreamingOutput</code> <p>An instance of the StreamingOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"spouts/redis_pubsub/#redis_pubsub.RedisPubSub.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius RedisPubSub rise \\\nstreaming \\\n--output_kafka_topic redis_test \\\n--output_kafka_cluster_connection_string localhost:9094 \\\npostgres \\\n--postgres_host 127.0.0.1 \\\n--postgres_port 5432 \\\n--postgres_user postgres \\\n--postgres_password postgres \\\n--postgres_database geniusrise \\\n--postgres_table state \\\nlisten \\\n--args channel=my_channel host=localhost port=6379 db=0\n</code></pre>"},{"location":"spouts/redis_pubsub/#redis_pubsub.RedisPubSub.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_redis_spout:\nname: \"RedisPubSub\"\nmethod: \"listen\"\nargs:\nchannel: \"my_channel\"\nhost: \"localhost\"\nport: 6379\ndb: 0\noutput:\ntype: \"streaming\"\nargs:\noutput_topic: \"redis_test\"\nkafka_servers: \"localhost:9094\"\nstate:\ntype: \"postgres\"\nargs:\npostgres_host: \"127.0.0.1\"\npostgres_port: 5432\npostgres_user: \"postgres\"\npostgres_password: \"postgres\"\npostgres_database: \"geniusrise\"\npostgres_table: \"state\"\ndeploy:\ntype: \"k8s\"\nargs:\nname: \"my_redis_spout\"\nnamespace: \"default\"\nimage: \"my_redis_spout_image\"\nreplicas: 1\n</code></pre>"},{"location":"spouts/redis_pubsub/#redis_pubsub.RedisPubSub.listen","title":"<code>listen(channel, host='localhost', port=6379, db=0, password=None)</code>","text":"<p>\ud83d\udcd6 Start listening for data from the Redis Pub/Sub channel.</p> <p>Parameters:</p> Name Type Description Default <code>channel</code> <code>str</code> <p>The Redis Pub/Sub channel to listen to.</p> required <code>host</code> <code>str</code> <p>The Redis server host. Defaults to \"localhost\".</p> <code>'localhost'</code> <code>port</code> <code>int</code> <p>The Redis server port. Defaults to 6379.</p> <code>6379</code> <code>db</code> <code>int</code> <p>The Redis database index. Defaults to 0.</p> <code>0</code> <code>password</code> <code>Optional[str]</code> <p>The password for authentication. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the Redis server.</p>"},{"location":"spouts/redis_streams/","title":"Redis streams","text":"<p>Spout for Redis streams</p> <p>             Bases: <code>Spout</code></p>"},{"location":"spouts/redis_streams/#redis_streams.RedisStream.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the RedisStream class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>StreamingOutput</code> <p>An instance of the StreamingOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"spouts/redis_streams/#redis_streams.RedisStream.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius RedisStream rise \\\nstreaming \\\n--output_kafka_topic redis_stream_test \\\n--output_kafka_cluster_connection_string localhost:9094 \\\npostgres \\\n--postgres_host 127.0.0.1 \\\n--postgres_port 5432 \\\n--postgres_user postgres \\\n--postgres_password postgres \\\n--postgres_database geniusrise \\\n--postgres_table state \\\nlisten \\\n--args stream_key=my_stream host=localhost port=6379 db=0\n</code></pre>"},{"location":"spouts/redis_streams/#redis_streams.RedisStream.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_redis_stream:\nname: \"RedisStream\"\nmethod: \"listen\"\nargs:\nstream_key: \"my_stream\"\nhost: \"localhost\"\nport: 6379\ndb: 0\noutput:\ntype: \"streaming\"\nargs:\noutput_topic: \"redis_stream_test\"\nkafka_servers: \"localhost:9094\"\nstate:\ntype: \"postgres\"\nargs:\npostgres_host: \"127.0.0.1\"\npostgres_port: 5432\npostgres_user: \"postgres\"\npostgres_password: \"postgres\"\npostgres_database: \"geniusrise\"\npostgres_table: \"state\"\ndeploy:\ntype: \"k8s\"\nargs:\nname: \"my_redis_stream\"\nnamespace: \"default\"\nimage: \"my_redis_stream_image\"\nreplicas: 1\n</code></pre>"},{"location":"spouts/redis_streams/#redis_streams.RedisStream.listen","title":"<code>listen(stream_key, host='localhost', port=6379, db=0, password=None)</code>","text":"<p>\ud83d\udcd6 Start the asyncio event loop to listen for data from the Redis stream.</p> <p>Parameters:</p> Name Type Description Default <code>stream_key</code> <code>str</code> <p>The Redis stream key to listen to.</p> required <code>host</code> <code>str</code> <p>The Redis server host. Defaults to \"localhost\".</p> <code>'localhost'</code> <code>port</code> <code>int</code> <p>The Redis server port. Defaults to 6379.</p> <code>6379</code> <code>db</code> <code>int</code> <p>The Redis database index. Defaults to 0.</p> <code>0</code> <code>password</code> <code>Optional[str]</code> <p>The password for authentication. Defaults to None.</p> <code>None</code>"},{"location":"spouts/sns/","title":"SNS","text":"<p>Spout for SNS</p> <p>             Bases: <code>Spout</code></p>"},{"location":"spouts/sns/#sns.SNS.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the SNS class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>StreamingOutput</code> <p>An instance of the StreamingOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"spouts/sns/#sns.SNS.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius SNS rise \\\nstreaming \\\n--output_kafka_topic sns_test \\\n--output_kafka_cluster_connection_string localhost:9094 \\\npostgres \\\n--postgres_host 127.0.0.1 \\\n--postgres_port 5432 \\\n--postgres_user postgres \\\n--postgres_password postgres \\\n--postgres_database geniusrise \\\n--postgres_table state \\\nlisten\n</code></pre>"},{"location":"spouts/sns/#sns.SNS.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_sns_spout:\nname: \"SNS\"\nmethod: \"listen\"\noutput:\ntype: \"streaming\"\nargs:\noutput_topic: \"sns_test\"\nkafka_servers: \"localhost:9094\"\nstate:\ntype: \"postgres\"\nargs:\npostgres_host: \"127.0.0.1\"\npostgres_port: 5432\npostgres_user: \"postgres\"\npostgres_password: \"postgres\"\npostgres_database: \"geniusrise\"\npostgres_table: \"state\"\ndeploy:\ntype: \"k8s\"\nargs:\nname: \"my_sns_spout\"\nnamespace: \"default\"\nimage: \"my_sns_spout_image\"\nreplicas: 1\n</code></pre>"},{"location":"spouts/sns/#sns.SNS.listen","title":"<code>listen()</code>","text":"<p>\ud83d\udcd6 Start the asyncio event loop to listen for data from AWS SNS.</p>"},{"location":"spouts/socket.io/","title":"Socket.io","text":"<p>Spout for socket.io</p> <p>             Bases: <code>Spout</code></p>"},{"location":"spouts/socket.io/#socketio.SocketIo.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the SocketIo class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>StreamingOutput</code> <p>An instance of the StreamingOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"spouts/socket.io/#socketio.SocketIo.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius SocketIo rise \\\nstreaming \\\n--output_kafka_topic socketio_test \\\n--output_kafka_cluster_connection_string localhost:9094 \\\npostgres \\\n--postgres_host 127.0.0.1 \\\n--postgres_port 5432 \\\n--postgres_user postgres \\\n--postgres_password postgres \\\n--postgres_database geniusrise \\\n--postgres_table state \\\nlisten \\\n--args url=http://localhost:3000 namespace=/chat\n</code></pre>"},{"location":"spouts/socket.io/#socketio.SocketIo.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_socketio_spout:\nname: \"SocketIo\"\nmethod: \"listen\"\nargs:\nurl: \"http://localhost:3000\"\nnamespace: \"/chat\"\noutput:\ntype: \"streaming\"\nargs:\noutput_topic: \"socketio_test\"\nkafka_servers: \"localhost:9094\"\nstate:\ntype: \"postgres\"\nargs:\npostgres_host: \"127.0.0.1\"\npostgres_port: 5432\npostgres_user: \"postgres\"\npostgres_password: \"postgres\"\npostgres_database: \"geniusrise\"\npostgres_table: \"state\"\ndeploy:\ntype: \"k8s\"\nargs:\nname: \"my_socketio_spout\"\nnamespace: \"default\"\nimage: \"my_socketio_spout_image\"\nreplicas: 1\n</code></pre>"},{"location":"spouts/socket.io/#socketio.SocketIo.listen","title":"<code>listen(url, namespace=None, event='message', auth=None)</code>","text":"<p>\ud83d\udcd6 Start listening for data from the Socket.io server.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The Socket.io server URL.</p> required <code>namespace</code> <code>Optional[str]</code> <p>The Socket.io namespace. Defaults to None.</p> <code>None</code> <code>event</code> <code>str</code> <p>The Socket.io event to listen to. Defaults to \"message\".</p> <code>'message'</code> <code>auth</code> <code>Optional[dict]</code> <p>Authentication dictionary. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the Socket.io server.</p>"},{"location":"spouts/sqs/","title":"SQS","text":"<p>Spout for SQS</p> <p>             Bases: <code>Spout</code></p>"},{"location":"spouts/sqs/#sqs.SQS.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the SQS class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>StreamingOutput</code> <p>An instance of the StreamingOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"spouts/sqs/#sqs.SQS.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius SQS rise \\\nstreaming \\\n--output_kafka_topic sqs_test \\\n--output_kafka_cluster_connection_string localhost:9094 \\\npostgres \\\n--postgres_host 127.0.0.1 \\\n--postgres_port 5432 \\\n--postgres_user postgres \\\n--postgres_password postgres \\\n--postgres_database geniusrise \\\n--postgres_table state \\\nlisten \\\n--args queue_url=https://sqs.us-east-1.amazonaws.com/123456789012/my-queue batch_size=10 batch_interval=10\n</code></pre>"},{"location":"spouts/sqs/#sqs.SQS.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_sqs_spout:\nname: \"SQS\"\nmethod: \"listen\"\nargs:\nqueue_url: \"https://sqs.us-east-1.amazonaws.com/123456789012/my-queue\"\nbatch_size: 10\nbatch_interval: 10\noutput:\ntype: \"streaming\"\nargs:\noutput_topic: \"sqs_test\"\nkafka_servers: \"localhost:9094\"\nstate:\ntype: \"postgres\"\nargs:\npostgres_host: \"127.0.0.1\"\npostgres_port: 5432\npostgres_user: \"postgres\"\npostgres_password: \"postgres\"\npostgres_database: \"geniusrise\"\npostgres_table: \"state\"\ndeploy:\ntype: \"k8s\"\nargs:\nname: \"my_sqs_spout\"\nnamespace: \"default\"\nimage: \"my_sqs_spout_image\"\nreplicas: 1\n</code></pre>"},{"location":"spouts/sqs/#sqs.SQS.listen","title":"<code>listen(queue_url, batch_size=10, batch_interval=10)</code>","text":"<p>\ud83d\udcd6 Start listening for new messages in the SQS queue.</p> <p>Parameters:</p> Name Type Description Default <code>queue_url</code> <code>str</code> <p>The URL of the SQS queue to listen to.</p> required <code>batch_size</code> <code>int</code> <p>The maximum number of messages to receive in each batch. Defaults to 10.</p> <code>10</code> <code>batch_interval</code> <code>int</code> <p>The time in seconds to wait for a new message if the queue is empty. Defaults to 10.</p> <code>10</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the SQS service.</p>"},{"location":"spouts/udp/","title":"UDP","text":"<p>Spout for UDP</p> <p>             Bases: <code>Spout</code></p>"},{"location":"spouts/udp/#udp.Udp.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the Udp class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>StreamingOutput</code> <p>An instance of the StreamingOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"spouts/udp/#udp.Udp.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius Udp rise \\\nstreaming \\\n--output_kafka_topic udp_test \\\n--output_kafka_cluster_connection_string localhost:9094 \\\npostgres \\\n--postgres_host 127.0.0.1 \\\n--postgres_port 5432 \\\n--postgres_user postgres \\\n--postgres_password postgres \\\n--postgres_database geniusrise \\\n--postgres_table state \\\nlisten \\\n--args host=localhost port=12345\n</code></pre>"},{"location":"spouts/udp/#udp.Udp.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_udp_spout:\nname: \"Udp\"\nmethod: \"listen\"\nargs:\nhost: \"localhost\"\nport: 12345\noutput:\ntype: \"streaming\"\nargs:\noutput_topic: \"udp_test\"\nkafka_servers: \"localhost:9094\"\nstate:\ntype: \"postgres\"\nargs:\npostgres_host: \"127.0.0.1\"\npostgres_port: 5432\npostgres_user: \"postgres\"\npostgres_password: \"postgres\"\npostgres_database: \"geniusrise\"\npostgres_table: \"state\"\ndeploy:\ntype: \"k8s\"\nargs:\nname: \"my_udp_spout\"\nnamespace: \"default\"\nimage: \"my_udp_spout_image\"\nreplicas: 1\n</code></pre>"},{"location":"spouts/udp/#udp.Udp.listen","title":"<code>listen(host='localhost', port=12345)</code>","text":"<p>\ud83d\udcd6 Start listening for data from the UDP server.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The UDP server host. Defaults to \"localhost\".</p> <code>'localhost'</code> <code>port</code> <code>int</code> <p>The UDP server port. Defaults to 12345.</p> <code>12345</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the UDP server.</p>"},{"location":"spouts/webhook/","title":"Webhook","text":"<p>Spout for Webhook</p> <p>             Bases: <code>Spout</code></p>"},{"location":"spouts/webhook/#webhook.Webhook.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the Webhook class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>StreamingOutput</code> <p>An instance of the StreamingOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"spouts/webhook/#webhook.Webhook.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius Webhook rise \\\nstreaming \\\n--output_kafka_topic webhook_test \\\n--output_kafka_cluster_connection_string localhost:9094 \\\npostgres \\\n--postgres_host 127.0.0.1 \\\n--postgres_port 5432 \\\n--postgres_user postgres \\\n--postgres_password postgres \\\n--postgres_database geniusrise \\\n--postgres_table state \\\nlisten \\\n--args endpoint=* port=3000\n</code></pre>"},{"location":"spouts/webhook/#webhook.Webhook.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_webhook_spout:\nname: \"Webhook\"\nmethod: \"listen\"\nargs:\nendpoint: \"*\"\nport: 3000\noutput:\ntype: \"streaming\"\nargs:\noutput_topic: \"webhook_test\"\nkafka_servers: \"localhost:9094\"\nstate:\ntype: \"postgres\"\nargs:\npostgres_host: \"127.0.0.1\"\npostgres_port: 5432\npostgres_user: \"postgres\"\npostgres_password: \"postgres\"\npostgres_database: \"geniusrise\"\npostgres_table: \"state\"\ndeploy:\ntype: \"k8s\"\nargs:\nname: \"my_webhook_spout\"\nnamespace: \"default\"\nimage: \"my_webhook_spout_image\"\nreplicas: 1\n</code></pre>"},{"location":"spouts/webhook/#webhook.Webhook.listen","title":"<code>listen(endpoint='*', port=3000, username=None, password=None)</code>","text":"<p>\ud83d\udcd6 Start listening for data from the webhook.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>The webhook endpoint to listen to. Defaults to \"*\".</p> <code>'*'</code> <code>port</code> <code>int</code> <p>The port to listen on. Defaults to 3000.</p> <code>3000</code> <code>username</code> <code>Optional[str]</code> <p>The username for basic authentication. Defaults to None.</p> <code>None</code> <code>password</code> <code>Optional[str]</code> <p>The password for basic authentication. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to start the CherryPy server.</p>"},{"location":"spouts/websocket/","title":"Websocket","text":"<p>Spout for Websocket</p> <p>             Bases: <code>Spout</code></p>"},{"location":"spouts/websocket/#websocket.Websocket.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the Websocket class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>StreamingOutput</code> <p>An instance of the StreamingOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"spouts/websocket/#websocket.Websocket.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius Websocket rise \\\nstreaming \\\n--output_kafka_topic websocket_test \\\n--output_kafka_cluster_connection_string localhost:9094 \\\npostgres \\\n--postgres_host 127.0.0.1 \\\n--postgres_port 5432 \\\n--postgres_user postgres \\\n--postgres_password postgres \\\n--postgres_database geniusrise \\\n--postgres_table state \\\nlisten \\\n--args host=localhost port=8765\n</code></pre>"},{"location":"spouts/websocket/#websocket.Websocket.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_websocket_spout:\nname: \"Websocket\"\nmethod: \"listen\"\nargs:\nhost: \"localhost\"\nport: 8765\noutput:\ntype: \"streaming\"\nargs:\noutput_topic: \"websocket_test\"\nkafka_servers: \"localhost:9094\"\nstate:\ntype: \"postgres\"\nargs:\npostgres_host: \"127.0.0.1\"\npostgres_port: 5432\npostgres_user: \"postgres\"\npostgres_password: \"postgres\"\npostgres_database: \"geniusrise\"\npostgres_table: \"state\"\ndeploy:\ntype: \"k8s\"\nargs:\nname: \"my_websocket_spout\"\nnamespace: \"default\"\nimage: \"my_websocket_spout_image\"\nreplicas: 1\n</code></pre>"},{"location":"spouts/websocket/#websocket.Websocket.__listen","title":"<code>__listen(host, port)</code>  <code>async</code>","text":"<p>Start listening for data from the WebSocket server.</p>"},{"location":"spouts/websocket/#websocket.Websocket.listen","title":"<code>listen(host='localhost', port=8765)</code>","text":"<p>\ud83d\udcd6 Start the WebSocket server.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The WebSocket server host. Defaults to \"localhost\".</p> <code>'localhost'</code> <code>port</code> <code>int</code> <p>The WebSocket server port. Defaults to 8765.</p> <code>8765</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to start the WebSocket server.</p>"},{"location":"spouts/websocket/#websocket.Websocket.receive_message","title":"<code>receive_message(websocket, path)</code>  <code>async</code>","text":"<p>Receive a message from a WebSocket client and save it along with metadata.</p> <p>Parameters:</p> Name Type Description Default <code>websocket</code> <p>WebSocket client connection.</p> required <code>path</code> <p>WebSocket path.</p> required"},{"location":"spouts/zeromq/","title":"ZeroMQ","text":"<p>Spout for ZeroMQ</p> <p>             Bases: <code>Spout</code></p>"},{"location":"spouts/zeromq/#zeromq.ZeroMQ.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the ZeroMQ class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>StreamingOutput</code> <p>An instance of the StreamingOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"spouts/zeromq/#zeromq.ZeroMQ.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius ZeroMQ rise \\\nstreaming \\\n--output_kafka_topic zmq_test \\\n--output_kafka_cluster_connection_string localhost:9094 \\\npostgres \\\n--postgres_host 127.0.0.1 \\\n--postgres_port 5432 \\\n--postgres_user postgres \\\n--postgres_password postgres \\\n--postgres_database geniusrise \\\n--postgres_table state \\\nlisten \\\n--args endpoint=tcp://localhost:5555 topic=my_topic syntax=json\n</code></pre>"},{"location":"spouts/zeromq/#zeromq.ZeroMQ.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_zmq_spout:\nname: \"ZeroMQ\"\nmethod: \"listen\"\nargs:\nendpoint: \"tcp://localhost:5555\"\ntopic: \"my_topic\"\nsyntax: \"json\"\noutput:\ntype: \"streaming\"\nargs:\noutput_topic: \"zmq_test\"\nkafka_servers: \"localhost:9094\"\nstate:\ntype: \"postgres\"\nargs:\npostgres_host: \"127.0.0.1\"\npostgres_port: 5432\npostgres_user: \"postgres\"\npostgres_password: \"postgres\"\npostgres_database: \"geniusrise\"\npostgres_table: \"state\"\ndeploy:\ntype: \"k8s\"\nargs:\nname: \"my_zmq_spout\"\nnamespace: \"default\"\nimage: \"my_zmq_spout_image\"\nreplicas: 1\n</code></pre>"},{"location":"spouts/zeromq/#zeromq.ZeroMQ.listen","title":"<code>listen(endpoint, topic, syntax, socket_type='SUB')</code>","text":"<p>\ud83d\udcd6 Start listening for data from the ZeroMQ server.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>The endpoint to connect to (e.g., \"tcp://localhost:5555\").</p> required <code>topic</code> <code>str</code> <p>The topic to subscribe to.</p> required <code>syntax</code> <code>str</code> <p>The syntax to be used (e.g., \"json\").</p> required <code>socket_type</code> <code>Optional[str]</code> <p>The type of ZeroMQ socket (default is \"SUB\").</p> <code>'SUB'</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the ZeroMQ server or process messages.</p>"}]}