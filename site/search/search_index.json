{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#geniusrise-microservices-ecosystem","title":"Geniusrise Microservices Ecosystem","text":"<p>Geniusrise is a modular, loosely-coupled AI-microservices framework.</p> <p>It can be used to perform various tasks, including hosting inference endpoints, performing bulk inference, fine tune etc with open source models or closed source APIs.</p> <ul> <li>The framework provides structure for modules and operationalizes and orchestrates them.</li> <li>The modular ecosystem provides a layer of abstraction over the myriad of models, libraries, tools, parameters and optimizations underlying the operationalization of modern AI models.</li> </ul> <p>Together the framework and ecosystem can be used for:</p> <ol> <li>Rapid prototyping by hosting APIs on a wide range of models<ol> <li>Host and experiment on local and iterate fast</li> <li>Deploy on kubernetes to production</li> </ol> </li> <li>Building AI-side components using the framework and CLI<ol> <li>Build complex AI microservices using multiple models</li> <li>Iterate fast from development to production</li> <li>Manage, scale and monitor deployments in production</li> <li>Build once, run anywhere</li> </ol> </li> <li>Using the ecosystem as a library: Many interesting applications can be built using this, e.g.:<ol> <li>A multi-cloud AI cloud, see geniusrise.com</li> <li>Local model pipeline server for personal or home IOT devices (e.g. a personal AI pin connected to voice-LLM pipeline hosted on desktop)</li> <li>Desktop and CLI applications</li> </ol> </li> </ol>"},{"location":"#guides","title":"Guides","text":""},{"location":"#getting-started","title":"\ud83d\ude80 Getting Started","text":"<ol> <li>\ud83d\udca5 Usage - TLDR Usage.</li> <li>\ud83d\udee0\ufe0f Installation - Installation and setup.</li> <li>\ud83d\udcd8 Concepts - Concepts of the framework, start here.</li> <li>\ud83c\udfd7\ufe0f Architecture - Design and architecture of the framework.</li> </ol>"},{"location":"#development","title":"\ud83d\udcbb Development","text":"<ol> <li>\ud83c\udfe0 Local Experimentation - Local setup and project creation.</li> <li>\ud83d\udd04 Dev Cycle - Describes one full local development cycle.</li> <li>\ud83d\udce6 Packaging - Packaging your application.</li> <li>\ud83d\ude80 Deployment - Deploying parts or whole of your application.</li> </ol>"},{"location":"#reference","title":"\ud83d\udcda Reference","text":"<ol> <li>\ud83d\udcc4 YAML Structure - Geniusfile structure and configuration.</li> <li>\ud83c\udf10 CLI reference - Command line reference.</li> <li>\ud83c\udfa8 Project Templates - Project templates for community plugins.</li> </ol>"},{"location":"#runners","title":"\ud83c\udfc3 Runners","text":"\ud83c\udf10 Runners \ud83d\udfe2 k8s deployment \ud83d\udfe4 k8s service \ud83d\udfe1 k8s job \ud83d\udfe0 k8s cron job \ud83d\udfe7 k8s pods \ud83d\udfe6 Apache Airflow \ud83d\udd34 Docker \ud83d\udfe3 Docker Swarm"},{"location":"#models","title":"\ud83e\udd16 Models","text":""},{"location":"#text-inference-apis","title":"Text Inference APIs","text":"\ud83c\udf10 Local &amp; Huggingface \ud83d\udfe2 Language Model \ud83d\udfe3 Named Entity Recognition \ud83d\udfe1 Question Answering \ud83d\udfe0 Sentiment Analysis \ud83d\udfe4 Summarization \ud83d\udfe6 Translation \ud83d\udd35 Classification \ud83d\udd34 Natural Language Inference \ud83d\udfe7 Instruction Tuning \ud83d\udfe7 Base"},{"location":"#text-bulk-inference","title":"Text Bulk Inference","text":"\ud83c\udf10 Local &amp; Huggingface \ud83d\udfe2 Language Model \ud83d\udfe3 Named Entity Recognition \ud83d\udfe1 Question Answering \ud83d\udfe0 Sentiment Analysis \ud83d\udfe4 Summarization \ud83d\udfe6 Translation \ud83d\udd35 Classification \ud83d\udd34 Natural Language Inference \ud83d\udfe7 Instruction Tuning \ud83d\udfe7 Base"},{"location":"#text-fine-tuning","title":"Text Fine-tuning","text":"\ud83c\udf10 Local &amp; Huggingface \ud83d\udfe2 Language Model \ud83d\udfe3 Named Entity Recognition \ud83d\udfe1 Question Answering \ud83d\udfe0 Sentiment Analysis \ud83d\udfe4 Summarization \ud83d\udfe6 Translation \ud83d\udd35 Classification \ud83d\udd34 Natural Language Inference \ud83d\udfe7 Instruction Tuning \ud83d\udfe7 Base \ud83c\udf10 OpenAI \ud83d\udfe2 Classification \ud83d\udfe3 Natural Language Inference \ud83d\udfe1 Instruction Tuning \ud83d\udfe0 Language Model \ud83d\udfe4 Named Entity Recognition \ud83d\udfe6 Question Answering \ud83d\udd35 Sentiment Analysis \ud83d\udd34 Summarization \ud83d\udfe7 Translation \ud83d\udfe7 Base"},{"location":"#vision-inference-apis","title":"Vision Inference APIs","text":"\ud83c\udf10 Local &amp; Huggingface \ud83d\udfe2 Image Classification \ud83d\udfe3 OCR \ud83d\udfe1 Image Segmentation \ud83d\udfe0 Visual Question Answering \ud83d\udfe4 Base"},{"location":"#audio-inference-apis","title":"Audio Inference APIs","text":"\ud83c\udf10 Local &amp; Huggingface \ud83d\udfe2 Text to Speech \ud83d\udfe3 Speech to Text \ud83d\udfe7 Base"},{"location":"#audio-bulk-inference","title":"Audio Bulk Inference","text":"\ud83c\udf10 Local &amp; Huggingface \ud83d\udfe2 Text to Speech \ud83d\udfe3 Speech to Text \ud83d\udfe7 Base"},{"location":"#data","title":"\u26a1 Data","text":""},{"location":"#ingestion","title":"Ingestion","text":"\ud83c\udf10 Streaming \ud83d\udfe2 Http Polling \ud83d\udfe3 Socket.io \ud83d\udfe1 gRPC \ud83d\udfe0 QUIC \ud83d\udfe4 UDP \ud83d\udd35 Webhook \ud83d\udfe5 Websocket \ud83d\udfe9 SNS \ud83d\udfe7 SQS \ud83d\udfe8 AMQP \ud83d\udfeb Kafka \ud83d\udfea Kinesis Streams \ud83d\udfe9 MQTT \ud83d\udfe8 ActiveMQ \ud83d\udfeb ZeroMQ \ud83d\udfea Redis Pubsub \ud83d\udfe7 Redis Streams \ud83d\udce6 Databases \ud83d\udfe2 HBase \ud83d\udfe3 PostgreSQL \ud83d\udd35 MySQL \ud83d\udfe0 MongoDB \ud83d\udfe2 Cassandra \ud83d\udfe3 Redis \ud83d\udd35 Elasticsearch \ud83d\udfe0 Oracle \ud83d\udfe2 SQL Server \ud83d\udfe3 SQLite \ud83d\udd35 Neo4j \ud83d\udfe0 Bigtable \ud83d\udfe2 DynamoDB \ud83d\udfe3 Azure Table Storage \ud83d\udd35 Couchbase \ud83d\udfe0 InfluxDB \ud83d\udfe2 TimescaleDB \ud83d\udfe3 Teradata \ud83d\udd35 TiDB \ud83d\udfe0 Voltdb \ud83d\udfe2 Sybase \ud83d\udfe3 DB2 \ud83d\udd35 AWS Presto \ud83d\udfe0 Riak \ud83d\udfe2 MemSQL \ud83d\udfe3 LDAP \ud83d\udd35 AWS KeySpaces \ud83d\udfe0 KairosDB \ud83d\udfe2 Graphite \ud83d\udfe3 Google FireStore \ud83d\udd35 AWS DocumentDB \ud83d\udfe0 Cockroach \ud83d\udfe2 Cloud SQL \ud83d\udfe3 Azure CosmosDB \ud83d\udd35 AWS Athena \ud83d\udfe0 ArangoDB \ud83d\udfe2 Nuodb \ud83d\udfe3 OpenTSDB \ud83d\udd35 Google Bigquery \ud83d\udfe0 Vertica \ud83d\udfe2 Google Spanner"},{"location":"#preprocessing","title":"Preprocessing","text":"\ud83c\udf10 Document Processing \ud83c\udf10 Image Processing \ud83c\udf10 OCR \ud83d\udfe3 Parse PDF \ud83d\udfe1 Predict image classes \ud83d\udd35 TROCRImageOCR \ud83d\udfe3 ParseCBZCBR \ud83d\udfe1 Train image classifier \ud83d\udd35 FineTuneTROCR \ud83d\udfe3 ParseDjvu \ud83d\udfe1 Convert Images \ud83d\udd35 TROCRImageOCRAPI \ud83d\udfe3 ParseEpub \ud83d\udfe2 Pix2StructImageOCR \ud83d\udfe3 ParseMOBI \ud83d\udfe2 Pix2StructImageOCRAPI \ud83d\udfe3 ParsePostScript \ud83d\udfe2 FineTunePix2Struct \ud83d\udfe3 ParseXPS"},{"location":"#library","title":"\ud83d\udcda Library","text":"\ud83d\udce6 cli \ud83d\udce6 core \ud83d\udce6 data \ud83d\udce6 core.state \ud83d\udfe0 geniusctl \ud83d\udfe2 bolt \ud83d\udfe3 input \ud83d\udd34 base \ud83d\udfe0 yamlctl \ud83d\udfe2 spout \ud83d\udfe3 output \ud83d\udd34 dynamo \ud83d\udfe0 boltctl \ud83d\udfe4 base \ud83d\udfe3 batch_input \ud83d\udd34 memory \ud83d\udfe0 spoutctl \ud83d\udfe3 batch_output \ud83d\udd34 postgres \ud83d\udfe0 schema \ud83d\udfe3 streaming_input \ud83d\udd34 redis \ud83d\udfe0 discover \ud83d\udfe3 streaming_output \ud83d\udfe0 docker"},{"location":"audio/api/base/","title":"Base API","text":"<p>             Bases: <code>AudioBulk</code></p> <p>A class representing a Hugging Face API for generating text using a pre-trained language model.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>Any</code> <p>The pre-trained language model.</p> <code>processor</code> <code>Any</code> <p>The processor used to preprocess input text.</p> <code>model_name</code> <code>str</code> <p>The name of the pre-trained language model.</p> <code>model_revision</code> <code>Optional[str]</code> <p>The revision of the pre-trained language model.</p> <code>processor_name</code> <code>str</code> <p>The name of the processor used to preprocess input text.</p> <code>processor_revision</code> <code>Optional[str]</code> <p>The revision of the processor used to preprocess input text.</p> <code>model_class</code> <code>str</code> <p>The name of the class of the pre-trained language model.</p> <code>processor_class</code> <code>str</code> <p>The name of the class of the processor used to preprocess input text.</p> <code>use_cuda</code> <code>bool</code> <p>Whether to use a GPU for inference.</p> <code>quantization</code> <code>int</code> <p>The level of quantization to use for the pre-trained language model.</p> <code>precision</code> <code>str</code> <p>The precision to use for the pre-trained language model.</p> <code>device_map</code> <code>str | Dict | None</code> <p>The mapping of devices to use for inference.</p> <code>max_memory</code> <code>Dict[int, str]</code> <p>The maximum memory to use for inference.</p> <code>torchscript</code> <code>bool</code> <p>Whether to use a TorchScript-optimized version of the pre-trained language model.</p> <code>model_args</code> <code>Any</code> <p>Additional arguments to pass to the pre-trained language model.</p> Methods <p>text(**kwargs: Any) -&gt; Dict[str, Any]:     Generates text based on the given prompt and decoding strategy.</p> <p>listen(model_name: str, model_class: str = \"AutoModelForCausalLM\", processor_class: str = \"AutoProcessor\", use_cuda: bool = False, precision: str = \"float16\", quantization: int = 0, device_map: str | Dict | None = \"auto\", max_memory={0: \"24GB\"}, torchscript: bool = True, endpoint: str = \"\", port: int = 3000, cors_domain: str = \"http://localhost:3000\", username: Optional[str] = None, password: Optional[str] = None, *model_args: Any) -&gt; None:     Starts a CherryPy server to listen for requests to generate text.</p>"},{"location":"audio/api/base/#geniusrise_audio.base.api.AudioAPI.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>Initializes a new instance of the TextAPI class.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>The input data to process.</p> required <code>output</code> <code>BatchOutput</code> <p>The output data to process.</p> required <code>state</code> <code>State</code> <p>The state of the API.</p> required"},{"location":"audio/api/base/#geniusrise_audio.base.api.AudioAPI.listen","title":"<code>listen(model_name, model_class='AutoModel', processor_class='AutoProcessor', use_cuda=False, precision='float16', quantization=0, device_map='auto', max_memory={0: '24GB'}, torchscript=False, compile=False, concurrent_queries=False, use_whisper_cpp=False, endpoint='*', port=3000, cors_domain='http://localhost:3000', username=None, password=None, **model_args)</code>","text":"<p>Starts a CherryPy server to listen for requests to generate text.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the pre-trained language model.</p> required <code>model_class</code> <code>str</code> <p>The name of the class of the pre-trained language model. Defaults to \"AutoModelForCausalLM\".</p> <code>'AutoModel'</code> <code>processor_class</code> <code>str</code> <p>The name of the class of the processor used to preprocess input text. Defaults to \"AutoProcessor\".</p> <code>'AutoProcessor'</code> <code>use_cuda</code> <code>bool</code> <p>Whether to use a GPU for inference. Defaults to False.</p> <code>False</code> <code>precision</code> <code>str</code> <p>The precision to use for the pre-trained language model. Defaults to \"float16\".</p> <code>'float16'</code> <code>quantization</code> <code>int</code> <p>The level of quantization to use for the pre-trained language model. Defaults to 0.</p> <code>0</code> <code>device_map</code> <code>str | Dict | None</code> <p>The mapping of devices to use for inference. Defaults to \"auto\".</p> <code>'auto'</code> <code>max_memory</code> <code>Dict[int, str]</code> <p>The maximum memory to use for inference. Defaults to {0: \"24GB\"}.</p> <code>{0: '24GB'}</code> <code>torchscript</code> <code>bool</code> <p>Whether to use a TorchScript-optimized version of the pre-trained language model. Defaults to True.</p> <code>False</code> <code>compile</code> <code>bool</code> <p>Enable Torch JIT compilation.</p> <code>False</code> <code>concurrent_queries</code> <code>bool</code> <p>(bool): Whether the API supports concurrent API calls (usually false).</p> <code>False</code> <code>use_whisper_cpp</code> <code>bool</code> <p>Whether to use whisper.cpp to load the model. Defaults to False. Note: only works for these models: https://github.com/aarnphm/whispercpp/blob/524dd6f34e9d18137085fb92a42f1c31c9c6bc29/src/whispercpp/utils.py#L32</p> <code>False</code> <code>endpoint</code> <code>str</code> <p>The endpoint to listen on. Defaults to \"*\".</p> <code>'*'</code> <code>port</code> <code>int</code> <p>The port to listen on. Defaults to 3000.</p> <code>3000</code> <code>cors_domain</code> <code>str</code> <p>The domain to allow CORS requests from. Defaults to \"http://localhost:3000\".</p> <code>'http://localhost:3000'</code> <code>username</code> <code>Optional[str]</code> <p>The username to use for authentication. Defaults to None.</p> <code>None</code> <code>password</code> <code>Optional[str]</code> <p>The password to use for authentication. Defaults to None.</p> <code>None</code> <code>**model_args</code> <code>Any</code> <p>Additional arguments to pass to the pre-trained language model.</p> <code>{}</code>"},{"location":"audio/api/base/#geniusrise_audio.base.api.AudioAPI.validate_password","title":"<code>validate_password(realm, username, password)</code>","text":"<p>Validate the username and password against expected values.</p> <p>Parameters:</p> Name Type Description Default <code>realm</code> <code>str</code> <p>The authentication realm.</p> required <code>username</code> <code>str</code> <p>The provided username.</p> required <code>password</code> <code>str</code> <p>The provided password.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if credentials are valid, False otherwise.</p>"},{"location":"audio/api/s2t/","title":"Speech to Text","text":"<p>             Bases: <code>AudioAPI</code></p> <p>SpeechToTextAPI is a subclass of AudioAPI specifically designed for speech-to-text models. It extends the functionality to handle speech-to-text processing using various ASR models.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>AutoModelForCTC</code> <p>The speech-to-text model.</p> <code>processor</code> <code>AutoProcessor</code> <p>The processor to prepare input audio data for the model.</p> Methods <p>transcribe(audio_input: bytes) -&gt; str:     Transcribes the given audio input to text using the speech-to-text model.</p> <p>Example CLI Usage:</p> <pre><code>genius SpeechToTextAPI rise \\\nbatch \\\n--input_folder ./input \\\nbatch \\\n--output_folder ./output \\\nnone \\\n--id facebook/wav2vec2-large-960h-lv60-self \\\nlisten \\\n--args \\\nmodel_name=\"facebook/wav2vec2-large-960h-lv60-self\" \\\nmodel_class=\"Wav2Vec2ForCTC\" \\\nprocessor_class=\"Wav2Vec2Processor\" \\\nuse_cuda=True \\\nprecision=\"float32\" \\\nquantization=0 \\\ndevice_map=\"cuda:0\" \\\nmax_memory=None \\\ntorchscript=False \\\ncompile=True \\\nendpoint=\"*\" \\\nport=3000 \\\ncors_domain=\"http://localhost:3000\" \\\nusername=\"user\" \\\npassword=\"password\"\n</code></pre> <p>or using whisper.cpp:</p> <pre><code>genius SpeechToTextAPI rise \\\nbatch \\\n--input_folder ./input \\\nbatch \\\n--output_folder ./output \\\nnone \\\nlisten \\\n--args \\\nmodel_name=\"large\" \\\nuse_whisper_cpp=True \\\nendpoint=\"*\" \\\nport=3000 \\\ncors_domain=\"http://localhost:3000\" \\\nusername=\"user\" \\\npassword=\"password\"\n</code></pre>"},{"location":"audio/api/s2t/#geniusrise_audio.s2t.api.SpeechToTextAPI.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>Initializes the SpeechToTextAPI with configurations for speech-to-text processing.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>The input data configuration.</p> required <code>output</code> <code>BatchOutput</code> <p>The output data configuration.</p> required <code>state</code> <code>State</code> <p>The state configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"audio/api/s2t/#geniusrise_audio.s2t.api.SpeechToTextAPI.asr_pipeline","title":"<code>asr_pipeline(**kwargs)</code>","text":"<p>Recognizes named entities in the input text using the Hugging Face pipeline.</p> <p>This method leverages a pre-trained NER model to identify and classify entities in text into categories such as names, organizations, locations, etc. It's suitable for processing various types of text content.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments, typically containing 'text' for the input text.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the original input text and a list of recognized entities.</p> <p>Example CURL Request for transcription: <pre><code>(base64 -w 0 sample.flac | awk '{print \"{\\\"audio_file\\\": \\\"\"$0\"\\\", \\\"model_sampling_rate\\\": 16000, \\\"chunk_length_s\\\": 60}\"}' &gt; /tmp/payload.json)\ncurl -X POST http://localhost:3000/api/v1/asr_pipeline \\\n-H \"Content-Type: application/json\" \\\n-u user:password \\\n-d @/tmp/payload.json | jq\n</code></pre></p>"},{"location":"audio/api/s2t/#geniusrise_audio.s2t.api.SpeechToTextAPI.initialize_pipeline","title":"<code>initialize_pipeline()</code>","text":"<p>Lazy initialization of the NER Hugging Face pipeline.</p>"},{"location":"audio/api/s2t/#geniusrise_audio.s2t.api.SpeechToTextAPI.process_seamless","title":"<code>process_seamless(audio_input, model_sampling_rate, processor_args, chunk_size, overlap_size, generate_args)</code>","text":"<p>Process audio input with the Whisper model.</p>"},{"location":"audio/api/s2t/#geniusrise_audio.s2t.api.SpeechToTextAPI.process_wav2vec2","title":"<code>process_wav2vec2(audio_input, model_sampling_rate, processor_args, chunk_size, overlap_size)</code>","text":"<p>Process audio input with the Wav2Vec2 model.</p>"},{"location":"audio/api/s2t/#geniusrise_audio.s2t.api.SpeechToTextAPI.process_whisper","title":"<code>process_whisper(audio_input, model_sampling_rate, processor_args, chunk_size, overlap_size, generate_args)</code>","text":"<p>Process audio input with the Whisper model.</p>"},{"location":"audio/api/s2t/#geniusrise_audio.s2t.api.SpeechToTextAPI.transcribe","title":"<code>transcribe()</code>","text":"<p>API endpoint to transcribe the given audio input to text using the speech-to-text model. Expects a JSON input with 'audio_file' as a key containing the base64 encoded audio data.</p> <p>Returns:</p> Type Description <p>Dict[str, str]: A dictionary containing the transcribed text.</p> <p>Example CURL Request for transcription: <pre><code>(base64 -w 0 sample.flac | awk '{print \"{\\\"audio_file\\\": \\\"\"$0\"\\\", \\\"model_sampling_rate\\\": 16000, \\\"chunk_size\\\": 1280000, \\\"overlap_size\\\": 213333, \\\"do_sample\\\": true, \\\"num_beams\\\": 4, \\\"temperature\\\": 0.6, \\\"tgt_lang\\\": \\\"eng\\\"}\"}' &gt; /tmp/payload.json)\ncurl -X POST http://localhost:3000/api/v1/transcribe \\\n-H \"Content-Type: application/json\" \\\n-u user:password \\\n-d @/tmp/payload.json | jq\n</code></pre></p>"},{"location":"audio/api/t2s/","title":"Text to Speech","text":"<p>             Bases: <code>AudioAPI</code></p> <p>TextToSpeechAPI for converting text to speech using various TTS models.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>AutoModelForSeq2SeqLM</code> <p>The text-to-speech model.</p> <code>tokenizer</code> <code>AutoTokenizer</code> <p>The tokenizer for the model.</p> Methods <p>synthesize(text_input: str) -&gt; bytes:     Converts the given text input to speech using the text-to-speech model.</p> <p>Example CLI Usage:</p> <pre><code>genius TextToSpeechAPI rise \\\nbatch \\\n    --input_folder ./input \\\nbatch \\\n    --output_folder ./output \\\nnone \\\n    --id facebook/mms-tts-eng \\\n    listen \\\n        --args \\\n            model_name=\"facebook/mms-tts-eng\" \\\n            model_class=\"VitsModel\" \\\n            processor_class=\"VitsTokenizer\" \\\n            use_cuda=True \\\n            precision=\"float32\" \\\n            quantization=0 \\\n            device_map=\"cuda:0\" \\\n            max_memory=None \\\n            torchscript=False \\\n            compile=False \\\n            endpoint=\"*\" \\\n            port=3000 \\\n            cors_domain=\"http://localhost:3000\" \\\n            username=\"user\" \\\n            password=\"password\"\n</code></pre>"},{"location":"audio/api/t2s/#geniusrise_audio.t2s.api.TextToSpeechAPI.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>Initializes the TextToSpeechAPI with configurations for text-to-speech processing.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>The input data configuration.</p> required <code>output</code> <code>BatchOutput</code> <p>The output data configuration.</p> required <code>state</code> <code>State</code> <p>The state configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"audio/api/t2s/#geniusrise_audio.t2s.api.TextToSpeechAPI.initialize_pipeline","title":"<code>initialize_pipeline()</code>","text":"<p>Lazy initialization of the TTS Hugging Face pipeline.</p>"},{"location":"audio/api/t2s/#geniusrise_audio.t2s.api.TextToSpeechAPI.synthesize","title":"<code>synthesize()</code>","text":"<p>API endpoint to convert text input to speech using the text-to-speech model. Expects a JSON input with 'text' as a key containing the text to be synthesized.</p> <p>Returns:</p> Type Description <p>Dict[str, str]: A dictionary containing the base64 encoded audio data.</p> <p>Example CURL Request for synthesis: ... [Provide example CURL request] ...</p>"},{"location":"audio/api/t2s/#geniusrise_audio.t2s.api.TextToSpeechAPI.tts_pipeline","title":"<code>tts_pipeline(**kwargs)</code>","text":"<p>Converts text to speech using the Hugging Face pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments, typically containing 'text' for the input text.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the base64 encoded audio data.</p> <p>Example CURL Request for synthesis: ... [Provide example CURL request] ...</p>"},{"location":"audio/bulk/base/","title":"Base Bulk Inference","text":"<p>             Bases: <code>Bolt</code></p> <p>AudioBulk is a class designed for bulk processing of audio data using various audio models from Hugging Face. It focuses on audio generation and transformation tasks, supporting a range of models and configurations.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>AutoModelForAudioClassification</code> <p>The audio model for generation or transformation tasks.</p> <code>processor</code> <code>AutoFeatureExtractor</code> <p>The processor for preparing input data for the model.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>Configuration and data inputs for the batch process.</p> required <code>output</code> <code>BatchOutput</code> <p>Configurations for output data handling.</p> required <code>state</code> <code>State</code> <p>State management for the Bolt.</p> required <code>**kwargs</code> <p>Arbitrary keyword arguments for extended configurations.</p> <code>{}</code> Methods <p>audio(**kwargs: Any) -&gt; Dict[str, Any]:     Provides an API endpoint for audio processing functionality.     Accepts various parameters for customizing the audio processing tasks.</p> <p>process(audio_input: Union[str, bytes], **processing_params: Any) -&gt; dict:     Processes the audio input based on the provided parameters. Supports multiple processing methods.</p>"},{"location":"audio/bulk/base/#geniusrise_audio.base.bulk.AudioBulk.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>Initializes the AudioBulk with configurations and sets up logging. Prepares the environment for audio processing tasks.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>The input data configuration for the audio processing task.</p> required <code>output</code> <code>BatchOutput</code> <p>The output data configuration for the results of the audio processing.</p> required <code>state</code> <code>State</code> <p>The state configuration for the Bolt, managing its operational status.</p> required <code>**kwargs</code> <p>Additional keyword arguments for extended functionality and model configurations.</p> <code>{}</code>"},{"location":"audio/bulk/base/#geniusrise_audio.base.bulk.AudioBulk.done","title":"<code>done()</code>","text":"<p>Finalizes the AudioBulk processing. Sends notification email if configured.</p> <p>This method should be called after all audio processing tasks are complete. It handles any final steps such as sending notifications or cleaning up resources.</p>"},{"location":"audio/bulk/base/#geniusrise_audio.base.bulk.AudioBulk.load_models","title":"<code>load_models(model_name, processor_name, model_revision=None, processor_revision=None, model_class='', processor_class='AutoFeatureExtractor', use_cuda=False, precision='float16', quantization=0, device_map='auto', max_memory={0: '24GB'}, torchscript=False, compile=False, flash_attention=False, better_transformers=False, **model_args)</code>","text":"<p>Loads and configures the specified audio model and processor for audio processing.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name or path of the audio model to load.</p> required <code>processor_name</code> <code>str</code> <p>Name or path of the processor to load.</p> required <code>model_revision</code> <code>Optional[str]</code> <p>Specific model revision to load (e.g., commit hash).</p> <code>None</code> <code>processor_revision</code> <code>Optional[str]</code> <p>Specific processor revision to load.</p> <code>None</code> <code>model_class</code> <code>str</code> <p>Class of the model to be loaded.</p> <code>''</code> <code>processor_class</code> <code>str</code> <p>Class of the processor to be loaded.</p> <code>'AutoFeatureExtractor'</code> <code>use_cuda</code> <code>bool</code> <p>Flag to use CUDA for GPU acceleration.</p> <code>False</code> <code>precision</code> <code>str</code> <p>Desired precision for computations (\"float32\", \"float16\", etc.).</p> <code>'float16'</code> <code>quantization</code> <code>int</code> <p>Bit level for model quantization (0 for none, 8 for 8-bit).</p> <code>0</code> <code>device_map</code> <code>Union[str, Dict, None]</code> <p>Specific device(s) for model operations.</p> <code>'auto'</code> <code>max_memory</code> <code>Dict[int, str]</code> <p>Maximum memory allocation for the model.</p> <code>{0: '24GB'}</code> <code>torchscript</code> <code>bool</code> <p>Enable TorchScript for model optimization.</p> <code>False</code> <code>compile</code> <code>bool</code> <p>Enable Torch JIT compilation.</p> <code>False</code> <code>flash_attention</code> <code>bool</code> <p>Flag to enable Flash Attention optimization for faster processing.</p> <code>False</code> <code>better_transformers</code> <code>bool</code> <p>Flag to enable Better Transformers optimization for faster processing.</p> <code>False</code> <code>**model_args</code> <code>Any</code> <p>Additional arguments for model loading.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[AutoModelForAudioClassification, AutoFeatureExtractor]</code> <p>Tuple[AutoModelForAudioClassification, AutoFeatureExtractor]: Loaded model and processor.</p>"},{"location":"audio/bulk/s2t/","title":"Speech to Text","text":"<p>             Bases: <code>AudioAPI</code></p> <p>SpeechToTextAPI is a subclass of AudioAPI specifically designed for speech-to-text models. It extends the functionality to handle speech-to-text processing using various ASR models.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>AutoModelForCTC</code> <p>The speech-to-text model.</p> <code>processor</code> <code>AutoProcessor</code> <p>The processor to prepare input audio data for the model.</p> Methods <p>transcribe(audio_input: bytes) -&gt; str:     Transcribes the given audio input to text using the speech-to-text model.</p> <p>Example CLI Usage:</p> <pre><code>genius SpeechToTextAPI rise \\\nbatch \\\n--input_folder ./input \\\nbatch \\\n--output_folder ./output \\\nnone \\\n--id facebook/wav2vec2-large-960h-lv60-self \\\nlisten \\\n--args \\\nmodel_name=\"facebook/wav2vec2-large-960h-lv60-self\" \\\nmodel_class=\"Wav2Vec2ForCTC\" \\\nprocessor_class=\"Wav2Vec2Processor\" \\\nuse_cuda=True \\\nprecision=\"float32\" \\\nquantization=0 \\\ndevice_map=\"cuda:0\" \\\nmax_memory=None \\\ntorchscript=False \\\ncompile=True \\\nendpoint=\"*\" \\\nport=3000 \\\ncors_domain=\"http://localhost:3000\" \\\nusername=\"user\" \\\npassword=\"password\"\n</code></pre> <p>or using whisper.cpp:</p> <pre><code>genius SpeechToTextAPI rise \\\nbatch \\\n--input_folder ./input \\\nbatch \\\n--output_folder ./output \\\nnone \\\nlisten \\\n--args \\\nmodel_name=\"large\" \\\nuse_whisper_cpp=True \\\nendpoint=\"*\" \\\nport=3000 \\\ncors_domain=\"http://localhost:3000\" \\\nusername=\"user\" \\\npassword=\"password\"\n</code></pre>"},{"location":"audio/bulk/s2t/#geniusrise_audio.s2t.api.SpeechToTextAPI.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>Initializes the SpeechToTextAPI with configurations for speech-to-text processing.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>The input data configuration.</p> required <code>output</code> <code>BatchOutput</code> <p>The output data configuration.</p> required <code>state</code> <code>State</code> <p>The state configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"audio/bulk/s2t/#geniusrise_audio.s2t.api.SpeechToTextAPI.asr_pipeline","title":"<code>asr_pipeline(**kwargs)</code>","text":"<p>Recognizes named entities in the input text using the Hugging Face pipeline.</p> <p>This method leverages a pre-trained NER model to identify and classify entities in text into categories such as names, organizations, locations, etc. It's suitable for processing various types of text content.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments, typically containing 'text' for the input text.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the original input text and a list of recognized entities.</p> <p>Example CURL Request for transcription: <pre><code>(base64 -w 0 sample.flac | awk '{print \"{\\\"audio_file\\\": \\\"\"$0\"\\\", \\\"model_sampling_rate\\\": 16000, \\\"chunk_length_s\\\": 60}\"}' &gt; /tmp/payload.json)\ncurl -X POST http://localhost:3000/api/v1/asr_pipeline \\\n-H \"Content-Type: application/json\" \\\n-u user:password \\\n-d @/tmp/payload.json | jq\n</code></pre></p>"},{"location":"audio/bulk/s2t/#geniusrise_audio.s2t.api.SpeechToTextAPI.initialize_pipeline","title":"<code>initialize_pipeline()</code>","text":"<p>Lazy initialization of the NER Hugging Face pipeline.</p>"},{"location":"audio/bulk/s2t/#geniusrise_audio.s2t.api.SpeechToTextAPI.process_seamless","title":"<code>process_seamless(audio_input, model_sampling_rate, processor_args, chunk_size, overlap_size, generate_args)</code>","text":"<p>Process audio input with the Whisper model.</p>"},{"location":"audio/bulk/s2t/#geniusrise_audio.s2t.api.SpeechToTextAPI.process_wav2vec2","title":"<code>process_wav2vec2(audio_input, model_sampling_rate, processor_args, chunk_size, overlap_size)</code>","text":"<p>Process audio input with the Wav2Vec2 model.</p>"},{"location":"audio/bulk/s2t/#geniusrise_audio.s2t.api.SpeechToTextAPI.process_whisper","title":"<code>process_whisper(audio_input, model_sampling_rate, processor_args, chunk_size, overlap_size, generate_args)</code>","text":"<p>Process audio input with the Whisper model.</p>"},{"location":"audio/bulk/s2t/#geniusrise_audio.s2t.api.SpeechToTextAPI.transcribe","title":"<code>transcribe()</code>","text":"<p>API endpoint to transcribe the given audio input to text using the speech-to-text model. Expects a JSON input with 'audio_file' as a key containing the base64 encoded audio data.</p> <p>Returns:</p> Type Description <p>Dict[str, str]: A dictionary containing the transcribed text.</p> <p>Example CURL Request for transcription: <pre><code>(base64 -w 0 sample.flac | awk '{print \"{\\\"audio_file\\\": \\\"\"$0\"\\\", \\\"model_sampling_rate\\\": 16000, \\\"chunk_size\\\": 1280000, \\\"overlap_size\\\": 213333, \\\"do_sample\\\": true, \\\"num_beams\\\": 4, \\\"temperature\\\": 0.6, \\\"tgt_lang\\\": \\\"eng\\\"}\"}' &gt; /tmp/payload.json)\ncurl -X POST http://localhost:3000/api/v1/transcribe \\\n-H \"Content-Type: application/json\" \\\n-u user:password \\\n-d @/tmp/payload.json | jq\n</code></pre></p>"},{"location":"audio/bulk/t2s/","title":"Text to Speech","text":"<p>             Bases: <code>AudioAPI</code></p> <p>TextToSpeechAPI for converting text to speech using various TTS models.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>AutoModelForSeq2SeqLM</code> <p>The text-to-speech model.</p> <code>tokenizer</code> <code>AutoTokenizer</code> <p>The tokenizer for the model.</p> Methods <p>synthesize(text_input: str) -&gt; bytes:     Converts the given text input to speech using the text-to-speech model.</p> <p>Example CLI Usage:</p> <pre><code>genius TextToSpeechAPI rise \\\nbatch \\\n    --input_folder ./input \\\nbatch \\\n    --output_folder ./output \\\nnone \\\n    --id facebook/mms-tts-eng \\\n    listen \\\n        --args \\\n            model_name=\"facebook/mms-tts-eng\" \\\n            model_class=\"VitsModel\" \\\n            processor_class=\"VitsTokenizer\" \\\n            use_cuda=True \\\n            precision=\"float32\" \\\n            quantization=0 \\\n            device_map=\"cuda:0\" \\\n            max_memory=None \\\n            torchscript=False \\\n            compile=False \\\n            endpoint=\"*\" \\\n            port=3000 \\\n            cors_domain=\"http://localhost:3000\" \\\n            username=\"user\" \\\n            password=\"password\"\n</code></pre>"},{"location":"audio/bulk/t2s/#geniusrise_audio.t2s.api.TextToSpeechAPI.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>Initializes the TextToSpeechAPI with configurations for text-to-speech processing.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>The input data configuration.</p> required <code>output</code> <code>BatchOutput</code> <p>The output data configuration.</p> required <code>state</code> <code>State</code> <p>The state configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"audio/bulk/t2s/#geniusrise_audio.t2s.api.TextToSpeechAPI.initialize_pipeline","title":"<code>initialize_pipeline()</code>","text":"<p>Lazy initialization of the TTS Hugging Face pipeline.</p>"},{"location":"audio/bulk/t2s/#geniusrise_audio.t2s.api.TextToSpeechAPI.synthesize","title":"<code>synthesize()</code>","text":"<p>API endpoint to convert text input to speech using the text-to-speech model. Expects a JSON input with 'text' as a key containing the text to be synthesized.</p> <p>Returns:</p> Type Description <p>Dict[str, str]: A dictionary containing the base64 encoded audio data.</p> <p>Example CURL Request for synthesis: ... [Provide example CURL request] ...</p>"},{"location":"audio/bulk/t2s/#geniusrise_audio.t2s.api.TextToSpeechAPI.tts_pipeline","title":"<code>tts_pipeline(**kwargs)</code>","text":"<p>Converts text to speech using the Hugging Face pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments, typically containing 'text' for the input text.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the base64 encoded audio data.</p> <p>Example CURL Request for synthesis: ... [Provide example CURL request] ...</p>"},{"location":"blog/huggingface/chat/","title":"Host Chat Models Using Geniusrise","text":"<p>Integrating chat models into applications can dramatically enhance user interaction, making it more engaging and intuitive. Geniusrise offers a powerful and flexible way to deploy state-of-the-art chat models as APIs. This guide explores how to set up these APIs for various use cases, focusing on two types: standard and Very Large Language Models (VLLM).</p>"},{"location":"blog/huggingface/chat/#quick-setup","title":"Quick Setup","text":"<p>Installation:</p> <p>To get started, install Geniusrise and its vision package:</p> <pre><code>pip install geniusrise\npip install geniusrise-vision\n</code></pre> <p>Configuration File (<code>genius.yml</code>):</p> <p>Your configuration file is crucial. Here\u2019s how you set it up for a standard chat model and a VLLM.</p>"},{"location":"blog/huggingface/chat/#standard-chat-model-example","title":"Standard Chat Model Example:","text":"<pre><code>version: \"1\"\nbolts:\nmy_bolt:\nname: ChatAPI\nstate:\ntype: none\ninput:\ntype: batch\nargs:\ninput_folder: ./input\noutput:\ntype: batch\nargs:\noutput_folder: ./output\nmethod: listen\nargs:\nmodel_name: \"codellama/CodeLlama-7b-Instruct-hf\"\nmodel_class: \"AutoModelForCausalLM\"\ntokenizer_class: \"AutoTokenizer\"\nuse_cuda: True\nprecision: \"float16\"\ndevice_map: \"auto\"\nendpoint: \"*\"\nport: 3000\ncors_domain: \"http://localhost:3000\"\nusername: \"user\"\npassword: \"password\"\n</code></pre>"},{"location":"blog/huggingface/chat/#vllm-example","title":"VLLM Example:","text":"<p>For models like GPT-3, which require handling of larger context sizes and nuanced generation controls:</p> <pre><code>version: \"1\"\nbolts:\nmy_bolt:\nname: ChatVLLMAPI\nstate:\ntype: none\ninput:\ntype: batch\nargs:\ninput_folder: ./input\noutput:\ntype: batch\nargs:\noutput_folder: ./output\nmethod: listen\nargs:\nmodel_name: \"mistralai/Mistral-7B-Instruct-v0.1\"\nmodel_class: \"AutoModelForCausalLM\"\ntokenizer_class: \"AutoTokenizer\"\nuse_cuda: True\nprecision: \"bfloat16\"\ndevice_map: \"auto\"\nuse_vllm: True\nvllm_enforce_eager: True\nvllm_max_model_len: 1024\nendpoint: \"*\"\nport: 3000\ncors_domain: \"http://localhost:3000\"\nusername: \"user\"\npassword: \"password\"\n</code></pre>"},{"location":"blog/huggingface/chat/#interacting-with-your-api","title":"Interacting with Your API","text":"<p>For a chat model, you would typically send a prompt and receive a generated response:</p> <pre><code>curl -X POST \"http://localhost:3000/api/v1/chat\" \\\n-H \"Content-Type: application/json\" \\\n-u \"user:password\" \\\n-d '{\"prompt\": \"Your prompt here\", \"max_tokens\": 50}'\n</code></pre> <p>For VLLMs, you might want to control more parameters due to their capacity for larger context and nuanced outputs:</p> <pre><code>curl -X POST \"http://localhost:3000/api/v1/chat_vllm\" \\\n-H \"Content-Type: application/json\" \\\n-u \"user:password\" \\\n-d '{\n        \"messages\": [{\"role\": \"user\", \"content\": \"Your prompt here\"}],\n        \"max_tokens\": 50,\n        \"temperature\": 0.7\n    }'\n</code></pre>"},{"location":"blog/huggingface/chat/#use-cases-variations","title":"Use Cases &amp; Variations","text":""},{"location":"blog/huggingface/chat/#general-chat","title":"General Chat","text":"<p>Use models like <code>mistralai/Mistral-7B-Instruct-v0.1</code> for general-purpose chatting, answering questions, or providing instructions.</p>"},{"location":"blog/huggingface/chat/#code-generation","title":"Code Generation","text":"<p>For specialized tasks like coding questions, deploy models such as <code>codellama/CodeLlama-7b-Instruct-hf</code> to assist users in solving programming challenges.</p>"},{"location":"blog/huggingface/chat/#multilingual-conversations","title":"Multilingual Conversations","text":"<p>Deploy multilingual models to engage users in their native languages, enhancing accessibility and user experience.</p>"},{"location":"blog/huggingface/chat/#configuration-tips","title":"Configuration Tips","text":"<ul> <li>Model Selection: Tailor your choice of model based on the specific needs and language requirements of your application.</li> <li>Precision and CUDA: Adjust these settings based on your computational resources to optimize performance.</li> <li>VLLM Settings: For VLLMs, fine-tune parameters like <code>use_vllm</code>, <code>vllm_enforce_eager</code>, and <code>vllm_max_model_len</code> to handle complex conversations effectively.</li> </ul>"},{"location":"blog/huggingface/imgclass/","title":"Host Image Classification Models Using Geniusrise","text":"<p>Image classification is a cornerstone of machine learning and computer vision, providing the backbone for a myriad of applications from photo organization to medical imaging. With Geniusrise, developers can effortlessly deploy image classification models as APIs, making these powerful tools accessible for integration into various applications. This guide highlights the process of setting up image classification APIs using Geniusrise, offering a range of use cases and configurations.</p>"},{"location":"blog/huggingface/imgclass/#quick-setup","title":"Quick Setup","text":"<p>Installation:</p> <p>To start, ensure Geniusrise and its text extension are installed:</p> <pre><code>pip install geniusrise\npip install geniusrise-vision\n</code></pre> <p>Configuration File (<code>genius.yml</code>):</p> <p>Your <code>genius.yml</code> configuration will outline the API's structure. Below is a template adjusted for image classification:</p> <pre><code>version: \"1\"\nbolts:\nmy_bolt:\nname: ImageClassificationAPI\nstate:\ntype: none\ninput:\ntype: batch\nargs:\ninput_folder: ./input\noutput:\ntype: batch\nargs:\noutput_folder: ./output\nmethod: listen\nargs:\nmodel_name: \"google/vit-base-patch16-224\"\nmodel_class: \"AutoModelForImageClassification\"\nprocessor_class: \"AutoImageProcessor\"\ndevice_map: \"cuda:0\"\nuse_cuda: true\nprecision: \"float\"\nendpoint: \"0.0.0.0\"\nport: 3000\ncors_domain: \"http://localhost:3000\"\nusername: \"user\"\npassword: \"password\"\n</code></pre> <p>Activate your API by executing:</p> <pre><code>genius rise\n</code></pre>"},{"location":"blog/huggingface/imgclass/#configuration-parameters-explained","title":"Configuration Parameters Explained","text":"<ul> <li>model_name: Defines the pre-trained model used for classification. Choices vary based on the application, from generic models like Google's ViT to specialized ones for food or NSFW detection.</li> <li>model_class &amp; processor_class: Specifies the model and processor classes for handling image data.</li> <li>device_map &amp; use_cuda: Configures GPU usage for enhanced performance.</li> <li>endpoint, port, username, &amp; password: Details for accessing the API securely.</li> </ul>"},{"location":"blog/huggingface/imgclass/#interacting-with-the-image-classification-api","title":"Interacting with the Image Classification API","text":""},{"location":"blog/huggingface/imgclass/#example-with-curl","title":"Example with <code>curl</code>:","text":"<pre><code>(base64 -w 0 your_image.jpg | awk '{print \"{\\\"image_base64\\\": \\\"\"$0\"\\\"}\"}' &gt; /tmp/image_payload.json)\ncurl -X POST http://localhost:3000/api/v1/classify_image \\\n-H \"Content-Type: application/json\" \\\n-u user:password \\\n-d @/tmp/image_payload.json | jq\n</code></pre>"},{"location":"blog/huggingface/imgclass/#example-with-python-requests","title":"Example with <code>python-requests</code>:","text":"<pre><code>import requests\nimport base64\nwith open(\"your_image.jpg\", \"rb\") as image_file:\nimage_base64 = base64.b64encode(image_file.read()).decode('utf-8')\ndata = {\"image_base64\": image_base64}\nresponse = requests.post(\"http://localhost:3000/api/v1/classify_image\",\njson=data,\nauth=('user', 'password'))\nprint(response.json())\n</code></pre>"},{"location":"blog/huggingface/imgclass/#use-cases-variations","title":"Use Cases &amp; Variations","text":""},{"location":"blog/huggingface/imgclass/#different-image-classification-models","title":"Different Image Classification Models","text":"<p>Tailor your API for a variety of classification tasks by selecting appropriate models:</p> <ul> <li>Aesthetic Assessment: Use models like <code>cafeai/cafe_aesthetic</code> to classify images based on aesthetic qualities.</li> <li>Gender Classification: Apply models such as <code>rizvandwiki/gender-classification</code> for gender recognition.</li> <li>Food Recognition: Employ food-specific models like <code>nateraw/food</code> to categorize food items.</li> <li>Object Detection: Utilize models like <code>microsoft/ResNet-50</code> for broad object classification.</li> <li>NSFW Detection: Choose models designed for NSFW content detection, ensuring user-generated content is appropriate.</li> </ul>"},{"location":"blog/huggingface/imgclass/#customizing-classification-parameters","title":"Customizing Classification Parameters","text":"<p>For advanced needs, include additional parameters in your request to customize the classification, such as the confidence threshold or specific labels to focus on.</p>"},{"location":"blog/huggingface/lm/","title":"Host Language Models Using Geniusrise","text":"<p>Deploying language models for various tasks is now seamless with Geniusrise. This guide will walk you through setting up inference APIs for different language model applications, from text generation to code completion. We'll dive into the <code>genius.yml</code> configuration, illustrating how to fine-tune parameters for specific use cases and interact with your API using <code>curl</code> and <code>python-requests</code>.</p>"},{"location":"blog/huggingface/lm/#getting-started","title":"Getting Started","text":"<p>First, ensure Geniusrise and its vision component are installed:</p> <pre><code>pip install geniusrise\npip install geniusrise-vision\n</code></pre>"},{"location":"blog/huggingface/lm/#configuration-file-geniusyml","title":"Configuration File: <code>genius.yml</code>","text":"<p>The <code>genius.yml</code> file is the heart of your API setup. Here's a breakdown of its key parameters:</p> <ul> <li>version: Defines the configuration format version.</li> <li>bolts: A collection of components, with each representing a specific API configuration.</li> <li>name: The identifier for your API.</li> <li>state: Manages model state, typically <code>type: none</code> for stateless operations.</li> <li>input and output: Define batch processing folders.</li> <li>method: Operation mode, usually <code>listen</code> for API services.</li> <li>args: Detailed model and server specifications.</li> </ul>"},{"location":"blog/huggingface/lm/#example-configuration-for-standard-language-models","title":"Example Configuration for Standard Language Models","text":"<pre><code>version: \"1\"\n\nbolts:\n  my_bolt:\n    name: LanguageModelAPI\n    state:\n      type: none\n    input:\n      type: batch\n      args:\n        input_folder: ./input\n    output:\n      type: batch\n      args:\n        output_folder: ./output\n    method: listen\n    args:\n      model_name: \"mistralai/Mistral-7B-Instruct-v0.1\"\n      model_class: AutoModelForMaskedLM\n      tokenizer_class: AutoTokenizer\n      use_cuda: true\n      precision: float\n      device_map: cuda:0\n      endpoint: \"0.0.0.0\"\n      port: 3000\n      cors_domain: \"http://localhost:3000\"\n      username: user\n      password: password\n</code></pre>"},{"location":"blog/huggingface/lm/#vllm-very-large-language-models-configuration-example","title":"VLLM (Very Large Language Models) Configuration Example","text":"<p>For handling VLLMs with Geniusrise, adjust the <code>args</code> to accommodate specific requirements, such as enabling eager loading or managing memory more efficiently:</p> <pre><code>version: \"1\"\n\nbolts:\n  my_bolt:\n    name: VLLMAPI\n    state:\n      type: none\n    input:\n      type: batch\n      args:\n        input_folder: ./input\n    output:\n      type: batch\n      args:\n        output_folder: ./output\n    method: listen\n    args:\n      model_name: \"mistralai/Mistral-7B-Instruct-v0.1\"\n      model_class: AutoModelForCausalLM\n      tokenizer_class: AutoTokenizer\n      use_cuda: true\n      precision: float16\n      quantization: 0\n      device_map: auto\n      max_memory: None\n      torchscript: False\n      use_vllm: true\n      vllm_enforce_eager: True\n      vllm_max_model_len: 1024\n      endpoint: \"*\"\n      port: 3000\n      cors_domain: \"http://localhost:3000\"\n      username: user\n      password: password\n</code></pre>"},{"location":"blog/huggingface/lm/#launching-your-api","title":"Launching Your API","text":"<p>Execute the following in your terminal:</p> <pre><code>genius rise\n</code></pre>"},{"location":"blog/huggingface/lm/#interacting-with-your-api","title":"Interacting with Your API","text":""},{"location":"blog/huggingface/lm/#using-curl-for-http-requests","title":"Using <code>curl</code> for HTTP Requests","text":"<p>Example for a Text Generation API:</p> <pre><code>curl -X POST http://localhost:3000/api/v1/complete \\\n-H \"Content-Type: application/json\" \\\n-d '{\"prompt\": \"Here is your prompt.\", \"max_new_tokens\": 1024, \"do_sample\": true}'\n</code></pre> <p>For VLLM Use Case:</p> <pre><code>curl -X POST http://localhost:3000/api/v1/complete \\\n-H \"Content-Type: application/json\" \\\n-d '{\"prompt\": \"Your VLLM prompt.\", \"max_new_tokens\": 1024, \"do_sample\": true}'\n</code></pre>"},{"location":"blog/huggingface/lm/#python-requests-example","title":"Python <code>requests</code> Example","text":"<p>Standard Language Model:</p> <pre><code>import requests\nresponse = requests.post(\"http://localhost:3000/api/v1/complete\",\njson={\"prompt\": \"Here is your prompt.\", \"max_new_tokens\": 1024, \"do_sample\": true},\nauth=('user', 'password'))\nprint(response.json())\n</code></pre> <p>VLLM Request:</p> <pre><code>import requests\nresponse = requests.post(\"http://localhost:3000/api/v1/complete\",\njson={\"prompt\": \"Your VLLM prompt.\", \"max_new_tokens\": 1024, \"do_sample\": true},\nauth=('user', 'password'))\nprint(response.json())\n</code></pre>"},{"location":"blog/huggingface/lm/#usecases-variations","title":"Usecases &amp; Variations","text":"<p>Geniusrise caters to a</p> <p>wide array of language model applications, from text summarization with models like <code>facebook/bart-large-cnn</code> to code generation using <code>WizardLM/WizardCoder-Python-7B-V1.0</code>. By customizing the <code>model_name</code>, <code>model_class</code>, and related parameters in your <code>genius.yml</code>, you can tailor your API for specific tasks:</p> <ul> <li>Text Summarization: Use summarization models to condense articles or documents.</li> <li>Text Generation: Create stories, generate content, or even simulate dialogue.</li> <li>Code Generation: Assist developers by completing code snippets or generating code from descriptions.</li> </ul> <p>Remember, while Geniusrise is a powerful tool for deploying language models, it's important to understand the capabilities and limitations of the models you choose to deploy. Always test your configurations and APIs thoroughly to ensure they meet your application's needs.</p>"},{"location":"blog/huggingface/ner/","title":"Host NER Models Using Geniusrise","text":"<p>Named Entity Recognition (NER) is a crucial task in natural language processing (NLP), enabling the identification of predefined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc. Geniusrise offers a streamlined approach to deploying NER models as APIs, facilitating the integration of sophisticated NER capabilities into applications. This guide explores setting up NER APIs using Geniusrise, covering various use cases and configurations.</p>"},{"location":"blog/huggingface/ner/#quick-setup","title":"Quick Setup","text":"<p>Installation:</p> <p>Ensure Geniusrise and its vision package are installed:</p> <pre><code>pip install geniusrise\npip install geniusrise-vision\n</code></pre> <p>Configuration File (<code>genius.yml</code>):</p> <p>Craft a <code>genius.yml</code> for your NER API. Here's an example:</p> <pre><code>version: \"1\"\nbolts:\nmy_bolt:\nname: NamedEntityRecognitionAPI\nstate:\ntype: none\ninput:\ntype: batch\nargs:\ninput_folder: ./input\noutput:\ntype: batch\nargs:\noutput_folder: ./output\nmethod: listen\nargs:\nmodel_name: \"d4data/biomedical-ner-all\"\nmodel_class: \"AutoModelForTokenClassification\"\ntokenizer_class: \"AutoTokenizer\"\nuse_cuda: True\nprecision: \"float\"\ndevice_map: \"cuda:0\"\nendpoint: \"*\"\nport: 3000\ncors_domain: \"http://localhost:3000\"\nusername: \"user\"\npassword: \"password\"\n</code></pre> <p>This setup configures an API for a biomedical NER model.</p>"},{"location":"blog/huggingface/ner/#interacting-with-your-api","title":"Interacting with Your API","text":"<p>Extract named entities by making a POST request:</p> <pre><code>curl -X POST localhost:3000/api/v1/recognize_entities \\\n-H \"Content-Type: application/json\" \\\n-u user:password \\\n-d '{\"text\": \"Input text here.\"}' | jq\n</code></pre>"},{"location":"blog/huggingface/ner/#use-cases-variations","title":"Use Cases &amp; Variations","text":""},{"location":"blog/huggingface/ner/#biomedical-ner","title":"Biomedical NER","text":"<p>Deploy models like <code>d4data/biomedical-ner-all</code> for applications requiring identification of biomedical entities. This is useful for extracting specific terms from medical literature or patient records.</p>"},{"location":"blog/huggingface/ner/#multilingual-ner","title":"Multilingual NER","text":"<p>For global applications, choose models supporting multiple languages, such as <code>Babelscape/wikineural-multilingual-ner</code>. This enables entity recognition across different languages, broadening your application's user base.</p>"},{"location":"blog/huggingface/ner/#domain-specific-ner","title":"Domain-Specific NER","text":"<p>Models like <code>pruas/BENT-PubMedBERT-NER-Gene</code> are tailored for specific domains (e.g., genetics). Using domain-specific models can significantly improve accuracy for targeted applications.</p>"},{"location":"blog/huggingface/ner/#configuration-tips","title":"Configuration Tips","text":"<ul> <li>Model Selection: Evaluate different models to find the best match for your application's needs, considering factors like language, domain, and performance.</li> <li>Precision and Performance: Adjust <code>precision</code> and <code>use_cuda</code> settings based on your computational resources and response time requirements.</li> <li>Security: Implement basic authentication using <code>username</code> and <code>password</code> to protect your API.</li> </ul>"},{"location":"blog/huggingface/nli/","title":"Host NLI Models Using Geniusrise","text":"<ul> <li>Host NLI Models Using Geniusrise</li> <li>Setup and Configuration</li> <li>Understanding Configuration Parameters</li> <li>Use Cases \\&amp; API Interaction<ul> <li>1. Entailment Checking</li> <li>2. Classification</li> <li>3. Textual Similarity</li> <li>4. Fact Checking</li> <li>Customizing for Different NLI Models</li> </ul> </li> <li>Fun<ul> <li>Intent Tree Search</li> <li>Real-Time Debate Judging</li> <li>Automated Story Plot Analysis</li> <li>Customer Feedback Interpretation</li> </ul> </li> <li>Play Around</li> </ul> <p>Natural Language Inference (NLI) is like a game where you have to figure out if one sentence can logically follow from another or not. Imagine you hear someone say, \"The dog is sleeping in the sun.\" Then, someone asks if it's true that \"The dog is outside.\" In this game, you'd say \"yes\" because if the dog is sleeping in the sun, it must be outside. Sometimes, the sentences don't match up, like if someone asks if the dog is swimming. You'd say \"no\" because sleeping in the sun doesn't mean swimming. And sometimes, you can't tell, like if someone asks if the dog is dreaming. Since you don't know, you'd say \"maybe.\" NLI is all about playing this matching game with sentences to help computers understand and use language like we do.</p> <p>This post will explore setting up APIs for various NLI tasks using Geniusrise, including entailment, classification, textual similarity, and fact-checking. We\u2019ll dive into the configuration details, provide interaction examples, and discuss how to tailor the setup for specific use cases.</p>"},{"location":"blog/huggingface/nli/#setup-and-configuration","title":"Setup and Configuration","text":"<p>Installation:</p> <p>Start by installing Geniusrise and the necessary text processing extensions:</p> <pre><code>pip install geniusrise\npip install geniusrise-text\n</code></pre> <p>Configuration (<code>genius.yml</code>):</p> <p>To deploy an NLI model, create a <code>genius.yml</code> configuration file:</p> <pre><code>version: \"1\"\n\nbolts:\n    my_bolt:\n        name: NLIAPI\n        state:\n            type: none\n        input:\n            type: batch\n            args:\n                input_folder: ./input\n        output:\n            type: batch\n            args:\n                output_folder: ./output\n        method: listen\n        args:\n            model_name: NDugar/ZSD-microsoft-v2xxlmnli\n            model_class: AutoModelForSequenceClassification\n            tokenizer_class: AutoTokenizer\n            use_cuda: true\n            precision: float\n            device_map: cuda:0\n            endpoint: \"0.0.0.0\"\n            port: 3000\n            cors_domain: http://localhost:3000\n            username: user\n            password: password\n</code></pre> <p>Launch your API with the command:</p> <pre><code>genius rise\n</code></pre>"},{"location":"blog/huggingface/nli/#understanding-configuration-parameters","title":"Understanding Configuration Parameters","text":"<ul> <li>model_name: Identifies the pre-trained model from Hugging Face to be used.</li> <li>use_cuda: A boolean indicating whether to use GPU acceleration.</li> <li>precision: Determines the computational precision, affecting performance and resource usage.</li> <li>device_map: Specifies GPU allocation for model processing.</li> <li>endpoint &amp; port: Network address and port for API access.</li> <li>username &amp; password: Basic authentication credentials for API security.</li> </ul>"},{"location":"blog/huggingface/nli/#use-cases-api-interaction","title":"Use Cases &amp; API Interaction","text":""},{"location":"blog/huggingface/nli/#1-entailment-checking","title":"1. Entailment Checking","text":"<p>Objective: Assess whether a hypothesis is supported (entailment), contradicted (contradiction), or neither (neutral) by a premise.</p> <p>Using <code>curl</code>:</p> <pre><code>/usr/bin/curl -X POST localhost:3000/api/v1/entailment \\\n-H \"Content-Type: application/json\" \\\n-u \"user:password\" \\\n-d '{\n        \"premise\": \"This a very good entry level smartphone, battery last 2-3 days after fully charged when connected to the internet. No memory lag issue when playing simple hidden object games. Performance is beyond my expectation, i bought it with a good bargain, couldnt ask for more!\",\n        \"hypothesis\": \"the phone has an awesome battery life\"\n    }' | jq\n</code></pre> <p>Using <code>python-requests</code>:</p> <pre><code>import requests\ndata = {\n\"premise\": \"This a very good entry level smartphone, battery last 2-3 days after fully charged when connected to the internet. No memory lag issue when playing simple hidden object games. Performance is beyond my expectation, i bought it with a good bargain, couldnt ask for more!\",\n\"hypothesis\": \"the phone has an awesome battery life\"\n}\nresponse = requests.post(\"http://localhost:3000/api/v1/entailment\",\njson=data,\nauth=('user', 'password'))\nprint(response.json())\n</code></pre>"},{"location":"blog/huggingface/nli/#2-classification","title":"2. Classification","text":"<p>Objective: Classify a piece of text into predefined categories.</p> <p>Using <code>curl</code>:</p> <pre><code>curl -X POST http://localhost:3000/api/v1/classify \\\n-H \"Content-Type: application/json\" \\\n-u \"user:password\" \\\n-d '{\"text\": \"I love playing soccer.\", \"candidate_labels\": [\"sport\", \"cooking\", \"travel\"]}'\n</code></pre> <p>Using <code>python-requests</code>:</p> <pre><code>import requests\ndata = {\n\"text\": \"I love playing soccer.\",\n\"candidate_labels\": [\"sport\", \"cooking\", \"travel\"]\n}\nresponse = requests.post(\"http://localhost:3000/api/v1/classify\",\njson=data,\nauth=('user', 'password'))\nprint(response.json())\n</code></pre>"},{"location":"blog/huggingface/nli/#3-textual-similarity","title":"3. Textual Similarity","text":"<p>Objective: Determine the similarity score between two texts.</p> <p>Using <code>curl</code>:</p> <pre><code>curl -X POST http://localhost:3000/api/v1/textual_similarity \\\n-H \"Content-Type: application/json\" \\\n-u \"user:password\" \\\n-d '{\"text1\": \"I enjoy swimming.\", \"text2\": \"Swimming is my hobby.\"}'\n</code></pre> <p>Using <code>python-requests</code>:</p> <pre><code>import requests\ndata = {\n\"text1\": \"I enjoy swimming.\",\n\"text2\": \"Swimming is my hobby.\"\n}\nresponse = requests.post(\"http://localhost:3000/api/v1/textual_similarity\",\njson=data,\nauth=('user', 'password'))\nprint(response.json())\n</code></pre>"},{"location":"blog/huggingface/nli/#4-fact-checking","title":"4. Fact Checking","text":"<p>Objective: Verify the accuracy of a statement based on provided context or reference material.</p> <p>Using <code>curl</code>:</p> <pre><code>curl -X POST http://localhost:3000/api/v1/fact_checking \\\n-H \"Content-Type: application/json\" \\\n-u \"user:password\" \\\n-d '{\"context\": \"The Eiffel Tower is located in Paris.\", \"statement\": \"The Eiffel Tower is in France.\"}'\n</code></pre> <p>Using <code>python-requests</code>:</p> <pre><code>import requests\ndata = {\n\"context\": \"The Eiffel Tower is located in Paris.\",\n\"statement\": \"The Eiffel Tower is in France.\"\n}\nresponse = requests.post(\"http://localhost:3000/api/v1/fact_checking\",\njson=data,\nauth=('user', 'password'))\nprint(response.json())\n</code></pre> <p>Each of these endpoints serves a specific NLI-related purpose, from evaluating logical relationships between texts to classifying and checking facts. By leveraging these APIs, developers can enhance their applications with deep, contextual understanding of natural language.</p>"},{"location":"blog/huggingface/nli/#customizing-for-different-nli-models","title":"Customizing for Different NLI Models","text":"<p>To deploy APIs for various NLI tasks, simply adjust the <code>model_name</code> in your <code>genius.yml</code>. For instance, to switch to a model optimized for textual similarity or fact-checking, replace <code>microsoft/deberta-v2-xlarge-mnli</code> with the appropriate model identifier.</p>"},{"location":"blog/huggingface/nli/#fun","title":"Fun","text":""},{"location":"blog/huggingface/nli/#intent-tree-search","title":"Intent Tree Search","text":"<p>NLI when used for zero-shot classification can be used in a large number of contexts. Consider a chat usecase where there is an entire tree of possible scenarios, and you want to identify which node in the tree you're in to feed that particular prompt to another chat model.</p> <p>Lets consider a 2-level tree such as this for an internal helpdesk:</p> <pre><code>intents = {\n\"IT Support\": [\n\"Computer or hardware issues\",\n\"Software installation and updates\",\n\"Network connectivity problems\",\n\"Access to digital tools and resources\",\n],\n\"HR Inquiries\": [\n\"Leave policy and requests\",\n\"Benefits and compensation queries\",\n\"Employee wellness programs\",\n\"Performance review process\",\n],\n\"Facilities Management\": [\n\"Workspace maintenance requests\",\n\"Meeting room bookings\",\n\"Parking and transportation services\",\n\"Health and safety concerns\",\n],\n\"Finance and Expense\": [\n\"Expense report submission\",\n\"Payroll inquiries\",\n\"Budget allocation questions\",\n\"Procurement process\",\n],\n\"Training and Development\": [\n\"Professional development opportunities\",\n\"Training program schedules\",\n\"Certification and learning resources\",\n\"Mentorship and coaching programs\",\n],\n\"Project Management\": [\n\"Project collaboration tools\",\n\"Deadline extensions and modifications\",\n\"Resource allocation\",\n\"Project status updates\",\n],\n\"Travel and Accommodation\": [\n\"Business travel arrangements\",\n\"Travel policy and reimbursements\",\n\"Accommodation bookings\",\n\"Visa and travel documentation\",\n],\n\"Legal and Compliance\": [\n\"Contract review requests\",\n\"Data privacy and security policies\",\n\"Compliance training and certifications\",\n\"Legal consultation and support\",\n],\n\"Communications and Collaboration\": [\n\"Internal communication platforms\",\n\"Collaboration tools and access\",\n\"Team meeting coordination\",\n\"Cross-departmental initiatives\",\n],\n\"Employee Feedback and Suggestions\": [\n\"Employee satisfaction surveys\",\n\"Feedback submission channels\",\n\"Suggestion box for improvements\",\n\"Employee engagement activities\",\n],\n\"Onboarding and Offboarding\": [\n\"New employee onboarding process\",\n\"Offboarding procedures\",\n\"Orientation schedules\",\n\"Transition support\",\n],\n\"Administrative Assistance\": [\n\"Document and record-keeping\",\n\"Scheduling and calendar management\",\n\"Courier and mailing services\",\n\"Administrative support requests\",\n],\n}\n</code></pre> <p>Lets deploy a large model so its more intelligent:</p> <pre><code>version: \"1\"\n\nbolts:\n    my_bolt:\n        name: NLIAPI\n        state:\n            type: none\n        input:\n            type: batch\n            args:\n                input_folder: ./input\n        output:\n            type: batch\n            args:\n                output_folder: ./output\n        method: listen\n        args:\n            model_name: facebook/bart-large-mnli\n            model_class: AutoModelForSequenceClassification\n            tokenizer_class: AutoTokenizer\n            use_cuda: true\n            precision: float\n            device_map: cuda:0\n            endpoint: \"0.0.0.0\"\n            port: 3000\n            cors_domain: http://localhost:3000\n            username: user\n            password: password\n</code></pre> <p>we can browse through this tree to zero in on the user's micro-intent to retrieve our prompt to feed into the model:</p> <pre><code>import requests\nprompt =  \"I need to travel to singapore next week \ud83d\ude03.\"\ndef find_most_probable_class(prompt, intents):\nresponse = requests.post(\"http://localhost:3000/api/v1/classify\",\njson={\"text\": prompt, \"candidate_labels\": intents},\nauth=('user', 'password'))\nlabel_scores = response.json()[\"label_scores\"]\nmax_score = max(label_scores.values())\nchosen_label = [ k for k,v in label_scores.items() if v == max_score ][0]\nreturn chosen_label\nlevel1 = find_most_probable_class(prompt, list(intents.keys()))\nlevel2 = find_most_probable_class(prompt, list(intents[level1]))\nprint(f\"The request is for department: {level1} and specifically for {level2}\")\n# The request is for department: Travel and Accommodation and specifically for Visa and travel documentation\n</code></pre>"},{"location":"blog/huggingface/nli/#real-time-debate-judging","title":"Real-Time Debate Judging","text":"<p>Imagine a scenario where an AI is used to judge a debate competition in real-time. Each participant's argument is evaluated for logical consistency, relevance, and how well it counters the opponent's previous points.</p> <pre><code>debate_points = [\n{\"speaker\": \"Alice\", \"statement\": \"Renewable energy can effectively replace fossil fuels.\"},\n{\"speaker\": \"Bob\", \"statement\": \"Renewable energy is not yet reliable enough to meet all our energy needs.\"},\n]\nfor i in range(1, len(debate_points)):\npremise = debate_points[i-1][\"statement\"]\nhypothesis = debate_points[i][\"statement\"]\nresponse = requests.post(\"http://localhost:3000/api/v1/entailment\",\njson={\"premise\": premise, \"hypothesis\": hypothesis},\nauth=('user', 'password'))\nlabel_scores = response.json()[\"label_scores\"]\nmax_score = max(label_scores.values())\nchosen_label = [ k for k,v in label_scores.items() if v == max_score ][0]\nprint(f\"Debate point by {debate_points[i]['speaker']}: {hypothesis}\")\nprint(f\"Judgement: {chosen_label}\")\n# Debate point by Bob: Renewable energy is not yet reliable enough to meet all our energy needs.\n# Judgement: neutral\n</code></pre>"},{"location":"blog/huggingface/nli/#automated-story-plot-analysis","title":"Automated Story Plot Analysis","text":"<p>A model can be used to analyze a story plot to determine if the events and characters' decisions are logically consistent and plausible within the story's universe.</p> <pre><code>story_events = [\n\"The hero discovers a secret door in their house leading to a magical world.\",\n\"Despite being in a magical world, the hero uses their smartphone to call for help.\",\n\"The hero defeats the villain using a magical sword found in the new world.\",\n]\nfor i in range(1, len(story_events)):\npremise = story_events[i-1]\nhypothesis = story_events[i]\nresponse = requests.post(\"http://localhost:3000/api/v1/entailment\",\njson={\"premise\": premise, \"hypothesis\": hypothesis},\nauth=('user', 'password'))\nlabel_scores = response.json()[\"label_scores\"]\nif \"neutral\" in label_scores:\ndel label_scores[\"neutral\"]\nmax_score = max(label_scores.values())\nchosen_label = [ k for k,v in label_scores.items() if v == max_score ][0]\nprint(f\"Story event - {chosen_label}: {hypothesis}\")\n# Story event - contradiction: Despite being in a magical world, the hero uses their smartphone to call for help.\n# Story event - contradiction: The hero defeats the villain using a magical sword found in the new world.\n</code></pre>"},{"location":"blog/huggingface/nli/#customer-feedback-interpretation","title":"Customer Feedback Interpretation","text":"<p>This application involves analyzing customer feedback to categorize it into compliments, complaints, or suggestions, providing valuable insights into customer satisfaction and areas for improvement.</p> <pre><code>feedbacks = [\n\"The new update makes the app much easier to use. Great job!\",\n\"I've been facing frequent crashes after the last update.\",\n\"It would be great if you could add a dark mode feature.\",\n\"Otherwise you leave me no choice but to slowly torture your soul.\"\n]\ncategories = [\"compliment\", \"complaint\", \"suggestion\", \"murderous intent\"]\nfor feedback in feedbacks:\nresponse = requests.post(\"http://localhost:3000/api/v1/classify\",\njson={\"text\": feedback, \"candidate_labels\": categories},\nauth=('user', 'password'))\nlabel_scores = response.json()[\"label_scores\"]\nmax_score = max(label_scores.values())\nchosen_label = [ k for k,v in label_scores.items() if v == max_score ][0]\nprint(f\"Feedback - {chosen_label}: {feedback}\")\n# Feedback - suggestion: The new update makes the app much easier to use. Great job!\n# Feedback - complaint: I've been facing frequent crashes after the last update.\n# Feedback - suggestion: It would be great if you could add a dark mode feature.\n# Feedback - murderous intent: Otherwise you leave me no choice but to slowly torture your soul.\n</code></pre>"},{"location":"blog/huggingface/nli/#play-around","title":"Play Around","text":"<p>There are 218 models under \"zero-shot-classification\" on the huggingface hub but a simple search for <code>nli</code> turns up 822 models so there are a lot of models that are not tagged properly. NLI is a very interesting and a core NLP task and a few good general models can be turned into a lot of fun!</p>"},{"location":"blog/huggingface/ocr/","title":"Host OCR Models Using Geniusrise","text":"<p>Optical Character Recognition (OCR) technology has revolutionized the way we process and digitize printed or handwritten documents, making it easier to edit, search, and store textual content in digital formats. Geniusrise facilitates the deployment of OCR models as APIs, enabling developers to integrate OCR capabilities into their applications seamlessly. This guide will demonstrate setting up OCR APIs using Geniusrise, covering the configuration, usage examples, and highlighting different use cases.</p>"},{"location":"blog/huggingface/ocr/#setup-and-configuration","title":"Setup and Configuration","text":"<p>Installation:</p> <p>First, install Geniusrise and its text extension:</p> <pre><code>pip install geniusrise\npip install geniusrise-vision\n</code></pre> <p>Configuration (<code>genius.yml</code>):</p> <p>Create a <code>genius.yml</code> file to define your OCR service:</p> <pre><code>version: \"1\"\n\nbolts:\n    my_bolt:\n        name: ImageOCRAPI\n        state:\n            type: none\n        input:\n            type: batch\n            args:\n                input_folder: ./input\n        output:\n            type: batch\n            args:\n                output_folder: ./output\n        method: listen\n        args:\n            model_name: facebook/bart-large-cnn\n            model_class: AutoModelForSeq2SeqLM\n            tokenizer_class: AutoTokenizer\n            use_cuda: true\n            precision: float\n            device_map: cuda:0\n            endpoint: \"0.0.0.0\"\n            port: 3000\n            cors_domain: http://localhost:3000\n            username: user\n            password: password\n</code></pre> <p>Activate your API with:</p> <pre><code>genius rise\n</code></pre>"},{"location":"blog/huggingface/ocr/#configuration-parameters-explained","title":"Configuration Parameters Explained","text":"<ul> <li>model_name: Specifies the pre-trained model. For OCR tasks, models like <code>paddleocr</code>, <code>facebook/nougat-base</code>, or <code>easyocr</code> are popular choices.</li> <li>use_cuda: Enables GPU acceleration.</li> <li>precision: Affects performance through computational precision.</li> <li>endpoint &amp; port: Network address and port for API access.</li> <li>username &amp; password: Security credentials for API usage.</li> </ul>"},{"location":"blog/huggingface/ocr/#using-paddleocr","title":"Using PaddleOCR","text":"<p>PaddleOCR offers state-of-the-art accuracy and supports multiple languages, making it a great choice for applications requiring high-performance OCR.</p>"},{"location":"blog/huggingface/ocr/#geniusyml-for-paddleocr","title":"<code>genius.yml</code> for PaddleOCR","text":"<pre><code>version: \"1\"\nbolts:\nmy_bolt:\nname: ImageOCRAPI\nstate:\ntype: none\ninput:\ntype: batch\nargs:\ninput_folder: ./input\noutput:\ntype: batch\nargs:\noutput_folder: ./output\nmethod: listen\nargs:\nmodel_name: \"paddleocr\"\ndevice_map: \"cuda:0\"\nendpoint: \"0.0.0.0\"\nport: 3000\ncors_domain: \"http://localhost:3000\"\nusername: \"user\"\npassword: \"password\"\n</code></pre> <p>This configuration sets up an OCR API using PaddleOCR. After setting up your <code>genius.yml</code>, activate your API by running:</p> <pre><code>genius rise\n</code></pre>"},{"location":"blog/huggingface/ocr/#using-easyocr","title":"Using EasyOCR","text":"<p>EasyOCR is a practical tool that supports more than 80 languages and doesn't require machine learning expertise to implement.</p>"},{"location":"blog/huggingface/ocr/#geniusyml-for-easyocr","title":"<code>genius.yml</code> for EasyOCR","text":"<pre><code>version: \"1\"\nbolts:\nmy_bolt:\nname: ImageOCRAPI\nstate:\ntype: none\ninput:\ntype: batch\nargs:\ninput_folder: ./input\noutput:\ntype: batch\nargs:\noutput_folder: ./output\nmethod: listen\nargs:\nmodel_name: \"easyocr\"\ndevice_map: \"cuda:0\"\nendpoint: \"0.0.0.0\"\nport: 3000\ncors_domain: \"http://localhost:3000\"\nusername: \"user\"\npassword: \"password\"\n</code></pre> <p>This YAML file configures an OCR API utilizing EasyOCR. Like with PaddleOCR, you'll need to execute <code>genius rise</code> to get the API running.</p>"},{"location":"blog/huggingface/ocr/#general-api-interaction-examples","title":"General API Interaction Examples","text":"<p>Interacting with these OCR APIs can be done through HTTP requests, where you send a base64-encoded image and receive the detected text in response. Here's a generic example on how to send a request to either OCR API configured above:</p>"},{"location":"blog/huggingface/ocr/#example-with-curl","title":"Example with <code>curl</code>:","text":"<pre><code>(base64 -w 0 path_to_your_image.jpg | awk '{print \"{\\\"image_base64\\\": \\\"\"$0\"\\\"}\"}' &gt; /tmp/image_payload.json)\ncurl -X POST http://localhost:3000/api/v1/ocr \\\n-H \"Content-Type: application/json\" \\\n-u user:password \\\n-d @/tmp/image_payload.json | jq\n</code></pre>"},{"location":"blog/huggingface/ocr/#example-with-python-requests","title":"Example with <code>python-requests</code>:","text":"<pre><code>import requests\nimport base64\nwith open(\"path_to_your_image.jpg\", \"rb\") as image_file:\nimage_base64 = base64.b64encode(image_file.read()).decode('utf-8')\ndata = {\"image_base64\": image_base64}\nresponse = requests.post(\"http://localhost:3000/api/v1/ocr\",\njson=data,\nauth=('user', 'password'))\nprint(response.json())\n</code></pre>"},{"location":"blog/huggingface/ocr/#interacting-with-the-ocr-api","title":"Interacting with the OCR API","text":"<p>OCR tasks involve converting images of text into editable and searchable data. Here's how to interact with the OCR API using <code>curl</code> and <code>python-requests</code>:</p>"},{"location":"blog/huggingface/ocr/#example-with-curl_1","title":"Example with <code>curl</code>:","text":"<pre><code>(base64 -w 0 your_image.jpg | awk '{print \"{\\\"image_base64\\\": \\\"\"$0\"\\\"}\"}' &gt; /tmp/image_payload.json)\ncurl -X POST http://localhost:3000/api/v1/ocr \\\n-H \"Content-Type: application/json\" \\\n-u user:password \\\n-d @/tmp/image_payload.json | jq\n</code></pre>"},{"location":"blog/huggingface/ocr/#example-with-python-requests_1","title":"Example with <code>python-requests</code>:","text":"<pre><code>import requests\nimport base64\nimage_path = 'your_image.jpg'\nwith open(image_path, 'rb') as image_file:\nimage_base64 = base64.b64encode(image_file.read()).decode('utf-8')\ndata = {\n\"image_base64\": image_base64\n}\nresponse = requests.post(\"http://localhost:3000/api/v1/ocr\",\njson=data,\nauth=('user', 'password'))\nprint(response.json())\n</code></pre>"},{"location":"blog/huggingface/ocr/#use-cases-variations","title":"Use Cases &amp; Variations","text":""},{"location":"blog/huggingface/ocr/#different-ocr-models","title":"Different OCR Models","text":"<p>To adapt the API for various OCR tasks, such as document digitization, receipt scanning, or handwritten note conversion, you can switch the <code>model_name</code> in your <code>genius.yml</code>:</p> <ul> <li>Document OCR: Use models like <code>paddleocr</code> for general document recognition.</li> <li>Handwritten OCR: Opt for models specifically fine-tuned for handwriting, such as <code>facebook/nougat-base</code>.</li> <li>Receipt OCR: Utilize domain-specific models designed for extracting information from receipts or invoices.</li> </ul>"},{"location":"blog/huggingface/ocr/#customizing-ocr-parameters","title":"Customizing OCR Parameters","text":"<p>For advanced OCR needs, additional parameters can be included in your request to customize the OCR process, such as specifying the language, adjusting the resolution, or defining the output format.</p>"},{"location":"blog/huggingface/qa/","title":"Host Question Answering Models Using Geniusrise","text":"<ul> <li>Host Question Answering Models Using Geniusrise</li> <li>Types of Question Answering Tasks<ul> <li>Generative</li> <li>Extractive</li> <li>Why Extractive May be Better</li> </ul> </li> <li>Installation and Configuration</li> <li>Understanding <code>genius.yml</code></li> <li>Use Cases \\&amp; Variations</li> <li>Making API Requests<ul> <li>Direct Question Answering API</li> <li>Hugging Face Pipeline API</li> </ul> </li> <li>Fun<ul> <li>Long contexts</li> <li>Domain-specific</li> </ul> </li> <li>Play around</li> </ul> <p>Deploying question answering (QA) models can significantly enhance the capabilities of applications, providing users with specific, concise answers to their queries. Geniusrise simplifies this process, enabling developers to rapidly set up and deploy QA APIs. This guide will walk you through the steps to create inference APIs for different QA tasks using Geniusrise, focusing on configuring the <code>genius.yml</code> file and providing interaction examples via <code>curl</code> and <code>python-requests</code>.</p>"},{"location":"blog/huggingface/qa/#types-of-question-answering-tasks","title":"Types of Question Answering Tasks","text":"<p>Before diving into the setup and deployment of question answering (QA) models using Geniusrise, it's essential to understand the two main types of QA tasks: generative and extractive. This distinction is crucial for selecting the right model for your application and configuring your <code>genius.yml</code> file accordingly.</p>"},{"location":"blog/huggingface/qa/#generative","title":"Generative","text":"<p>Generative QA models are designed to produce answers by generating text based on the context and the question asked. These models do not restrict their responses to the text's snippets but rather \"generate\" a new text passage that answers the question. Generative models are powerful for open-ended questions where the answer may not be directly present in the context or requires synthesis of information from multiple parts of the context.</p>"},{"location":"blog/huggingface/qa/#extractive","title":"Extractive","text":"<p>Extractive QA models, on the other hand, identify and extract a specific snippet from the provided text that answers the question. This approach is particularly effective for factual questions where the answer is explicitly stated in the text. Extractive QA is advantageous because it limits the model's responses to the actual content of the input text, reducing the chances of hallucination (producing incorrect or unfounded information) that can occur with generative models.</p>"},{"location":"blog/huggingface/qa/#why-extractive-may-be-better","title":"Why Extractive May be Better","text":"<ul> <li>Accuracy: Extractive QA models provide answers directly sourced from the input text, ensuring that the information is accurate and grounded in the provided context.</li> <li>Reliability: By constraining the answers to the text snippets, extractive QA minimizes the risk of hallucinations, making it a reliable choice for applications where factual correctness is paramount.</li> <li>Efficiency for RAG: Extractive QA tasks can be particularly efficient for Retrieval-Augmented Generation (RAG) because they allow for precise information retrieval without the need for generating new text, which can be computationally more demanding.</li> </ul> <p>The models discussed in this guide focus on extractive QA tasks, which are particularly well-suited for direct, fact-based question answering from provided texts.</p> <p>Extractive QA models are ideal for applications requiring high precision and direct answers from given texts.</p>"},{"location":"blog/huggingface/qa/#installation-and-configuration","title":"Installation and Configuration","text":"<p>Requirements</p> <ul> <li>You need to have a GPU. Most of the system works with NVIDIA GPUs.</li> <li>Install CUDA.</li> </ul> <p>Optional: Set up a virtual environment:</p> <pre><code>virtualenv venv\nsource venv/bin/activate\n</code></pre> <p>Step 1: Install Geniusrise</p> <pre><code>pip install torch\npip install geniusrise\npip install geniusrise-text\n</code></pre> <p>Step 2: Create Your Configuration File (<code>genius.yml</code>)</p> <pre><code>version: \"1\"\n\nbolts:\n    my_bolt:\n        name: QAAPI\n        state:\n            type: none\n        input:\n            type: batch\n            args:\n                input_folder: ./input\n        output:\n            type: batch\n            args:\n                output_folder: ./output\n        method: listen\n        args:\n            model_name: deepset/deberta-v3-base-squad2\n            model_class: AutoModelForQuestionAnswering\n            tokenizer_class: AutoTokenizer\n            use_cuda: true\n            precision: float\n            device_map: cuda:0\n            endpoint: \"0.0.0.0\"\n            port: 3000\n            cors_domain: http://localhost:3000\n            username: user\n            password: password\n</code></pre> <p>After setting up your <code>genius.yml</code>, launch your API with:</p> <pre><code>genius rise\n</code></pre>"},{"location":"blog/huggingface/qa/#understanding-geniusyml","title":"Understanding <code>genius.yml</code>","text":"<p>Each parameter in the <code>genius.yml</code> file is crucial for customizing your QA API:</p> <ul> <li>model_name: The model identifier from Hugging Face, tailored to your specific QA task.</li> <li>use_cuda: Toggle GPU acceleration (<code>true</code> or <code>false</code>). Using GPUs can drastically reduce inference time.</li> <li>precision: Model precision (<code>float</code> for single precision). Adjusting this can affect performance and accuracy, e.g. to <code>bfloat16</code>.</li> <li>device_map: Assigns model parts to specific GPUs, useful for systems with multiple GPUs. <code>cuda:0</code> implies use GPU 0.</li> <li>endpoint &amp; port: Defines where your API is hosted, allowing for easy access.</li> <li>username &amp; password: Secure your API with basic authentication.</li> </ul>"},{"location":"blog/huggingface/qa/#use-cases-variations","title":"Use Cases &amp; Variations","text":"<p>Replacing Model for Different QA Tasks</p> <p>To adapt the API for various QA tasks, simply change the <code>model_name</code> in your <code>genius.yml</code>. For example, to switch to a model specializing in medical QA, you might use <code>bert-large-uncased-whole-word-masking-finetuned-squad</code> for broader coverage of medical inquiries.</p> <p>Example <code>genius.yml</code> for a Different Use Case:</p> <pre><code>args:\n  model_name: \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n</code></pre>"},{"location":"blog/huggingface/qa/#making-api-requests","title":"Making API Requests","text":"<p>Geniusrise enables two primary ways to interact with your Question Answering API: through direct question-answering and utilizing the Hugging Face pipeline. Below, we provide examples on how to use both endpoints using <code>curl</code> and <code>python-requests</code>.</p>"},{"location":"blog/huggingface/qa/#direct-question-answering-api","title":"Direct Question Answering API","text":"<p>This API endpoint directly answers questions based on the provided context.</p> <p>Using <code>curl</code>:</p> <pre><code>/usr/bin/curl -X POST localhost:3000/api/v1/answer \\\n-H \"Content-Type: application/json\" \\\n-u \"user:password\" \\\n-d '{\n        \"data\": \"Theres something magical about Recurrent Neural Networks (RNNs). I still remember when I trained my first recurrent network for Image Captioning. Within a few dozen minutes of training my first baby model (with rather arbitrarily-chosen hyperparameters) started to generate very nice looking descriptions of images that were on the edge of making sense. Sometimes the ratio of how simple your model is to the quality of the results you get out of it blows past your expectations, and this was one of those times. What made this result so shocking at the time was that the common wisdom was that RNNs were supposed to be difficult to train (with more experience Ive in fact reached the opposite conclusion). Fast forward about a year: Im training RNNs all the time and Ive witnessed their power and robustness many times, and yet their magical outputs still find ways of amusing me.\",\n        \"question\": \"What is the common wisdom about RNNs?\"\n    }' | jq\n</code></pre> <p>Using <code>python-requests</code>:</p> <pre><code>import requests\ndata = {\n\"data\": \"Theres something magical about Recurrent Neural Networks (RNNs). I still remember when I trained my first recurrent network for Image Captioning. Within a few dozen minutes of training my first baby model (with rather arbitrarily-chosen hyperparameters) started to generate very nice looking descriptions of images that were on the edge of making sense. Sometimes the ratio of how simple your model is to the quality of the results you get out of it blows past your expectations, and this was one of those times. What made this result so shocking at the time was that the common wisdom was that RNNs were supposed to be difficult to train (with more experience Ive in fact reached the opposite conclusion). Fast forward about a year: Im training RNNs all the time and Ive witnessed their power and robustness many times, and yet their magical outputs still find ways of amusing me.\",\n\"question\": \"What is the common wisdom about RNNs?\"\n}\nresponse = requests.post(\"http://localhost:3000/api/v1/answer\",\njson=data,\nauth=('user', 'password'))\nprint(response.json())\n</code></pre>"},{"location":"blog/huggingface/qa/#hugging-face-pipeline-api","title":"Hugging Face Pipeline API","text":"<p>This API endpoint leverages the Hugging Face pipeline for answering questions, offering a streamlined way to use pre-trained models for question answering.</p> <p>Using <code>curl</code>:</p> <pre><code>curl -X POST http://localhost:3000/api/v1/answer_pipeline \\\n-H \"Content-Type: application/json\" \\\n-u \"user:password\" \\\n-d '{\"question\": \"Who created Geniusrise?\", \"data\": \"Geniusrise was created by a team of dedicated developers.\"}'\n</code></pre> <p>Using <code>python-requests</code>:</p> <pre><code>import requests\ndata = {\n\"question\": \"Who created Geniusrise?\",\n\"data\": \"Geniusrise was created by a team of dedicated developers.\"\n}\nresponse = requests.post(\"http://localhost:3000/api/v1/answer_pipeline\",\njson=data,\nauth=('user', 'password'))\nprint(response.json())\n</code></pre>"},{"location":"blog/huggingface/qa/#fun","title":"Fun","text":""},{"location":"blog/huggingface/qa/#long-contexts","title":"Long contexts","text":"<p>An usual problem that faces QA models is small context sizes. This limits the model's capabilities for processing large documents or large amounts of text in their inputs. Though language models keep getting bigger contexts, QA models on the other hand tend to be much smaller and support smaller contexts.</p> <p>However there are exceptions like this one:</p> <pre><code>version: \"1\"\n\nbolts:\n    my_bolt:\n        name: QAAPI\n        state:\n            type: none\n        input:\n            type: batch\n            args:\n                input_folder: ./input\n        output:\n            type: batch\n            args:\n                output_folder: ./output\n        method: listen\n        args:\n            model_name: valhalla/longformer-base-4096-finetuned-squadv1\n            model_class: AutoModelForQuestionAnswering\n            tokenizer_class: AutoTokenizer\n            use_cuda: true\n            precision: float\n            device_map: cuda:0\n            endpoint: \"0.0.0.0\"\n            port: 3000\n            cors_domain: http://localhost:3000\n            username: user\n            password: password\n</code></pre> <pre><code>/usr/bin/curl -X POST localhost:3000/api/v1/answer \\\n-H \"Content-Type: application/json\" \\\n-u \"user:password\" \\\n-d '{\n        \"data\": \"Theres something magical about Recurrent Neural Networks (RNNs). I still remember when I trained my first recurrent network for Image Captioning. Within a few dozen minutes of training my first baby model (with rather arbitrarily-chosen hyperparameters) started to generate very nice looking descriptions of images that were on the edge of making sense. Sometimes the ratio of how simple your model is to the quality of the results you get out of it blows past your expectations, and this was one of those times. What made this result so shocking at the time was that the common wisdom was that RNNs were supposed to be difficult to train (with more experience Ive in fact reached the opposite conclusion). Fast forward about a year: Im training RNNs all the time and Ive witnessed their power and robustness many times, and yet their magical outputs still find ways of amusing me. This post is about sharing some of that magic with you. By the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we\u2019re getting ahead of ourselves; What are RNNs anyway? Recurrent Neural Networks Sequences. Depending on your background you might be wondering: What makes Recurrent Networks so special? A glaring limitation of Vanilla Neural Networks (and also Convolutional Networks) is that their API is too constrained: they accept a fixed-sized vector as input (e.g. an image) and produce a fixed-sized vector as output (e.g. probabilities of different classes). Not only that: These models perform this mapping using a fixed amount of computational steps (e.g. the number of layers in the model). The core reason that recurrent nets are more exciting is that they allow us to operate over sequences of vectors: Sequences in the input, the output, or in the most general case both. A few examples may make this more concrete: Each rectangle is a vector and arrows represent functions (e.g. matrix multiply). Input vectors are in red, output vectors are in blue and green vectors hold the RNNs state (more on this soon). From left to right: (1) Vanilla mode of processing without RNN, from fixed-sized input to fixed-sized output (e.g. image classification). (2) Sequence output (e.g. image captioning takes an image and outputs a sentence of words). (3) Sequence input (e.g. sentiment analysis where a given sentence is classified as expressing positive or negative sentiment). (4) Sequence input and sequence output (e.g. Machine Translation: an RNN reads a sentence in English and then outputs a sentence in French). (5) Synced sequence input and output (e.g. video classification where we wish to label each frame of the video). Notice that in every case are no pre-specified constraints on the lengths sequences because the recurrent transformation (green) is fixed and can be applied as many times as we like. As you might expect, the sequence regime of operation is much more powerful compared to fixed networks that are doomed from the get-go by a fixed number of computational steps, and hence also much more appealing for those of us who aspire to build more intelligent systems. Moreover, as we\u2019ll see in a bit, RNNs combine the input vector with their state vector with a fixed (but learned) function to produce a new state vector. This can in programming terms be interpreted as running a fixed program with certain inputs and some internal variables. Viewed this way, RNNs essentially describe programs. In fact, it is known that RNNs are Turing-Complete in the sense that they can to simulate arbitrary programs (with proper weights). But similar to universal approximation theorems for neural nets you shouldn\u2019t read too much into this. In fact, forget I said anything.\",\n        \"question\": \"What do the models essentially do?\"\n    }' | jq\n\n# {\n#   \"data\": \"Theres something magical about Recurrent Neural Networks (RNNs). I still remember when I trained my first recurrent network for Image Captioning. Within a few dozen minutes of training my first baby model (with rather arbitrarily-chosen hyperparameters) started to generate very nice looking descriptions of images that were on the edge of making sense. Sometimes the ratio of how simple your model is to the quality of the results you get out of it blows past your expectations, and this was one of those times. What made this result so shocking at the time was that the common wisdom was that RNNs were supposed to be difficult to train (with more experience Ive in fact reached the opposite conclusion). Fast forward about a year: Im training RNNs all the time and Ive witnessed their power and robustness many times, and yet their magical outputs still find ways of amusing me. This post is about sharing some of that magic with you. By the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we\u2019re getting ahead of ourselves; What are RNNs anyway? Recurrent Neural Networks Sequences. Depending on your background you might be wondering: What makes Recurrent Networks so special? A glaring limitation of Vanilla Neural Networks (and also Convolutional Networks) is that their API is too constrained: they accept a fixed-sized vector as input (e.g. an image) and produce a fixed-sized vector as output (e.g. probabilities of different classes). Not only that: These models perform this mapping using a fixed amount of computational steps (e.g. the number of layers in the model). The core reason that recurrent nets are more exciting is that they allow us to operate over sequences of vectors: Sequences in the input, the output, or in the most general case both. A few examples may make this more concrete: Each rectangle is a vector and arrows represent functions (e.g. matrix multiply). Input vectors are in red, output vectors are in blue and green vectors hold the RNNs state (more on this soon). From left to right: (1) Vanilla mode of processing without RNN, from fixed-sized input to fixed-sized output (e.g. image classification). (2) Sequence output (e.g. image captioning takes an image and outputs a sentence of words). (3) Sequence input (e.g. sentiment analysis where a given sentence is classified as expressing positive or negative sentiment). (4) Sequence input and sequence output (e.g. Machine Translation: an RNN reads a sentence in English and then outputs a sentence in French). (5) Synced sequence input and output (e.g. video classification where we wish to label each frame of the video). Notice that in every case are no pre-specified constraints on the lengths sequences because the recurrent transformation (green) is fixed and can be applied as many times as we like. As you might expect, the sequence regime of operation is much more powerful compared to fixed networks that are doomed from the get-go by a fixed number of computational steps, and hence also much more appealing for those of us who aspire to build more intelligent systems. Moreover, as we\u2019ll see in a bit, RNNs combine the input vector with their state vector with a fixed (but learned) function to produce a new state vector. This can in programming terms be interpreted as running a fixed program with certain inputs and some internal variables. Viewed this way, RNNs essentially describe programs. In fact, it is known that RNNs are Turing-Complete in the sense that they can to simulate arbitrary programs (with proper weights). But similar to universal approximation theorems for neural nets you shouldn\u2019t read too much into this. In fact, forget I said anything.\",\n#   \"question\": \"What do the models essentially do?\",\n#   \"answer\": {\n#     \"answers\": [\n#       \"they allow us to operate over sequences of vectors\" &lt;---\n#     ],\n#     \"aggregation\": \"NONE\"\n#   }\n# }\n</code></pre>"},{"location":"blog/huggingface/qa/#domain-specific","title":"Domain-specific","text":"<p>QA models can also be trained to be better at answering questions at chosen domains. This one is optimized for healthcare:</p> <pre><code>version: \"1\"\n\nbolts:\n    my_bolt:\n        name: QAAPI\n        state:\n            type: none\n        input:\n            type: batch\n            args:\n                input_folder: ./input\n        output:\n            type: batch\n            args:\n                output_folder: ./output\n        method: listen\n        args:\n            model_name: dmis-lab/biobert-large-cased-v1.1-squad\n            model_class: AutoModelForQuestionAnswering\n            tokenizer_class: AutoTokenizer\n            use_cuda: true\n            precision: float\n            device_map: cuda:0\n            endpoint: \"0.0.0.0\"\n            port: 3000\n            cors_domain: http://localhost:3000\n            username: user\n            password: password\n</code></pre> <pre><code>/usr/bin/curl -X POST localhost:3000/api/v1/answer \\\n-H \"Content-Type: application/json\" \\\n-u \"user:password\" \\\n-d '{\n        \"data\": \"The choice of medication or combination of medications depends on various factors, including your personal risk factors, your age, your health and possible drug side effects. Common choices include:  Statins. Statins block a substance your liver needs to make cholesterol. This causes your liver to remove cholesterol from your blood. Choices include atorvastatin, fluvastatin, lovastatin, pitavastatin, rosuvastatin and simvastatin. Cholesterol absorption inhibitors. The drug ezetimibe helps reduce blood cholesterol by limiting the absorption of dietary cholesterol. Ezetimibe can be used with a statin drug. Bempedoic acid. This newer drug works in much the same way as statins but is less likely to cause muscle pain. Adding bempedoic acid to a maximum statin dosage can help lower LDL significantly. A combination pill containing both bempedoic acid and ezetimibe also is available. Bile-acid-binding resins. Your liver uses cholesterol to make bile acids, a substance needed for digestion. The medications cholestyramine, colesevelam and colestipol lower cholesterol indirectly by binding to bile acids. This prompts your liver to use excess cholesterol to make more bile acids, which reduces the level of cholesterol in your blood. PCSK9 inhibitors. These drugs can help the liver absorb more LDL cholesterol, which lowers the amount of cholesterol circulating in your blood. Alirocumab and evolocumab might be used for people who have a genetic condition that causes very high levels of LDL or in people with a history of coronary disease who have intolerance to statins or other cholesterol medications. They are injected under the skin every few weeks and are expensive. Medications for high triglycerides If you also have high triglycerides, your doctor might prescribe:  Fibrates. The medications fenofibrate and gemfibrozil reduce your liver s production of very-low-density lipoprotein cholesterol and speed the removal of triglycerides from your blood. VLDL cholesterol contains mostly triglycerides. Using fibrates with a statin can increase the risk of statin side effects. Omega-3 fatty acid supplements. Omega-3 fatty acid supplements can help lower your triglycerides. They are available by prescription or over-the-counter.\",\n        \"question\": \"What do i take if i have high VLDL?\"\n    }' | jq\n\n# {\n#   \"data\": \"The choice of medication or combination of medications depends on various factors, including your personal risk factors, your age, your health and possible drug side effects. Common choices include:  Statins. Statins block a substance your liver needs to make cholesterol. This causes your liver to remove cholesterol from your blood. Choices include atorvastatin, fluvastatin, lovastatin, pitavastatin, rosuvastatin and simvastatin. Cholesterol absorption inhibitors. The drug ezetimibe helps reduce blood cholesterol by limiting the absorption of dietary cholesterol. Ezetimibe can be used with a statin drug. Bempedoic acid. This newer drug works in much the same way as statins but is less likely to cause muscle pain. Adding bempedoic acid to a maximum statin dosage can help lower LDL significantly. A combination pill containing both bempedoic acid and ezetimibe also is available. Bile-acid-binding resins. Your liver uses cholesterol to make bile acids, a substance needed for digestion. The medications cholestyramine, colesevelam and colestipol lower cholesterol indirectly by binding to bile acids. This prompts your liver to use excess cholesterol to make more bile acids, which reduces the level of cholesterol in your blood. PCSK9 inhibitors. These drugs can help the liver absorb more LDL cholesterol, which lowers the amount of cholesterol circulating in your blood. Alirocumab and evolocumab might be used for people who have a genetic condition that causes very high levels of LDL or in people with a history of coronary disease who have intolerance to statins or other cholesterol medications. They are injected under the skin every few weeks and are expensive. Medications for high triglycerides If you also have high triglycerides, your doctor might prescribe:  Fibrates. The medications fenofibrate and gemfibrozil reduce your liver s production of very-low-density lipoprotein cholesterol and speed the removal of triglycerides from your blood. VLDL cholesterol contains mostly triglycerides. Using fibrates with a statin can increase the risk of statin side effects. Omega-3 fatty acid supplements. Omega-3 fatty acid supplements can help lower your triglycerides. They are available by prescription or over-the-counter.\",\n#   \"question\": \"What do i take if i have high VLDL?\",\n#   \"answer\": {\n#     \"answers\": [\n#       \"fibrates\"  &lt;-------\n#     ],\n#     \"aggregation\": \"NONE\"\n#   }\n# }\n</code></pre> <p>Now there are also models like the sloshed lawyer but they are not recommended in production \ud83d\ude06</p>"},{"location":"blog/huggingface/qa/#play-around","title":"Play around","text":"<p>There are 9,593 QA models huggingface, go exlpore!</p>"},{"location":"blog/huggingface/segment/","title":"Host Segmentation Models Using Geniusrise","text":"<p>Segmentation models are pivotal in computer vision, allowing developers to delineate and understand the context within images by classifying each pixel into a set category. This capability is crucial for tasks ranging from autonomous driving to medical imaging. Geniusrise enables easy deployment of segmentation models as APIs, facilitating the integration of advanced vision capabilities into applications. This guide will demonstrate how to set up APIs for various segmentation tasks using Geniusrise, including semantic segmentation, panoptic segmentation, and instance segmentation.</p>"},{"location":"blog/huggingface/segment/#setup-and-configuration","title":"Setup and Configuration","text":"<p>Installation:</p> <p>To begin, ensure that Geniusrise and its text extension are installed:</p> <pre><code>pip install geniusrise\npip install geniusrise-vision\n</code></pre> <p>Configuration (<code>genius.yml</code>):</p> <p>Define your segmentation service in a <code>genius.yml</code> file. Here's an example for setting up a semantic segmentation model:</p> <pre><code>version: \"1\"\nbolts:\nmy_bolt:\nname: VisionSegmentationAPI\nstate:\ntype: none\ninput:\ntype: batch\nargs:\ninput_folder: ./input\noutput:\ntype: batch\nargs:\noutput_folder: ./output\nmethod: listen\nargs:\nmodel_name: \"facebook/mask2former-swin-large-ade-panoptic\"\nmodel_class: \"Mask2FormerForUniversalSegmentation\"\nprocessor_class: \"AutoImageProcessor\"\ndevice_map: \"cuda:0\"\nuse_cuda: true\nprecision: \"float\"\nendpoint: \"0.0.0.0\"\nport: 3000\ncors_domain: \"http://localhost:3000\"\nusername: \"user\"\npassword: \"password\"\n</code></pre> <p>Activate your API by running:</p> <pre><code>genius rise\n</code></pre>"},{"location":"blog/huggingface/segment/#configuration-parameters-explained","title":"Configuration Parameters Explained","text":"<ul> <li>model_name: The pre-trained model identifier, adaptable based on the segmentation task (semantic, panoptic, instance).</li> <li>model_class &amp; processor_class: Specify the model and processor classes, essential for interpreting and processing images.</li> <li>device_map &amp; use_cuda: Configure GPU acceleration for enhanced processing speed.</li> <li>endpoint, port, username, &amp; password: Network settings and authentication for API access.</li> </ul>"},{"location":"blog/huggingface/segment/#interacting-with-the-segmentation-api","title":"Interacting with the Segmentation API","text":"<p>The interaction involves sending a base64-encoded image to the API and receiving segmented output. Here's how to execute this using <code>curl</code> and <code>python-requests</code>:</p>"},{"location":"blog/huggingface/segment/#example-with-curl","title":"Example with <code>curl</code>:","text":"<pre><code>(base64 -w 0 your_image.jpg | awk '{print \"{\\\"image_base64\\\": \\\"\"$0\"\\\", \\\"subtask\\\": \\\"semantic\\\"}\"}' &gt; /tmp/image_payload.json)\ncurl -X POST http://localhost:3000/api/v1/segment_image \\\n-H \"Content-Type: application/json\" \\\n-u user:password \\\n-d @/tmp/image_payload.json | jq\n</code></pre>"},{"location":"blog/huggingface/segment/#example-with-python-requests","title":"Example with <code>python-requests</code>:","text":"<pre><code>import requests\nimport base64\nwith open(\"your_image.jpg\", \"rb\") as image_file:\nimage_base64 = base64.b64encode(image_file.read()).decode('utf-8')\ndata = {\n\"image_base64\": image_base64,\n\"subtask\": \"semantic\"  # or \"panoptic\" for panoptic segmentation\n}\nresponse = requests.post(\"http://localhost:3000/api/v1/segment_image\",\njson=data,\nauth=('user', 'password'))\nprint(response.json())\n</code></pre>"},{"location":"blog/huggingface/segment/#use-cases-variations","title":"Use Cases &amp; Variations","text":""},{"location":"blog/huggingface/segment/#different-segmentation-tasks","title":"Different Segmentation Tasks","text":"<p>By modifying the <code>subtask</code> parameter, you can tailor the API for various segmentation models:</p> <ul> <li>Semantic Segmentation: Classifies each pixel into a predefined category. Useful in urban scene understanding and medical image analysis.</li> <li>Panoptic Segmentation: Combines semantic and instance segmentation, identifying and delineating each object instance. Ideal for detailed scene analysis.</li> <li>Instance Segmentation: Identifies each instance of each object category. Used in scenarios requiring precise object boundaries.</li> </ul>"},{"location":"blog/huggingface/segment/#customizing-segmentation-parameters","title":"Customizing Segmentation Parameters","text":"<p>For advanced segmentation needs, additional parameters can be included in your request to customize the processing, such as specifying the output resolution or the segmentation task (semantic, panoptic, instance).</p>"},{"location":"blog/huggingface/speak/","title":"Host Text to Speech Models Using Geniusrise","text":"<p>Text to Speech (TTS) technology has transformed how we interact with digital devices, making information more accessible and enhancing user experiences. Geniusrise simplifies the deployment of TTS models as APIs, allowing developers to incorporate high-quality voice synthesis into their applications. This guide focuses on setting up TTS APIs with Geniusrise, showcasing various use cases and providing examples to help you get started.</p>"},{"location":"blog/huggingface/speak/#quick-setup","title":"Quick Setup","text":"<p>Installation:</p> <p>Begin by installing Geniusrise and its dependencies:</p> <pre><code>pip install geniusrise\npip install geniusrise-vision\n</code></pre> <p>Configuration File (<code>genius.yml</code>):</p> <p>Define your TTS API using a <code>genius.yml</code> file. Here's a basic example:</p> <pre><code>version: \"1\"\nbolts:\nmy_bolt:\nname: TextToSpeechAPI\nstate:\ntype: none\ninput:\ntype: batch\nargs:\ninput_folder: ./input\noutput:\ntype: batch\nargs:\noutput_folder: ./output\nmethod: listen\nargs:\nmodel_name: \"facebook/mms-tts-eng\"\nmodel_class: \"VitsModel\"\nprocessor_class: \"VitsTokenizer\"\nuse_cuda: True\nprecision: \"float32\"\ndevice_map: \"cuda:0\"\nendpoint: \"*\"\nport: 3000\ncors_domain: \"http://localhost:3000\"\nusername: \"user\"\npassword: \"password\"\n</code></pre> <p>This configuration sets up an API for Facebook's MMS TTS English model.</p>"},{"location":"blog/huggingface/speak/#interacting-with-your-api","title":"Interacting with Your API","text":"<p>Convert text to speech by making a POST request to your API. Here's how to do it using <code>curl</code>:</p> <pre><code>curl -X POST localhost:3000/api/v1/synthesize \\\n-H \"Content-Type: application/json\" \\\n-u user:password \\\n-d '{\"text\": \"Your text here.\", \"output_type\": \"mp3\"}' \\\n| jq -r '.audio_file' | base64 -d &gt; output.mp3 &amp;&amp; vlc output.mp3\n</code></pre>"},{"location":"blog/huggingface/speak/#use-cases-variations","title":"Use Cases &amp; Variations","text":""},{"location":"blog/huggingface/speak/#multilingual-support","title":"Multilingual Support","text":"<p>Deploy models capable of synthesizing speech in multiple languages. Modify the <code>model_name</code> and add <code>tgt_lang</code> parameters to target different languages.</p>"},{"location":"blog/huggingface/speak/#voice-personalization","title":"Voice Personalization","text":"<p>Some models support different voice presets. Use the <code>voice_preset</code> parameter to select various voices, adjusting tone and style to fit your application's context.</p>"},{"location":"blog/huggingface/speak/#high-quality-synthesis","title":"High-Quality Synthesis","text":"<p>For applications requiring high-fidelity audio, select models optimized for quality, such as <code>facebook/seamless-m4t-v2-large</code>. These models often have larger sizes but produce more natural and clear voice outputs.</p>"},{"location":"blog/huggingface/speak/#real-time-applications","title":"Real-Time Applications","text":"<p>For real-time TTS needs, focus on models with lower latency. Configuration options like <code>use_cuda</code> for GPU acceleration and <code>precision</code> adjustments can help reduce response times.</p>"},{"location":"blog/huggingface/speak/#configuration-tips","title":"Configuration Tips","text":"<ul> <li>Model Selection: Experiment with various models to find the best fit for your application's language, quality, and performance requirements.</li> <li>Security: Use the <code>username</code> and <code>password</code> fields to secure your API endpoint.</li> <li>Resource Management: Adjust <code>precision</code>, <code>quantization</code>, and <code>device_map</code> settings based on your server's capabilities and your application's needs.</li> </ul>"},{"location":"blog/huggingface/speech/","title":"Host Speech to Text Models Using Geniusrise","text":"<p>Speech to Text (STT) technology has become a cornerstone in creating accessible and efficient user interfaces. Geniusrise offers a streamlined approach to deploying STT models as APIs, enabling developers to integrate speech recognition capabilities into their applications with ease. This post will guide you through setting up STT APIs using Geniusrise, highlighting various use cases and providing practical examples.</p>"},{"location":"blog/huggingface/speech/#quick-setup","title":"Quick Setup","text":"<p>Installation:</p> <p>Before you start, make sure you have Geniusrise installed:</p> <pre><code>pip install geniusrise\npip install geniusrise-vision\n</code></pre> <p>Configuration File (<code>genius.yml</code>):</p> <p>Create a <code>genius.yml</code> configuration file to define your STT API's specifications. Here\u2019s an example configuration:</p> <pre><code>version: \"1\"\nbolts:\nmy_bolt:\nname: SpeechToTextAPI\nstate:\ntype: none\ninput:\ntype: batch\nargs:\ninput_folder: ./input\noutput:\ntype: batch\nargs:\noutput_folder: ./output\nmethod: listen\nargs:\nmodel_name: \"openai/whisper-large-v3\"\nmodel_class: \"WhisperForConditionalGeneration\"\nprocessor_class: \"AutoProcessor\"\nuse_cuda: True\nprecision: \"float32\"\ndevice_map: \"cuda:0\"\nendpoint: \"*\"\nport: 3000\ncors_domain: \"http://localhost:3000\"\nusername: \"user\"\npassword: \"password\"\n</code></pre> <p>This configuration launches an STT API using OpenAI's Whisper model.</p>"},{"location":"blog/huggingface/speech/#api-interaction","title":"API Interaction","text":"<p>To interact with your STT API, encode your audio file in base64 format and construct a JSON payload. Below are examples using <code>curl</code>:</p> <pre><code># Encode your audio file to base64 and create the payload\nbase64 -w 0 sample.mp3 | awk '{print \"{\\\"audio_file\\\": \\\"\"$0\"\\\", \\\"model_sampling_rate\\\": 16000}\"}' &gt; payload.json\n\n# Send the request to your API\ncurl -X POST http://localhost:3000/api/v1/transcribe \\\n-H \"Content-Type: application/json\" \\\n-u user:password \\\n-d @payload.json | jq\n</code></pre>"},{"location":"blog/huggingface/speech/#use-cases-variations","title":"Use Cases &amp; Variations","text":""},{"location":"blog/huggingface/speech/#general-speech-recognition","title":"General Speech Recognition","text":"<p>Deploy models like <code>openai/whisper-large-v3</code> for broad speech recognition tasks across various languages and domains.</p>"},{"location":"blog/huggingface/speech/#specialized-transcription","title":"Specialized Transcription","text":"<p>For specialized domains, such as medical or legal transcription, tailor your <code>genius.yml</code> to utilize domain-specific models to improve accuracy.</p>"},{"location":"blog/huggingface/speech/#long-audio-files","title":"Long Audio Files","text":"<p>Handling long audio files efficiently requires chunking the audio into manageable pieces. Adjust <code>chunk_size</code> in your configuration to enable this feature.</p>"},{"location":"blog/huggingface/speech/#real-time-transcription","title":"Real-time Transcription","text":"<p>For real-time applications, consider models optimized for speed and responsiveness. Adjust <code>endpoint</code>, <code>port</code>, and <code>device_map</code> accordingly to minimize latency.</p>"},{"location":"blog/huggingface/speech/#advanced-configuration-tips","title":"Advanced Configuration Tips","text":"<ul> <li>Model Selection: Experiment with different models to find the one that best suits your needs. Geniusrise supports a wide range of STT models.</li> <li>Precision and Performance: Adjust the <code>precision</code> and <code>use_cuda</code> settings to balance between transcription accuracy and resource utilization.</li> <li>Security: Use <code>username</code> and <code>password</code> in your configuration to secure your API endpoint.</li> </ul>"},{"location":"blog/huggingface/summz/","title":"Host Summarization Models Using Geniusrise","text":"<p>In today's fast-paced world, the ability to condense large texts into concise summaries is invaluable. Geniusrise provides a streamlined approach to deploying summarization models as APIs, enabling developers to integrate summarization capabilities directly into their applications. This guide will walk you through setting up, configuring, and interacting with a summarization API using Geniusrise, highlighting various use cases and how to adapt the configuration for different models.</p>"},{"location":"blog/huggingface/summz/#setup-and-configuration","title":"Setup and Configuration","text":"<p>Installation:</p> <p>Begin by installing Geniusrise and its text module:</p> <pre><code>pip install geniusrise\npip install geniusrise-text\n</code></pre> <p>Configuration (<code>genius.yml</code>):</p> <p>Create a <code>genius.yml</code> to define your summarization service:</p> <pre><code>version: \"1\"\n\nbolts:\n    my_bolt:\n        name: SummarizationAPI\n        state:\n            type: none\n        input:\n            type: batch\n            args:\n                input_folder: ./input\n        output:\n            type: batch\n            args:\n                output_folder: ./output\n        method: listen\n        args:\n            model_name: facebook/bart-large-cnn\n            model_class: AutoModelForSeq2SeqLM\n            tokenizer_class: AutoTokenizer\n            use_cuda: true\n            precision: float\n            device_map: cuda:0\n            endpoint: \"0.0.0.0\"\n            port: 3000\n            cors_domain: http://localhost:3000\n            username: user\n            password: password\n</code></pre> <p>Activate your API with:</p> <pre><code>genius rise\n</code></pre>"},{"location":"blog/huggingface/summz/#configuration-parameters-explained","title":"Configuration Parameters Explained","text":"<ul> <li>model_name: Specifies the pre-trained model, such as <code>facebook/bart-large-cnn</code> for summarization.</li> <li>use_cuda: Utilizes GPU acceleration for faster processing.</li> <li>precision: Controls computational precision, affecting performance.</li> <li>endpoint &amp; port: Network address and port for API access.</li> <li>username &amp; password: Basic authentication for API security.</li> </ul>"},{"location":"blog/huggingface/summz/#interacting-with-the-summarization-api","title":"Interacting with the Summarization API","text":""},{"location":"blog/huggingface/summz/#summarizing-text","title":"Summarizing Text","text":"<p>You can summarize text by making HTTP requests to your API.</p> <p>Example with <code>curl</code>:</p> <pre><code>/usr/bin/curl -X POST localhost:3000/api/v1/summarize \\\n-H \"Content-Type: application/json\" \\\n-u \"user:password\" \\\n-d '{\n        \"text\": \"Theres something magical about Recurrent Neural Networks (RNNs). I still remember when I trained my first recurrent network for Image Captioning. Within a few dozen minutes of training my first baby model (with rather arbitrarily-chosen hyperparameters) started to generate very nice looking descriptions of images that were on the edge of making sense. Sometimes the ratio of how simple your model is to the quality of the results you get out of it blows past your expectations, and this was one of those times. What made this result so shocking at the time was that the common wisdom was that RNNs were supposed to be difficult to train (with more experience Ive in fact reached the opposite conclusion). Fast forward about a year: Im training RNNs all the time and Ive witnessed their power and robustness many times, and yet their magical outputs still find ways of amusing me.\",\n        \"decoding_strategy\": \"generate\",\n        \"bos_token_id\": 0,\n        \"decoder_start_token_id\": 2,\n        \"early_stopping\": true,\n        \"eos_token_id\": 2,\n        \"forced_bos_token_id\": 0,\n        \"forced_eos_token_id\": 2,\n        \"length_penalty\": 2.0,\n        \"max_length\": 142,\n        \"min_length\": 56,\n        \"no_repeat_ngram_size\": 3,\n        \"num_beams\": 4,\n        \"pad_token_id\": 1,\n        \"do_sample\": false\n    }' | jq\n</code></pre> <p>Example with <code>python-requests</code>:</p> <pre><code>import requests\ndata = {\n\"text\": \"Theres something magical about Recurrent Neural Networks (RNNs). I still remember when I trained my first recurrent network for Image Captioning. Within a few dozen minutes of training my first baby model (with rather arbitrarily-chosen hyperparameters) started to generate very nice looking descriptions of images that were on the edge of making sense. Sometimes the ratio of how simple your model is to the quality of the results you get out of it blows past your expectations, and this was one of those times. What made this result so shocking at the time was that the common wisdom was that RNNs were supposed to be difficult to train (with more experience Ive in fact reached the opposite conclusion). Fast forward about a year: Im training RNNs all the time and Ive witnessed their power and robustness many times, and yet their magical outputs still find ways of amusing me.\",\n\"decoding_strategy\": \"generate\",\n\"bos_token_id\": 0,\n\"decoder_start_token_id\": 2,\n\"early_stopping\": true,\n\"eos_token_id\": 2,\n\"forced_bos_token_id\": 0,\n\"forced_eos_token_id\": 2,\n\"length_penalty\": 2.0,\n\"max_length\": 142,\n\"min_length\": 56,\n\"no_repeat_ngram_size\": 3,\n\"num_beams\": 4,\n\"pad_token_id\": 1,\n\"do_sample\": false\n}\nresponse = requests.post(\"http://localhost:3000/api/v1/summarize\",\njson=data,\nauth=('user', 'password'))\nprint(response.json())\n</code></pre>"},{"location":"blog/huggingface/summz/#advanced-summarization-features","title":"Advanced Summarization Features","text":"<p>For use cases requiring specific summarization strategies or adjustments (e.g., length penalty, no repeat ngram size), additional parameters can be included in your request to customize the summarization output.</p>"},{"location":"blog/huggingface/summz/#use-cases-variations","title":"Use Cases &amp; Variations","text":""},{"location":"blog/huggingface/summz/#different-summarization-models","title":"Different Summarization Models","text":"<p>To cater to various summarization needs, such as domain-specific texts or languages, simply adjust the <code>model_name</code> in your <code>genius.yml</code>. For example, for summarizing scientific papers, you might choose a model like <code>allenai/longformer-base-4096</code>.</p>"},{"location":"blog/huggingface/summz/#customizing-summarization-parameters","title":"Customizing Summarization Parameters","text":"<p>Adjust summarization parameters such as <code>max_length</code>, <code>min_length</code>, and <code>num_beams</code> to fine-tune the output based on the specific requirements of your application.</p>"},{"location":"blog/huggingface/table_qa/","title":"Host Table Question Answering Models Using Geniusrise","text":"<ul> <li>Host Table Question Answering Models Using Geniusrise</li> <li>Setup and Configuration</li> <li>Understanding <code>genius.yml</code> Parameters</li> <li>Use Cases \\&amp; Variations<ul> <li>Changing the Model for Different Table QA Tasks</li> <li>Example <code>genius.yml</code> for tabular fact-checking:</li> </ul> </li> <li>Interacting with Your API<ul> <li>Table QA</li> <li>Utilizing the Hugging Face Pipeline</li> </ul> </li> <li>Fun<ul> <li>Executing SQL on data</li> <li>Query generators</li> </ul> </li> <li>Play around</li> </ul> <p>Deploying table question answering (QA) models is a sophisticated task that Geniusrise simplifies for developers. This guide aims to demonstrate how you can use Geniusrise to set up and run APIs for table QA, a crucial functionality for extracting structured information from tabular data. We'll cover the setup process, explain the parameters in the <code>genius.yml</code> file with examples, and provide code snippets for interacting with your API using <code>curl</code> and <code>python-requests</code>.</p>"},{"location":"blog/huggingface/table_qa/#setup-and-configuration","title":"Setup and Configuration","text":"<p>Requirements</p> <ul> <li>You need to have a GPU. Most of the system works with NVIDIA GPUs.</li> <li>Install CUDA.</li> </ul> <p>Optional: Set up a virtual environment:</p> <pre><code>virtualenv venv\nsource venv/bin/activate\n</code></pre> <p>Step 1: Install Geniusrise</p> <pre><code>pip install geniusrise\npip install geniusrise-text\n</code></pre> <p>Step 2: Configure Your API</p> <p>Create a <code>genius.yml</code> file to define the settings of your table QA API.</p> <pre><code>version: \"1\"\n\nbolts:\n    my_bolt:\n        name: QAAPI\n        state:\n            type: none\n        input:\n            type: batch\n            args:\n                input_folder: ./input\n        output:\n            type: batch\n            args:\n                output_folder: ./output\n        method: listen\n        args:\n            model_name: google/tapas-base-finetuned-wtq\n            model_class: AutoModelForTableQuestionAnswering\n            tokenizer_class: AutoTokenizer\n            use_cuda: true\n            precision: float\n            device_map: cuda:0\n            endpoint: \"0.0.0.0\"\n            port: 3000\n            cors_domain: http://localhost:3000\n            username: user\n            password: password\n</code></pre> <p>Launch your API with:</p> <pre><code>genius rise\n</code></pre>"},{"location":"blog/huggingface/table_qa/#understanding-geniusyml-parameters","title":"Understanding <code>genius.yml</code> Parameters","text":"<ul> <li>model_name: The identifier for the model from Hugging Face, designed for table QA tasks.</li> <li>model_class &amp; tokenizer_class: Specifies the classes used for the model and tokenizer, respectively, suitable for table QA.</li> <li>use_cuda: Utilize GPU acceleration to speed up inference times.</li> <li>precision: Determines the floating-point precision for calculations (e.g., <code>float</code> for single precision).</li> <li>device_map: Designates model parts to specific GPUs, optimizing performance.</li> <li>endpoint &amp; port: The network address and port where the API will be accessible.</li> <li>username &amp; password: Basic authentication credentials to secure access to your API.</li> </ul>"},{"location":"blog/huggingface/table_qa/#use-cases-variations","title":"Use Cases &amp; Variations","text":""},{"location":"blog/huggingface/table_qa/#changing-the-model-for-different-table-qa-tasks","title":"Changing the Model for Different Table QA Tasks","text":"<p>To tailor your API for different table QA tasks, such as financial data analysis or sports statistics, you can modify the <code>model_name</code> in your <code>genius.yml</code>. For example, to switch to a model optimized for financial tables, you might use <code>google/tapas-large-finetuned-finance</code>.</p>"},{"location":"blog/huggingface/table_qa/#example-geniusyml-for-tabular-fact-checking","title":"Example <code>genius.yml</code> for tabular fact-checking:","text":"<pre><code>args:\n  model_name: \"google/tapas-large-finetuned-tabfact\"\n</code></pre>"},{"location":"blog/huggingface/table_qa/#interacting-with-your-api","title":"Interacting with Your API","text":""},{"location":"blog/huggingface/table_qa/#table-qa","title":"Table QA","text":"<p>Using <code>curl</code>:</p> <pre><code>curl -X POST http://localhost:3000/api/v1/answer \\\n-H \"Content-Type: application/json\" \\\n-u \"user:password\" \\\n-d '{\"question\": \"Who had the highest batting average?\", \"data\": [{\"player\": \"John Doe\", \"average\": \".312\"}, {\"player\": \"Jane Roe\", \"average\": \".328\"}]}'\n</code></pre> <p>Using <code>python-requests</code>:</p> <pre><code>import requests\ndata = {\n\"question\": \"Who had the highest batting average?\",\n\"data\": [\n{\"player\": \"John Doe\", \"average\": \".312\"},\n{\"player\": \"Jane Roe\", \"average\": \".328\"}\n]\n}\nresponse = requests.post(\"http://localhost:3000/api/v1/answer\",\njson=data,\nauth=('user', 'password'))\nprint(response.json())\n</code></pre>"},{"location":"blog/huggingface/table_qa/#utilizing-the-hugging-face-pipeline","title":"Utilizing the Hugging Face Pipeline","text":"<p>Although primarily for text-based QA, you might experiment with the pipeline for preprocessing or extracting text from tables before querying.</p> <p>Using <code>curl</code>:</p> <pre><code>curl -X POST http://localhost:3000/api/v1/answer_pipeline \\\n-H \"Content-Type: application/json\" \\\n-u \"user:password\" \\\n-d '{\"question\": \"What is the total revenue?\", \"data\": \"The total revenue in Q1 was $10M, and in Q2 was $15M.\"}'\n</code></pre> <p>Using <code>python-requests</code>:</p> <pre><code>import requests\ndata = {\n\"question\": \"What is the total revenue?\",\n\"data\": \"\nThe total revenue in Q1 was $10M, and in Q2 was $15M.\"\n}\nresponse = requests.post(\"http://localhost:3000/api/v1/answer_pipeline\",\njson=data,\nauth=('user', 'password'))\nprint(response.json())\n</code></pre>"},{"location":"blog/huggingface/table_qa/#fun","title":"Fun","text":"<p>Table QA is dominated by two families of base models: the google TAPAS and microsoft TAPEX.</p>"},{"location":"blog/huggingface/table_qa/#executing-sql-on-data","title":"Executing SQL on data","text":"<p>Given some data and an sql query, this model can return the results.</p> <pre><code>version: \"1\"\n\nbolts:\n    my_bolt:\n        name: QAAPI\n        state:\n            type: none\n        input:\n            type: batch\n            args:\n                input_folder: ./input\n        output:\n            type: batch\n            args:\n                output_folder: ./output\n        method: listen\n        args:\n            model_name: microsoft/tapex-large-sql-execution\n            model_class: BartForConditionalGeneration\n            tokenizer_class: TapexTokenizer\n            use_cuda: true\n            precision: float\n            device_map: cuda:0\n            endpoint: \"0.0.0.0\"\n            port: 3000\n            cors_domain: http://localhost:3000\n            username: user\n            password: password\n</code></pre> <pre><code>/usr/bin/curl -X POST localhost:3000/api/v1/answer \\\n-H \"Content-Type: application/json\" \\\n-u \"user:password\" \\\n-d '{\n        \"data\": {\n            \"year\": [1896, 1900, 1904, 2004, 2008, 2012],\n            \"city\": [\"athens\", \"paris\", \"st. louis\", \"athens\", \"beijing\", \"london\"]\n        },\n        \"question\": \"select year where city = beijing\"\n  }\n  ' | jq\n\n# {\n#   \"data\": {\n#     \"year\": [\n#       1896,\n#       1900,\n#       1904,\n#       2004,\n#       2008,\n#       2012\n#     ],\n#     \"city\": [\n#       \"athens\",\n#       \"paris\",\n#       \"st. louis\",\n#       \"athens\",\n#       \"beijing\",\n#       \"london\"\n#     ]\n#   },\n#   \"question\": \"select year where city = beijing\",\n#   \"answer\": {\n#     \"answers\": [\n#       \"2008\"        # &lt;----\n#     ],\n#     \"aggregation\": \"NONE\"\n#   }\n# }\n</code></pre>"},{"location":"blog/huggingface/table_qa/#query-generators","title":"Query generators","text":"<p>Given some data and a natural language query, these models generate a query that can be used to compute the result. These models are what power spreadsheet automations.</p> <pre><code>version: \"1\"\n\nbolts:\n    my_bolt:\n        name: QAAPI\n        state:\n            type: none\n        input:\n            type: batch\n            args:\n                input_folder: ./input\n        output:\n            type: batch\n            args:\n                output_folder: ./output\n        method: listen\n        args:\n            model_name: google/tapas-large-finetuned-wtq\n            model_class: AutoModelForTableQuestionAnswering\n            tokenizer_class: AutoTokenizer\n            use_cuda: true\n            precision: float\n            device_map: cuda:0\n            endpoint: \"0.0.0.0\"\n            port: 3000\n            cors_domain: http://localhost:3000\n            username: user\n            password: password\n</code></pre> <pre><code>/usr/bin/curl -X POST localhost:3000/api/v1/answer \\\n-H \"Content-Type: application/json\" \\\n-u \"user:password\" \\\n-d '{\n        \"data\": {\n          \"population\": [\"10.6\", \"12.6\", \"12.9\", \"11.9\", \"10.3\", \"11.5\", \"12.5\", \"12.0\", \"11.5\", \"12.4\", \"11.0\", \"12.8\", \"12.5\", \"10.6\", \"11.9\", \"12.0\", \"12.6\", \"11.7\", \"12.3\", \"10.8\", \"11.2\", \"12.7\", \"10.5\", \"11.3\", \"12.2\", \"10.9\", \"11.7\", \"10.3\", \"10.9\", \"10.2\", \"10.6\", \"10.4\", \"10.5\", \"11.5\", \"11.7\", \"10.9\", \"10.4\", \"11.0\", \"12.4\", \"12.2\", \"11.3\", \"10.2\", \"11.0\", \"11.5\", \"11.0\", \"10.9\", \"11.5\", \"12.8\", \"11.3\", \"11.9\", \"12.9\", \"10.9\", \"11.4\", \"12.8\", \"10.3\", \"12.6\", \"11.1\", \"10.6\", \"12.0\", \"12.4\", \"10.2\", \"12.9\", \"11.7\", \"12.3\", \"12.4\", \"12.0\", \"10.9\", \"10.9\", \"12.3\", \"12.7\", \"10.2\", \"11.7\", \"12.4\", \"12.5\", \"12.0\", \"11.0\", \"12.9\", \"10.9\", \"10.4\", \"12.8\", \"10.3\", \"11.6\", \"12.9\", \"12.4\", \"12.4\", \"10.2\", \"11.2\", \"10.2\", \"10.1\", \"12.7\", \"11.2\", \"12.5\", \"11.7\", \"11.4\", \"10.7\", \"10.9\", \"11.5\", \"11.3\", \"10.3\", \"10.7\", \"11.2\", \"10.6\", \"11.0\", \"12.3\", \"11.7\", \"10.0\", \"10.4\", \"11.4\", \"11.5\", \"12.2\"],\n          \"city\": [\"Tokyo\", \"Delhi\", \"Shanghai\", \"Sao Paulo\", \"Mumbai\", \"Mexico City\", \"Beijing\", \"Osaka\", \"Cairo\", \"New York\", \"Dhaka\", \"Karachi\", \"Buenos Aires\", \"Kolkata\", \"Istanbul\", \"Chongqing\", \"Lagos\", \"Rio de Janeiro\", \"Tianjin\", \"Kinshasa\", \"Guangzhou\", \"Los Angeles\", \"Moscow\", \"Shenzhen\", \"Lahore\", \"Bangalore\", \"Paris\", \"Bogota\", \"Jakarta\", \"Chennai\", \"Lima\", \"Bangkok\", \"Seoul\", \"Nagoya\", \"Hyderabad\", \"London\", \"Tehran\", \"Chicago\", \"Chengdu\", \"Nanjing\", \"Wuhan\", \"Ho Chi Minh City\", \"Luanda\", \"Ahmedabad\", \"Kuala Lumpur\", \"Riyadh\", \"Baghdad\", \"Santiago\", \"Surat\", \"Madrid\", \"Suzhou\", \"Pune\", \"Houston\", \"Dallas\", \"Toronto\", \"Dar es Salaam\", \"Miami\", \"Belo Horizonte\", \"Singapore\", \"Philadelphia\", \"Atlanta\", \"Fukuoka\", \"Khartoum\", \"Barcelona\", \"Johannesburg\", \"Saint Petersburg\", \"Qingdao\", \"Dalian\", \"Washington, D.C.\", \"Yangon\", \"Alexandria\", \"Jinan\", \"Guadalajara\", \"Harbin\", \"San Francisco\", \"Fort Worth\", \"Boston\", \"Detroit\", \"Montreal\", \"Porto Alegre\", \"Ankara\", \"Monterrey\", \"Nairobi\", \"Doha\", \"Luoyang\", \"Kuwait City\", \"Dublin\", \"Mecca\", \"Medina\", \"Amman\", \"Algiers\", \"Kampala\", \"Maputo\", \"Addis Ababa\", \"Brasilia\", \"Havana\", \"Faisalabad\", \"Tashkent\", \"Accra\", \"Sapporo\", \"Manila\", \"Hanoi\", \"Sydney\", \"Melbourne\", \"Cape Town\", \"Auckland\", \"Oslo\", \"Stockholm\", \"Helsinki\", \"Copenhagen\"]\n        },\n        \"question\": \"what is the total population of these cities\"\n  }\n  ' | jq\n\n# {\n#   \"data\": {\n#     \"population\": [ ...\n#     ],\n#     \"city\": [\n#       \"Tokyo\", ...\n#     ]\n#   },\n#   \"question\": \"what is the total population of these cities\",\n#   \"answer\": {\n#     \"answers\": [\n#       \"10.6\",\n#       ...\n#       \"12.2\"\n#     ],\n#     \"aggregation\": \"COUNT\" # &lt;---\n#   }\n# }\n</code></pre> <p>The <code>answer.aggregation</code> field indicates the operation to be done on the <code>answer.answers</code> field to get the answer.</p> <p>However, when queries involve selecting one value from the data, the value of <code>answer.aggregation</code> remains as <code>NONE</code>.</p> <pre><code>/usr/bin/curl -X POST localhost:3000/api/v1/answer \\\n-H \"Content-Type: application/json\" \\\n-u \"user:password\" \\\n-d '{\n      \"data\": [\n        {\n          \"Name\": \"Acme Corp\",\n          \"Revenue\": \"1622908.31\",\n          \"Expenses\": \"802256.16\",\n          \"Profit\": \"820652.15\",\n          \"Assets\": \"2758871.86\",\n          \"Liabilities\": \"1786333.21\",\n          \"Equity\": \"972538.65\"\n        },\n        {\n          \"Name\": \"Globex Inc\",\n          \"Revenue\": \"1846200.97\",\n          \"Expenses\": \"1414781.1\",\n          \"Profit\": \"431419.87\",\n          \"Assets\": \"246642.65\",\n          \"Liabilities\": \"1969146.36\",\n          \"Equity\": \"-1722503.71\"\n        },\n        {\n          \"Name\": \"Soylent Corp\",\n          \"Revenue\": \"1585575.02\",\n          \"Expenses\": \"1457030.2\",\n          \"Profit\": \"128544.82\",\n          \"Assets\": \"1599655.56\",\n          \"Liabilities\": \"1260425.14\",\n          \"Equity\": \"339230.42\"\n        },\n        {\n          \"Name\": \"Initech LLC\",\n          \"Revenue\": \"179462.76\",\n          \"Expenses\": \"792898.88\",\n          \"Profit\": \"-613436.12\",\n          \"Assets\": \"780230.44\",\n          \"Liabilities\": \"990416.97\",\n          \"Equity\": \"-210186.53\"\n        },\n        {\n          \"Name\": \"Umbrella Corp\",\n          \"Revenue\": \"1882828.73\",\n          \"Expenses\": \"487215.16\",\n          \"Profit\": \"1395613.57\",\n          \"Assets\": \"2933377.54\",\n          \"Liabilities\": \"1519978.31\",\n          \"Equity\": \"1413399.23\"\n        },\n        {\n          \"Name\": \"Vandelay Ind\",\n          \"Revenue\": \"1641614.11\",\n          \"Expenses\": \"722957.57\",\n          \"Profit\": \"918656.54\",\n          \"Assets\": \"1818305.88\",\n          \"Liabilities\": \"1051099.45\",\n          \"Equity\": \"767206.43\"\n        },\n        {\n          \"Name\": \"Hooli Inc\",\n          \"Revenue\": \"784472.77\",\n          \"Expenses\": \"1035568.89\",\n          \"Profit\": \"-251096.12\",\n          \"Assets\": \"1011898.52\",\n          \"Liabilities\": \"757685.31\",\n          \"Equity\": \"254213.21\"\n        },\n        {\n          \"Name\": \"Stark Industries\",\n          \"Revenue\": \"1752780.24\",\n          \"Expenses\": \"954382.19\",\n          \"Profit\": \"798398.05\",\n          \"Assets\": \"1828265.8\",\n          \"Liabilities\": \"1785958.67\",\n          \"Equity\": \"42307.13\"\n        },\n        {\n          \"Name\": \"Wayne Enterprises\",\n          \"Revenue\": \"772662.41\",\n          \"Expenses\": \"724219.29\",\n          \"Profit\": \"48443.12\",\n          \"Assets\": \"2952379.67\",\n          \"Liabilities\": \"1255329.61\",\n          \"Equity\": \"1697050.06\"\n        },\n        {\n          \"Name\": \"Weyland-Yutani\",\n          \"Revenue\": \"1157644.0\",\n          \"Expenses\": \"1454230.66\",\n          \"Profit\": \"-296586.66\",\n          \"Assets\": \"776909.75\",\n          \"Liabilities\": \"759733.68\",\n          \"Equity\": \"17176.07\"\n        }\n      ],\n      \"question\": \"Given the balance sheet data, identify the company with the highest equity to assets ratio.\"\n}\n' | jq\n\n# {\n#   \"data\": [\n#     ...\n#   ],\n#   \"question\": \"Given the balance sheet data, identify the company with the highest equity to assets ratio.\",\n#   \"answer\": {\n#     \"answers\": [\n#       \"Wayne Enterprises\"\n#     ],\n#     \"aggregation\": \"NONE\"\n#   }\n# }\n</code></pre> <p>Lets verify this:</p> <pre><code>def calculate_highest_equity_to_assets_ratio(data):\n    ratios = {}\nfor company in data[\"data\"]:\n        name = company[\"Name\"]\nequity = float(company[\"Equity\"])\nassets = float(company[\"Assets\"])\nratio = equity / assets if assets != 0 else 0\nratios[name] = ratio\n\nhighest_ratio_company = max(ratios, key=ratios.get)\nhighest_ratio = ratios[highest_ratio_company]\nreturn highest_ratio_company, highest_ratio\n\nhighest_ratio_company, highest_ratio = calculate_highest_equity_to_assets_ratio(financial_data)\nhighest_ratio_company, highest_ratio\n</code></pre> <p>which gives us:</p> <pre><code>('Wayne Enterprises', 0.574807528057528)\n</code></pre> <p>yay \ud83e\udd73</p>"},{"location":"blog/huggingface/table_qa/#play-around","title":"Play around","text":"<p>This kind of models are few with 82 models on the huggingface hub.</p>"},{"location":"blog/huggingface/trans/","title":"Host Translation Models Using Geniusrise","text":"<p>This guide will walk you through deploying translation models using Geniusrise, covering the setup, configuration, and interaction with the translation API for various use cases.</p>"},{"location":"blog/huggingface/trans/#setup-and-configuration","title":"Setup and Configuration","text":"<p>Installation:</p> <p>Begin by installing Geniusrise and the necessary text processing extensions:</p> <pre><code>pip install geniusrise\npip install geniusrise-text\n</code></pre> <p>Configuration (<code>genius.yml</code>):</p> <p>Next, define your translation service in a <code>genius.yml</code> file:</p> <pre><code>version: \"1\"\n\nbolts:\n    my_bolt:\n        name: TranslationAPI\n        state:\n            type: none\n        input:\n            type: batch\n            args:\n                input_folder: ./input\n        output:\n            type: batch\n            args:\n                output_folder: ./output\n        method: listen\n        args:\n            model_name: facebook/mbart-large-50-many-to-many-mmt\n            model_class: AutoModelForSeq2SeqLM\n            tokenizer_class: AutoTokenizer\n            use_cuda: true\n            precision: float\n            device_map: cuda:0\n            endpoint: \"0.0.0.0\"\n            port: 3000\n            cors_domain: http://localhost:3000\n            username: user\n            password: password\n</code></pre> <p>To launch your API, execute:</p> <pre><code>genius rise\n</code></pre>"},{"location":"blog/huggingface/trans/#configuration-parameters-explained","title":"Configuration Parameters Explained","text":"<ul> <li>model_name: Specifies the model to use, such as <code>facebook/mbart-large-50-many-to-many-mmt</code> for multilingual translation.</li> <li>model_class &amp; tokenizer_class: Defines the classes for the model and tokenizer, crucial for the translation process.</li> <li>use_cuda: Indicates whether to use GPU acceleration for faster processing.</li> <li>precision: The computational precision (e.g., <code>float</code>) affects performance and resource usage.</li> <li>endpoint &amp; port: The network address where the API is accessible.</li> <li>username &amp; password: Security credentials for accessing the API.</li> </ul>"},{"location":"blog/huggingface/trans/#interacting-with-the-translation-api","title":"Interacting with the Translation API","text":""},{"location":"blog/huggingface/trans/#translating-text","title":"Translating Text","text":"<p>Translate text from one language to another using a simple HTTP request.</p> <p>Example using <code>curl</code>:</p> <pre><code>curl -X POST http://localhost:3000/api/v1/translate \\\n-H \"Content-Type: application/json\" \\\n-u \"user:password\" \\\n-d '{\n        \"text\": \"\u0938\u0902\u092f\u0941\u0915\u094d\u0924 \u0930\u093e\u0937\u094d\u091f\u094d\u0930 \u0915\u0947 \u092a\u094d\u0930\u092e\u0941\u0916 \u0915\u093e \u0915\u0939\u0928\u093e \u0939\u0948 \u0915\u093f \u0938\u0940\u0930\u093f\u092f\u093e \u092e\u0947\u0902 \u0915\u094b\u0908 \u0938\u0948\u0928\u094d\u092f \u0938\u092e\u093e\u0927\u093e\u0928 \u0928\u0939\u0940\u0902 \u0939\u0948\",\n        \"source_lang\": \"hi_IN\",\n        \"target_lang\": \"en_XX\",\n        \"decoding_strategy\": \"generate\",\n        \"decoder_start_token_id\": 2,\n        \"early_stopping\": true,\n        \"eos_token_id\": 2,\n        \"forced_eos_token_id\": 2,\n        \"max_length\": 200,\n        \"num_beams\": 5,\n        \"pad_token_id\": 1\n    }'\n</code></pre> <p>Example using <code>python-requests</code>:</p> <pre><code>import requests\ndata = {\n\"text\": \"\u0938\u0902\u092f\u0941\u0915\u094d\u0924 \u0930\u093e\u0937\u094d\u091f\u094d\u0930 \u0915\u0947 \u092a\u094d\u0930\u092e\u0941\u0916 \u0915\u093e \u0915\u0939\u0928\u093e \u0939\u0948 \u0915\u093f \u0938\u0940\u0930\u093f\u092f\u093e \u092e\u0947\u0902 \u0915\u094b\u0908 \u0938\u0948\u0928\u094d\u092f \u0938\u092e\u093e\u0927\u093e\u0928 \u0928\u0939\u0940\u0902 \u0939\u0948\",\n\"source_lang\": \"hi_IN\",\n\"target_lang\": \"en_XX\",\n\"decoding_strategy\": \"generate\",\n\"decoder_start_token_id\": 2,\n\"early_stopping\": true,\n\"eos_token_id\": 2,\n\"forced_eos_token_id\": 2,\n\"max_length\": 200,\n\"num_beams\": 5,\n\"pad_token_id\": 1\n}\nresponse = requests.post(\"http://localhost:3000/api/v1/translate\",\njson=data,\nauth=('user', 'password'))\nprint(response.json())\n</code></pre>"},{"location":"blog/huggingface/trans/#advanced-translation-features","title":"Advanced Translation Features","text":"<p>For use cases requiring specific translation strategies or parameters (e.g., beam search, number of beams), you can pass additional parameters in your request to customize the translation process.</p>"},{"location":"blog/huggingface/trans/#use-cases-variations","title":"Use Cases &amp; Variations","text":""},{"location":"blog/huggingface/trans/#different-language-pairs","title":"Different Language Pairs","text":"<p>Adjust the <code>source_lang</code> and <code>target_lang</code> parameters to cater to various language pairs, enabling translation between numerous languages supported by the chosen model.</p>"},{"location":"blog/huggingface/trans/#customizing-translation-parameters","title":"Customizing Translation Parameters","text":"<p>For advanced translation needs, such as controlling the length of the output or employing beam search, modify the <code>additional_params</code> in your requests:</p> <pre><code>{\n\"text\": \"Your text here\",\n\"source_lang\": \"en_XX\",\n\"target_lang\": \"es_XX\",\n\"num_beams\": 4\n}\n</code></pre>"},{"location":"blog/huggingface/trans/#fun","title":"Fun","text":"<p>There are two families of models from facebook that can perform any to any language translation among a large number of languages.</p> <ul> <li>facebook/mbart-large-50-many-to-many-mmt: 50 languages</li> <li>facebook/nllb-200-distilled-600M: 200 languages</li> </ul> <p>Both the MBART and the NLLB families have several members, with facebook/nllb-moe-54b 54billion parameter mixture of experts being the largest and most capable one.</p> <p>See here for the language codes for the FLORES-200 dataset.</p> <pre><code>version: \"1\"\n\nbolts:\n    my_bolt:\n        name: TranslationAPI\n        state:\n            type: none\n        input:\n            type: batch\n            args:\n                input_folder: ./input\n        output:\n            type: batch\n            args:\n                output_folder: ./output\n        method: listen\n        args:\n            model_name: facebook/nllb-200-3.3B\n            model_class: AutoModelForSeq2SeqLM\n            tokenizer_class: AutoTokenizer\n            use_cuda: true\n            precision: float\n            device_map: cuda:0\n            endpoint: \"0.0.0.0\"\n            port: 3000\n            cors_domain: http://localhost:3000\n            username: user\n            password: password\n</code></pre> <p>We can try translating from hindi to tatar:</p> <pre><code>curl -X POST http://localhost:3000/api/v1/translate \\\n-H \"Content-Type: application/json\" \\\n-u \"user:password\" \\\n-d '{\n        \"text\": \"\u0938\u0902\u092f\u0941\u0915\u094d\u0924 \u0930\u093e\u0937\u094d\u091f\u094d\u0930 \u0915\u0947 \u092a\u094d\u0930\u092e\u0941\u0916 \u0915\u093e \u0915\u0939\u0928\u093e \u0939\u0948 \u0915\u093f \u0938\u0940\u0930\u093f\u092f\u093e \u092e\u0947\u0902 \u0915\u094b\u0908 \u0938\u0948\u0928\u094d\u092f \u0938\u092e\u093e\u0927\u093e\u0928 \u0928\u0939\u0940\u0902 \u0939\u0948\",\n        \"target_lang\": \"tat_Cyrl\",\n        \"decoding_strategy\": \"generate\",\n        \"bos_token_id\": 0,\n        \"decoder_start_token_id\": 2,\n        \"eos_token_id\": 2,\n        \"max_length\": 200,\n        \"pad_token_id\": 1\n    }'\n</code></pre> <p>Now how do we even verify whether this is correct? Lets reverse translate followed by sentence similarity from NLI. We need to launch 2 containers - one for translation and another for NLI:</p> <pre><code>version: \"1\"\nbolts:\nmy_translation_bolt:\nname: TranslationAPI\nstate:\ntype: none\ninput:\ntype: batch\nargs:\ninput_folder: ./input\noutput:\ntype: batch\nargs:\noutput_folder: ./output\nmethod: listen\nargs:\nmodel_name: facebook/nllb-200-3.3B\nmodel_class: AutoModelForSeq2SeqLM\ntokenizer_class: AutoTokenizer\nuse_cuda: true\nprecision: float\ndevice_map: cuda:0\nendpoint: \"0.0.0.0\"\nport: 3000\ncors_domain: http://localhost:3000\nusername: user\npassword: password\nmy_nli_bolt:\nname: NLIAPI\nstate:\ntype: none\ninput:\ntype: batch\nargs:\ninput_folder: ./input\noutput:\ntype: batch\nargs:\noutput_folder: ./output\nmethod: listen\nargs:\nmodel_name: facebook/bart-large-mnli\nmodel_class: AutoModelForSequenceClassification\ntokenizer_class: AutoTokenizer\nuse_cuda: true\nprecision: float\ndevice_map: cuda:0\nendpoint: \"0.0.0.0\"\nport: 3001\ncors_domain: http://localhost:3001\nusername: user\npassword: password\n</code></pre> <p><pre><code>import requests\n# First we translate this hindi sentence to tatar\ndata = {\n\"text\": \"\u0938\u0902\u092f\u0941\u0915\u094d\u0924 \u0930\u093e\u0937\u094d\u091f\u094d\u0930 \u0915\u0947 \u092a\u094d\u0930\u092e\u0941\u0916 \u0915\u093e \u0915\u0939\u0928\u093e \u0939\u0948 \u0915\u093f \u0938\u0940\u0930\u093f\u092f\u093e \u092e\u0947\u0902 \u0915\u094b\u0908 \u0938\u0948\u0928\u094d\u092f \u0938\u092e\u093e\u0927\u093e\u0928 \u0928\u0939\u0940\u0902 \u0939\u0948\",\n\"target_lang\": \"tat_Cyrl\",\n\"decoding_strategy\": \"generate\",\n\"bos_token_id\": 0,\n\"decoder_start_token_id\": 2,\n\"eos_token_id\": 2,\n\"max_length\": 200,\n\"pad_token_id\": 1\n}\nresponse = requests.post(\"http://localhost:3000/api/v1/translate\",\njson=data,\nauth=('user', 'password'))\ntranslated = response.json()[\"translated_text\"]\n# \u0411\u041c\u041e \u0431\u0430\u0448\u043b\u044b\u0433\u044b \u0421\u04af\u0440\u0438\u044f\u0434\u04d9 \u0445\u04d9\u0440\u0431\u0438 \u0447\u0430\u0440\u0430\u043b\u0430\u0440 \u044e\u043a \u0434\u0438\u043f \u0431\u0435\u043b\u0434\u0435\u0440\u04d9\n# Then we translate the tatar back to hindi\nrev = data.copy()\nrev[\"text\"] = translated\nrev[\"target_lang\"] = \"hin_Deva\"\nresponse = requests.post(\"http://localhost:3000/api/v1/translate\",\njson=rev,\nauth=('user', 'password'))\nrev_translated = response.json()[\"translated_text\"]\n# Finally we look at similarity of the source and reverse-translated hindi sentences\ndata = {\n\"text1\": data[\"text\"],\n\"text2\": rev_translated\n}\nresponse = requests.post(\"http://localhost:3001/api/v1/textual_similarity\",\njson=data,\nauth=('user', 'password'))\nprint(response.json())\n# {\n#     'text1': '\u0938\u0902\u092f\u0941\u0915\u094d\u0924 \u0930\u093e\u0937\u094d\u091f\u094d\u0930 \u0915\u0947 \u092a\u094d\u0930\u092e\u0941\u0916 \u0915\u093e \u0915\u0939\u0928\u093e \u0939\u0948 \u0915\u093f \u0938\u0940\u0930\u093f\u092f\u093e \u092e\u0947\u0902 \u0915\u094b\u0908 \u0938\u0948\u0928\u094d\u092f \u0938\u092e\u093e\u0927\u093e\u0928 \u0928\u0939\u0940\u0902 \u0939\u0948',\n#     'text2': '\u092c\u0940\u090f\u092e\u0913 \u092a\u094d\u0930\u092e\u0941\u0916 \u0928\u0947 \u0915\u0939\u093e \u0915\u093f \u0938\u0940\u0930\u093f\u092f\u093e \u092e\u0947\u0902 \u0915\u094b\u0908 \u0938\u0948\u0928\u094d\u092f \u0909\u092a\u093e\u092f \u0928\u0939\u0940\u0902 \u0939\u0948\u0902',\n#     'similarity_score': 0.9829527983379287\n# }\n</code></pre> <code>0.9829527983379287</code> looks like a great similarity score, so the translation really works! (or the mistakes are isomorphic) \ud83e\udd73\ud83d\udc4d</p>"},{"location":"blog/huggingface/trans/#play-around","title":"Play around","text":"<p>There is not much to really do in translation except mess around with different languagues \ud83e\udd37\u200d\u2642\ufe0f Not many models either, facebook is the undisputed leader in translation models.</p>"},{"location":"blog/huggingface/txtclass/","title":"Host Text Classification Models Using Geniusrise","text":"<ul> <li>Host Text Classification Models Using Geniusrise</li> <li>Quick Setup</li> <li>Configuration Breakdown</li> <li>Use Cases \\&amp; Variations<ul> <li>Sentiment Analysis</li> <li>Content Moderation</li> <li>Language Detection</li> <li>Making API Requests</li> <li>Classify Text</li> <li>Classification Pipeline</li> </ul> </li> <li>Fun<ul> <li>Political bias detection</li> <li>Intent classification</li> <li>Hallucination Evaluation</li> <li>Irony Detection</li> </ul> </li> <li>Play around</li> </ul> <p>This post will guide you through creating inference APIs for different text classification tasks using geniusrise, explaining the <code>genius.yml</code> configuration and providing examples of how to interact with your API using <code>curl</code> and <code>python-requests</code>.</p>"},{"location":"blog/huggingface/txtclass/#quick-setup","title":"Quick Setup","text":"<p>Requirements:</p> <ul> <li>You need to have a GPU. Most of the system works with NVIDIA GPUs.</li> <li>Install CUDA.</li> </ul> <p>Installation:</p> <p>Optional: Set up a virtual environment:</p> <pre><code>virtualenv venv\nsource venv/bin/activate\n</code></pre> <p>Install the packages:</p> <pre><code>pip install torch\npip install geniusrise\npip install geniusrise-text\n</code></pre> <p>Configuration File (<code>genius.yml</code>):</p> <p>Create a <code>genius.yml</code> with the necessary configuration for your text classification API:</p> <pre><code>version: \"1\"\n\nbolts:\n    my_bolt:\n        name: TextClassificationAPI\n        state:\n            type: none\n        input:\n            type: batch\n            args:\n                input_folder: ./input\n        output:\n            type: batch\n            args:\n                output_folder: ./output\n        method: listen\n        args:\n            model_name: tomh/toxigen_roberta\n            model_class: AutoModelForSequenceClassification\n            tokenizer_class: AutoTokenizer\n            use_cuda: true\n            precision: float\n            device_map: cuda:0\n            compile: false\n            endpoint: \"0.0.0.0\"\n            port: 3000\n            cors_domain: http://localhost:3000\n            username: user\n            password: password\n</code></pre> <p>Launch your API with:</p> <pre><code>genius rise\n</code></pre>"},{"location":"blog/huggingface/txtclass/#configuration-breakdown","title":"Configuration Breakdown","text":"<ul> <li>model_name: Specify the Hugging Face model ID, e.g., <code>bert-base-uncased</code> for sentiment analysis.</li> <li>use_cuda: Enable GPU acceleration with <code>true</code> or <code>false</code> for CPU.</li> <li>precision: Set to <code>float</code> for single precision; consider <code>half</code> for faster inference on compatible GPUs. Does not work for most small models.</li> <li>device_map: Assign model parts to specific GPUs, e.g., <code>cuda:0</code>.</li> <li>endpoint &amp; port: Define the API access point.</li> <li>username &amp; password: Secure your API with basic authentication.</li> </ul>"},{"location":"blog/huggingface/txtclass/#use-cases-variations","title":"Use Cases &amp; Variations","text":""},{"location":"blog/huggingface/txtclass/#sentiment-analysis","title":"Sentiment Analysis","text":"<p>For sentiment analysis, swap the <code>model_name</code> to a model trained for sentiment, like <code>distilbert-base-uncased-finetuned-sst-2-english</code>.</p> <pre><code>args:\n  model_name: \"distilbert-base-uncased-finetuned-sst-2-english\"\n</code></pre>"},{"location":"blog/huggingface/txtclass/#content-moderation","title":"Content Moderation","text":"<p>To filter inappropriate content, use a model like <code>roberta-base-openai-detector</code>.</p> <pre><code>args:\n  model_name: \"roberta-base-openai-detector\"\n</code></pre>"},{"location":"blog/huggingface/txtclass/#language-detection","title":"Language Detection","text":"<p>For detecting the language of the input text, a model like <code>xlm-roberta-base</code> is suitable.</p> <pre><code>args:\n  model_name: \"xlm-roberta-base\"\n</code></pre> <p>Try out various models from huggingface.</p>"},{"location":"blog/huggingface/txtclass/#making-api-requests","title":"Making API Requests","text":""},{"location":"blog/huggingface/txtclass/#classify-text","title":"Classify Text","text":"<p>cURL:</p> <pre><code>curl -X POST http://localhost:3000/api/v1/classify \\\n-H \"Content-Type: application/json\" \\\n-u \"user:password\" \\\n-d '{\"text\": \"Your text here.\"}'\n</code></pre> <p>Python-Requests:</p> <pre><code>import requests\nresponse = requests.post(\"http://localhost:3000/api/v1/classify\",\njson={\"text\": \"Your text here.\"},\nauth=('user', 'password'))\nprint(response.json())\n</code></pre>"},{"location":"blog/huggingface/txtclass/#classification-pipeline","title":"Classification Pipeline","text":"<p>cURL:</p> <pre><code>curl -X POST http://localhost:3000/api/v1/classification_pipeline \\\n-H \"Content-Type: application/json\" \\\n-u \"user:password\" \\\n-d '{\"text\": \"Your text here.\"}'\n</code></pre> <p>Python-Requests:</p> <pre><code>import requests\nresponse = requests.post(\"http://localhost:3000/api/v1/classification_pipeline\",\njson={\"text\": \"Your text here.\"},\nauth=('user', 'password'))\nprint(response.json())\n</code></pre>"},{"location":"blog/huggingface/txtclass/#fun","title":"Fun","text":"<p>There are quite a few fun models to try out from huggingface!</p>"},{"location":"blog/huggingface/txtclass/#political-bias-detection","title":"Political bias detection","text":"<p>This model tries to classify text according to the political bias they might possess.</p> <pre><code>version: \"1\"\n\nbolts:\n    my_bolt:\n        name: TextClassificationAPI\n        state:\n            type: none\n        input:\n            type: batch\n            args:\n                input_folder: ./input\n        output:\n            type: batch\n            args:\n                output_folder: ./output\n        method: listen\n        args:\n            model_name: bucketresearch/politicalBiasBERT\n            model_class: AutoModelForSequenceClassification\n            tokenizer_class: AutoTokenizer\n            use_cuda: true\n            precision: float\n            device_map: cuda:0\n            compile: false\n            endpoint: \"0.0.0.0\"\n            port: 3000\n            cors_domain: http://localhost:3000\n            username: user\n            password: password\n</code></pre> <pre><code>/usr/bin/curl -X POST localhost:3000/api/v1/classify \\\n-H \"Content-Type: application/json\" \\\n-u \"user:password\" \\\n-d '{\n        \"text\": \"i think i agree with bjp that hindus need to be respected\"\n    }' | jq\n\n# {\n#   \"input\": \"i think i agree with bjp that hindus need to be respected\",\n#   \"label_scores\": {\n#     \"LEFT\": 0.28080788254737854,\n#     \"CENTER\": 0.18140915036201477,\n#     \"RIGHT\": 0.5377829670906067 # &lt;--\n#   }\n# }\n</code></pre> <pre><code>/usr/bin/curl -X POST localhost:3000/api/v1/classify \\\n-H \"Content-Type: application/json\" \\\n-u \"user:password\" \\\n-d '{\n        \"text\": \"these ghettos are sprawling these days and the people who live there stink\"\n    }' | jq\n\n# {\n#   \"input\": \"these ghettos are sprawling these days and the people who live there stink\",\n#   \"label_scores\": {\n#     \"LEFT\": 0.38681042194366455, # &lt;-- NIMBY?\n#     \"CENTER\": 0.20437702536582947,\n#     \"RIGHT\": 0.408812552690506 # &lt;--\n#   }\n# }\n</code></pre> <p>Works fairly well empirically for medium-sized sentences and in an american context.</p>"},{"location":"blog/huggingface/txtclass/#intent-classification","title":"Intent classification","text":"<p>Text classification can be used to figure out the intent of the user in a chat conversation scenario. For e.g. to determine whether the user has an intent to explore or to buy.</p> <pre><code>version: \"1\"\n\nbolts:\n    my_bolt:\n        name: TextClassificationAPI\n        state:\n            type: none\n        input:\n            type: batch\n            args:\n                input_folder: ./input\n        output:\n            type: batch\n            args:\n                output_folder: ./output\n        method: listen\n        args:\n            model_name: Falconsai/intent_classification\n            model_class: AutoModelForSequenceClassification\n            tokenizer_class: AutoTokenizer\n            use_cuda: true\n            precision: float\n            device_map: cuda:0\n            compile: false\n            endpoint: \"0.0.0.0\"\n            port: 3000\n            cors_domain: http://localhost:3000\n            username: user\n            password: password\n</code></pre> <pre><code>/usr/bin/curl -X POST localhost:3000/api/v1/classify \\\n-H \"Content-Type: application/json\" \\\n-u \"user:password\" \\\n-d '{\n        \"text\": \"hey i havent got my package yet where is it\"\n    }' | jq\n\n# {\n#   \"input\": \"hey i havent got my package yet where is it\",\n#   \"label_scores\": {\n#     \"cancellation\": 6.553709398088303E-12,\n#     \"ordering\": 4.977344745534613E-15,\n#     \"shipping\": 4.109915668426903E-15,\n#     \"invoicing\": 1.3524543897996955E-13,\n#     \"billing and payment\": 2.5260177283215057E-10,\n#     \"returns and refunds\": 1.915349389508547E-12,\n#     \"complaints and feedback\": 1.0671016614826126E-13,\n#     \"speak to person\": 2.6417441435886042E-15,\n#     \"edit account\": 3.1924864227900196E-13,\n#     \"delete account\": 2.704471304022793E-13,\n#     \"delivery information\": 1.0,                 # &lt;--\n#     \"subscription\": 1.2307567616963444E-13,\n#     \"recover password\": 1.387644556552492E-12,\n#     \"registration problems\": 2.686436142984583E-13,\n#     \"appointment\": 3.555285948454723E-13\n#   }\n# }\n</code></pre>"},{"location":"blog/huggingface/txtclass/#hallucination-evaluation","title":"Hallucination Evaluation","text":"<p>Figuring out whether your chat / LLM model is hallucinating or not is a text classification task!</p> <pre><code>version: \"1\"\n\nbolts:\n    my_bolt:\n        name: TextClassificationAPI\n        state:\n            type: none\n        input:\n            type: batch\n            args:\n                input_folder: ./input\n        output:\n            type: batch\n            args:\n                output_folder: ./output\n        method: listen\n        args:\n            model_name: vectara/hallucination_evaluation_model\n            model_class: AutoModelForSequenceClassification\n            tokenizer_class: AutoTokenizer\n            use_cuda: true\n            precision: float\n            device_map: cuda:0\n            compile: false\n            endpoint: \"0.0.0.0\"\n            port: 3000\n            cors_domain: http://localhost:3000\n            username: user\n            password: password\n</code></pre> <pre><code>/usr/bin/curl -X POST localhost:3000/api/v1/classify \\\n-H \"Content-Type: application/json\" \\\n-u \"user:password\" \\\n-d '{\n        \"text\": \"A man walks into a bar and buys a drink [SEP] A bloke swigs alcohol at a pub\"\n    }' | jq\n\n# {\n#   \"input\": \"A man walks into a bar and buys a drink [SEP] A bloke swigs alcohol at a pub\",\n#   \"label_scores\": [\n#     0.6105160713195801\n#   ]\n# }\n</code></pre>"},{"location":"blog/huggingface/txtclass/#irony-detection","title":"Irony Detection","text":"<p>Yussss NLP has advanced enough for us to be easily be able to detect irony!</p> <pre><code>version: \"1\"\n\nbolts:\n    my_bolt:\n        name: TextClassificationAPI\n        state:\n            type: none\n        input:\n            type: batch\n            args:\n                input_folder: ./input\n        output:\n            type: batch\n            args:\n                output_folder: ./output\n        method: listen\n        args:\n            model_name: cardiffnlp/twitter-roberta-base-irony\n            model_class: AutoModelForSequenceClassification\n            tokenizer_class: AutoTokenizer\n            use_cuda: true\n            precision: float\n            device_map: cuda:0\n            compile: false\n            endpoint: \"0.0.0.0\"\n            port: 3000\n            cors_domain: http://localhost:3000\n            username: user\n            password: password\n</code></pre> <pre><code>/usr/bin/curl -X POST localhost:3000/api/v1/classify \\\n-H \"Content-Type: application/json\" \\\n-u \"user:password\" \\\n-d '{\n        \"text\": \"What a wonderful day to have a flat tire!\"\n    }' | jq\n\n# {\n#   \"input\": \"What a wonderful day to have a flat tire!\",\n#   \"label_scores\": {\n#     \"non_irony\": 0.023495545610785484,\n#     \"irony\": 0.9765045046806335  &lt;---\n#   }\n# }\n</code></pre>"},{"location":"blog/huggingface/txtclass/#play-around","title":"Play around","text":"<p>There are 49,863 text classification models as of this article on huggingface. Play around with them, tweak various parameters, learn about various usecases and cool shit that can be built with \"mere\" text classification!</p>"},{"location":"blog/huggingface/vqa/","title":"Host Visual QA Models Using Geniusrise","text":"<p>Visual Question Answering (VQA) combines the power of visual understanding with natural language processing to answer questions about images. Geniusrise offers a streamlined process to deploy VQA models as APIs, making it accessible to developers to integrate advanced AI capabilities into their applications. This blog post demonstrates how to set up VQA APIs using Geniusrise and provides examples for various use cases.</p>"},{"location":"blog/huggingface/vqa/#setting-up","title":"Setting Up","text":"<p>To begin, ensure you have Geniusrise and Geniusrise-Vision installed:</p> <pre><code>pip install geniusrise\npip install geniusrise-vision\n</code></pre> <p>Create a <code>genius.yml</code> configuration file tailored to your API requirements, specifying the model, tokenizer, and additional parameters necessary for inference.</p>"},{"location":"blog/huggingface/vqa/#sample-configuration","title":"Sample Configuration","text":"<p>Below is an example of a configuration file for a VQA API:</p> <pre><code>version: \"1\"\nbolts:\nmy_bolt:\nname: VisualQAAPI\nstate:\ntype: none\ninput:\ntype: batch\nargs:\ninput_folder: ./input\noutput:\ntype: batch\nargs:\noutput_folder: ./output\nmethod: listen\nargs:\nmodel_name: \"google/pix2struct-ai2d-base\"\nmodel_class: \"Pix2StructForConditionalGeneration\"\nprocessor_class: \"Pix2StructProcessor\"\nuse_cuda: true\nprecision: \"float\"\ndevice_map: \"cuda:0\"\nendpoint: \"*\"\nport: 3000\ncors_domain: \"http://localhost:3000\"\nusername: \"user\"\npassword: \"password\"\n</code></pre> <p>This configuration sets up a VQA API using the Pix2Struct model, ready to process images and answer questions about them.</p>"},{"location":"blog/huggingface/vqa/#interacting-with-your-api","title":"Interacting with Your API","text":"<p>To interact with your VQA API, encode your images in base64 format and construct a JSON payload with the image and the question. Here are examples using <code>curl</code>:</p> <pre><code># Convert the image to base64 and prepare the payload\nbase64 -w 0 image.jpg | awk '{print \"{\\\"image_base64\\\": \\\"\"$0\"\\\", \\\"question\\\": \\\"What is in this image?\\\"}\"}' &gt; payload.json\n\n# Send the request to your API\ncurl -X POST http://localhost:3000/api/v1/answer_question \\\n-H \"Content-Type: application/json\" \\\n-u user:password \\\n-d @payload.json | jq\n</code></pre>"},{"location":"blog/huggingface/vqa/#use-cases-variations","title":"Use Cases &amp; Variations","text":""},{"location":"blog/huggingface/vqa/#general-vqa","title":"General VQA","text":"<p>Use models like <code>google/pix2struct-ai2d-base</code> for general VQA tasks, where the model predicts answers based on the image content and the posed question.</p>"},{"location":"blog/huggingface/vqa/#specialized-vqa","title":"Specialized VQA","text":"<p>For specialized domains, such as medical imaging or technical diagrams, tailor your <code>genius.yml</code> to use domain-specific models. This requires replacing the <code>model_name</code>, <code>model_class</code>, and <code>processor_class</code> with those suitable for your specific application.</p>"},{"location":"blog/huggingface/vqa/#advanced-configuration","title":"Advanced Configuration","text":"<p>Experiment with different models, precision levels, and CUDA settings to optimize performance and accuracy for your use case. Geniusrise allows for detailed configuration, including quantization and torchscript options, to fine-tune the deployment according to your requirements.</p>"},{"location":"bolts/openai/base/","title":"Base Fine Tuner","text":"<p>             Bases: <code>Bolt</code></p> <p>An abstract base class for writing bolts for fine-tuning OpenAI models.</p> <p>This base class is intended to be subclassed for fine-tuning OpenAI models. The chief objective of its subclasses is to load and preprocess the dataset, though of course, other methods, including fine-tuning, can be overridden for customization.</p> <p>This bolt uses the OpenAI API to fine-tune a pre-trained model.</p> <p>Each subclass can be invoked using the <code>genius</code> cli or yaml.</p>"},{"location":"bolts/openai/base/#geniusrise_openai.OpenAIFineTuner--using-genius-cli","title":"Using genius cli","text":"<pre><code>genius &lt;bolt_name&gt; rise \\\nbatch \\\n--input_s3_bucket my-input-bucket \\\n--input_s3_folder my-input-folder \\\nbatch \\\n--output_s3_bucket my-output-bucket \\\n--output_s3_folder my-output-folder \\\npostgres \\\n--postgres_host 127.0.0.1 \\\n--postgres_port 5432 \\\n--postgres_user postgres \\\n--postgres_password postgres \\\n--postgres_database geniusrise \\\n--postgres_table task_state \\\nfine_tune \\\n--args\n        model=gpt-3.5-turbo \\\nn_epochs=2 \\\nbatch_size=64 \\\nlearning_rate_multiplier=0.5 \\\nprompt_loss_weight=1 \\\nwait=True\n</code></pre> <p>This will load and preprocess data from input s3 location, and upload it to openai for fine tuning, and wait.</p>"},{"location":"bolts/openai/base/#geniusrise_openai.OpenAIFineTuner--using-yaml","title":"Using YAML","text":"<p>Bolts can be invoked using the <code>genius</code> cli on a yaml file.</p> <p>Create a yaml file with the following content (looks very similar to cli):</p> <pre><code>version: 1\nbolts:\nmy_fine_tuner:\nname: OpenAIClassificationFineTuner\nmethod: fine_tune\nargs:\nmodel: gpt-3.5-turbo\nn_epochs: 2\nbatch_size: 64\nlearning_rate_multiplier: 0.5\nprompt_loss_weight: 1\nwait: True\ninput:\ntype: batch\nbucket: my-input-bucket\nfolder: my-input-folder\noutput:\ntype: batch\nbucket: my-output-bucket\nfolder: my-output-folder\nstate:\ntype: postgres\nhost: 127.0.0.1\nport: 5432\nuser: postgres\npassword: postgres\ndatabase: geniusrise\ntable: state\n</code></pre> <pre><code>genius rise\n</code></pre> <p>Gotchas:</p> <ol> <li>Extra command line arguments can be passed to the load_dataset method via fine_tune method by appending <code>data_</code> to the param name.</li> </ol> <p>e.g.</p> <pre><code>        args:\nmodel: gpt-3.5-turbo\n...\ndata_some_arbitrary_key: passed_to_load_dataset_method\n</code></pre>"},{"location":"bolts/openai/base/#geniusrise_openai.base.OpenAIFineTuner.__init__","title":"<code>__init__(input, output, state)</code>","text":"<p>Initialize the bolt.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>The batch input data.</p> required <code>output</code> <code>BatchOutput</code> <p>The output data.</p> required <code>state</code> <code>State</code> <p>The state manager.</p> required"},{"location":"bolts/openai/base/#geniusrise_openai.base.OpenAIFineTuner.delete_fine_tuned_model","title":"<code>delete_fine_tuned_model(model_id)</code>  <code>staticmethod</code>","text":"<p>Delete a fine-tuned model.</p>"},{"location":"bolts/openai/base/#geniusrise_openai.base.OpenAIFineTuner.fine_tune","title":"<code>fine_tune(model, n_epochs, batch_size, learning_rate_multiplier, prompt_loss_weight, suffix=None, wait=False, data_extractor_lambda=None, **kwargs)</code>","text":"<p>Fine-tune the model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The pre-trained model name.</p> required <code>suffix</code> <code>str</code> <p>The suffix to append to the model name.</p> <code>None</code> <code>n_epochs</code> <code>int</code> <p>Total number of training epochs to perform.</p> required <code>batch_size</code> <code>int</code> <p>Batch size during training.</p> required <code>learning_rate_multiplier</code> <code>int</code> <p>Learning rate multiplier.</p> required <code>prompt_loss_weight</code> <code>int</code> <p>Prompt loss weight.</p> required <code>wait</code> <code>bool</code> <p>Whether to wait for the fine-tuning to complete. Defaults to False.</p> <code>False</code> <code>data_extractor_lambda</code> <code>str</code> <p>A lambda function run on each data element to extract the actual data.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for training and data loading.</p> <code>{}</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If any step in the fine-tuning process fails.</p>"},{"location":"bolts/openai/base/#geniusrise_openai.base.OpenAIFineTuner.get_fine_tuning_job","title":"<code>get_fine_tuning_job(job_id)</code>  <code>staticmethod</code>","text":"<p>Get the status of a fine-tuning job.</p>"},{"location":"bolts/openai/base/#geniusrise_openai.base.OpenAIFineTuner.load_dataset","title":"<code>load_dataset(dataset_path, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Load a dataset from a file.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the dataset file.</p> required <code>**kwargs</code> <p>Additional keyword arguments to pass to the <code>load_dataset</code> method.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Union[Dataset, DatasetDict, Optional[Dataset]]</code> <p>The loaded dataset.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method should be overridden by subclasses.</p>"},{"location":"bolts/openai/base/#geniusrise_openai.base.OpenAIFineTuner.prepare_fine_tuning_data","title":"<code>prepare_fine_tuning_data(data, data_type)</code>","text":"<p>Prepare the given data for fine-tuning.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Dataset, DatasetDict, Optional[Dataset]]</code> <p>The dataset to prepare.</p> required <code>data_type</code> <code>str</code> <p>Either 'train' or 'eval' to specify the type of data.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If data_type is not 'train' or 'eval'.</p>"},{"location":"bolts/openai/base/#geniusrise_openai.base.OpenAIFineTuner.preprocess_data","title":"<code>preprocess_data(**kwargs)</code>","text":"<p>Load and preprocess the dataset.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If any step in the preprocessing fails.</p>"},{"location":"bolts/openai/base/#geniusrise_openai.base.OpenAIFineTuner.wait_for_fine_tuning","title":"<code>wait_for_fine_tuning(job_id, check_interval=60)</code>","text":"<p>Wait for a fine-tuning job to complete, checking the status every <code>check_interval</code> seconds.</p>"},{"location":"bolts/openai/classification/","title":"Classification Fine Tuner","text":"<p>             Bases: <code>OpenAIFineTuner</code></p> <p>A bolt for fine-tuning OpenAI models for text classification tasks.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>The batch input data.</p> required <code>output</code> <code>BatchOutput</code> <p>The output data.</p> required <code>state</code> <code>State</code> <p>The state manager.</p> required <p>CLI Usage:</p> <pre><code>    genius HuggingFaceCommonsenseReasoningFineTuner rise \\\nbatch \\\n--input_s3_bucket geniusrise-test \\\n--input_s3_folder train \\\nbatch \\\n--output_s3_bucket geniusrise-test \\\n--output_s3_folder model \\\nfine_tune \\\n--args model_name=my_model tokenizer_name=my_tokenizer num_train_epochs=3 per_device_train_batch_size=8\n</code></pre> <p>YAML Configuration:</p> <pre><code>    version: \"1\"\nbolts:\nmy_fine_tuner:\nname: \"HuggingFaceCommonsenseReasoningFineTuner\"\nmethod: \"fine_tune\"\nargs:\nmodel_name: \"my_model\"\ntokenizer_name: \"my_tokenizer\"\nnum_train_epochs: 3\nper_device_train_batch_size: 8\ndata_max_length: 512\ninput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\nfolder: \"my_dataset\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\nfolder: \"my_model\"\ndeploy:\ntype: k8s\nargs:\nkind: deployment\nname: my_fine_tuner\ncontext_name: arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\nnamespace: geniusrise\nimage: geniusrise/geniusrise\nkube_config_path: ~/.kube/config\n</code></pre> Supported Data Formats <ul> <li>JSONL</li> <li>CSV</li> <li>Parquet</li> <li>JSON</li> <li>XML</li> <li>YAML</li> <li>TSV</li> <li>Excel (.xls, .xlsx)</li> <li>SQLite (.db)</li> <li>Feather</li> </ul>"},{"location":"bolts/openai/classification/#geniusrise_openai.classification.OpenAIClassificationFineTuner.load_dataset","title":"<code>load_dataset(dataset_path, **kwargs)</code>","text":"<p>Load a classification dataset from a directory.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the dataset directory.</p> required <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Union[Dataset, DatasetDict, Optional[Dataset]]</code> <p>The loaded dataset.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there was an error loading the dataset.</p>"},{"location":"bolts/openai/classification/#geniusrise_openai.classification.OpenAIClassificationFineTuner.load_dataset--supported-data-formats-and-structures","title":"Supported Data Formats and Structures:","text":""},{"location":"bolts/openai/classification/#geniusrise_openai.classification.OpenAIClassificationFineTuner.load_dataset--jsonl","title":"JSONL","text":"<p>Each line is a JSON object representing an example. <pre><code>{\"text\": \"The text content\", \"label\": \"The label\"}\n</code></pre></p>"},{"location":"bolts/openai/classification/#geniusrise_openai.classification.OpenAIClassificationFineTuner.load_dataset--csv","title":"CSV","text":"<p>Should contain 'text' and 'label' columns. <pre><code>text,label\n\"The text content\",\"The label\"\n</code></pre></p>"},{"location":"bolts/openai/classification/#geniusrise_openai.classification.OpenAIClassificationFineTuner.load_dataset--parquet","title":"Parquet","text":"<p>Should contain 'text' and 'label' columns.</p>"},{"location":"bolts/openai/classification/#geniusrise_openai.classification.OpenAIClassificationFineTuner.load_dataset--json","title":"JSON","text":"<p>An array of dictionaries with 'text' and 'label' keys. <pre><code>[{\"text\": \"The text content\", \"label\": \"The label\"}]\n</code></pre></p>"},{"location":"bolts/openai/classification/#geniusrise_openai.classification.OpenAIClassificationFineTuner.load_dataset--xml","title":"XML","text":"<p>Each 'record' element should contain 'text' and 'label' child elements. <pre><code>&lt;record&gt;\n&lt;text&gt;The text content&lt;/text&gt;\n&lt;label&gt;The label&lt;/label&gt;\n&lt;/record&gt;\n</code></pre></p>"},{"location":"bolts/openai/classification/#geniusrise_openai.classification.OpenAIClassificationFineTuner.load_dataset--yaml","title":"YAML","text":"<p>Each document should be a dictionary with 'text' and 'label' keys. <pre><code>- text: \"The text content\"\nlabel: \"The label\"\n</code></pre></p>"},{"location":"bolts/openai/classification/#geniusrise_openai.classification.OpenAIClassificationFineTuner.load_dataset--tsv","title":"TSV","text":"<p>Should contain 'text' and 'label' columns separated by tabs.</p>"},{"location":"bolts/openai/classification/#geniusrise_openai.classification.OpenAIClassificationFineTuner.load_dataset--excel-xls-xlsx","title":"Excel (.xls, .xlsx)","text":"<p>Should contain 'text' and 'label' columns.</p>"},{"location":"bolts/openai/classification/#geniusrise_openai.classification.OpenAIClassificationFineTuner.load_dataset--sqlite-db","title":"SQLite (.db)","text":"<p>Should contain a table with 'text' and 'label' columns.</p>"},{"location":"bolts/openai/classification/#geniusrise_openai.classification.OpenAIClassificationFineTuner.load_dataset--feather","title":"Feather","text":"<p>Should contain 'text' and 'label' columns.</p>"},{"location":"bolts/openai/commonsense_reasoning/","title":"Natural Language Inference Fine Tuner","text":"<p>             Bases: <code>OpenAIFineTuner</code></p> <p>A bolt for fine-tuning OpenAI models for commonsense reasoning tasks.</p> <p>This bolt uses the OpenAI API to fine-tune a pre-trained model for commonsense reasoning.</p> <p>CLI Usage:</p> <pre><code>    genius HuggingFaceCommonsenseReasoningFineTuner rise \\\nbatch \\\n--input_s3_bucket geniusrise-test \\\n--input_s3_folder train \\\nbatch \\\n--output_s3_bucket geniusrise-test \\\n--output_s3_folder model \\\nfine_tune \\\n--args model_name=my_model tokenizer_name=my_tokenizer num_train_epochs=3 per_device_train_batch_size=8\n</code></pre> <p>YAML Configuration:</p> <pre><code>    version: \"1\"\nbolts:\nmy_fine_tuner:\nname: \"HuggingFaceCommonsenseReasoningFineTuner\"\nmethod: \"fine_tune\"\nargs:\nmodel_name: \"my_model\"\ntokenizer_name: \"my_tokenizer\"\nnum_train_epochs: 3\nper_device_train_batch_size: 8\ndata_max_length: 512\ninput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\nfolder: \"my_dataset\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\nfolder: \"my_model\"\ndeploy:\ntype: k8s\nargs:\nkind: deployment\nname: my_fine_tuner\ncontext_name: arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\nnamespace: geniusrise\nimage: geniusrise/geniusrise\nkube_config_path: ~/.kube/config\n</code></pre> Supported Data Formats <ul> <li>JSONL</li> <li>CSV</li> <li>Parquet</li> <li>JSON</li> <li>XML</li> <li>YAML</li> <li>TSV</li> <li>Excel (.xls, .xlsx)</li> <li>SQLite (.db)</li> <li>Feather</li> </ul>"},{"location":"bolts/openai/commonsense_reasoning/#geniusrise_openai.commonsense_reasoning.OpenAICommonsenseReasoningFineTuner.load_dataset","title":"<code>load_dataset(dataset_path, **kwargs)</code>","text":"<p>Load a commonsense reasoning dataset from a directory.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the dataset directory.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Union[Dataset, DatasetDict, Optional[Dataset]]</code> <p>The loaded dataset.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there was an error loading the dataset.</p>"},{"location":"bolts/openai/commonsense_reasoning/#geniusrise_openai.commonsense_reasoning.OpenAICommonsenseReasoningFineTuner.load_dataset--supported-data-formats-and-structures","title":"Supported Data Formats and Structures:","text":""},{"location":"bolts/openai/commonsense_reasoning/#geniusrise_openai.commonsense_reasoning.OpenAICommonsenseReasoningFineTuner.load_dataset--hugging-face-dataset","title":"Hugging Face Dataset","text":"<p>Dataset files saved by the Hugging Face datasets library.</p>"},{"location":"bolts/openai/commonsense_reasoning/#geniusrise_openai.commonsense_reasoning.OpenAICommonsenseReasoningFineTuner.load_dataset--jsonl","title":"JSONL","text":"<p>Each line is a JSON object representing an example.</p>"},{"location":"bolts/openai/commonsense_reasoning/#geniusrise_openai.commonsense_reasoning.OpenAICommonsenseReasoningFineTuner.load_dataset--csv","title":"CSV","text":"<p>Should contain 'premise', 'hypothesis', and 'label' columns.</p>"},{"location":"bolts/openai/commonsense_reasoning/#geniusrise_openai.commonsense_reasoning.OpenAICommonsenseReasoningFineTuner.load_dataset--parquet","title":"Parquet","text":"<p>Should contain 'premise', 'hypothesis', and 'label' columns.</p>"},{"location":"bolts/openai/commonsense_reasoning/#geniusrise_openai.commonsense_reasoning.OpenAICommonsenseReasoningFineTuner.load_dataset--json","title":"JSON","text":"<p>An array of dictionaries with 'premise', 'hypothesis', and 'label' keys.</p>"},{"location":"bolts/openai/commonsense_reasoning/#geniusrise_openai.commonsense_reasoning.OpenAICommonsenseReasoningFineTuner.load_dataset--xml","title":"XML","text":"<p>Each 'record' element should contain 'premise', 'hypothesis', and 'label' child elements.</p>"},{"location":"bolts/openai/commonsense_reasoning/#geniusrise_openai.commonsense_reasoning.OpenAICommonsenseReasoningFineTuner.load_dataset--yaml","title":"YAML","text":"<p>Each document should be a dictionary with 'premise', 'hypothesis', and 'label' keys.</p>"},{"location":"bolts/openai/commonsense_reasoning/#geniusrise_openai.commonsense_reasoning.OpenAICommonsenseReasoningFineTuner.load_dataset--tsv","title":"TSV","text":"<p>Should contain 'premise', 'hypothesis', and 'label' columns separated by tabs.</p>"},{"location":"bolts/openai/commonsense_reasoning/#geniusrise_openai.commonsense_reasoning.OpenAICommonsenseReasoningFineTuner.load_dataset--excel-xls-xlsx","title":"Excel (.xls, .xlsx)","text":"<p>Should contain 'premise', 'hypothesis', and 'label' columns.</p>"},{"location":"bolts/openai/commonsense_reasoning/#geniusrise_openai.commonsense_reasoning.OpenAICommonsenseReasoningFineTuner.load_dataset--sqlite-db","title":"SQLite (.db)","text":"<p>Should contain a table with 'premise', 'hypothesis', and 'label' columns.</p>"},{"location":"bolts/openai/commonsense_reasoning/#geniusrise_openai.commonsense_reasoning.OpenAICommonsenseReasoningFineTuner.load_dataset--feather","title":"Feather","text":"<p>Should contain 'premise', 'hypothesis', and 'label' columns.</p>"},{"location":"bolts/openai/commonsense_reasoning/#geniusrise_openai.commonsense_reasoning.OpenAICommonsenseReasoningFineTuner.prepare_fine_tuning_data","title":"<code>prepare_fine_tuning_data(data, data_type)</code>","text":"<p>Prepare the given data for fine-tuning.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Dataset, DatasetDict, Optional[Dataset]]</code> <p>The dataset to prepare.</p> required <code>data_type</code> <code>str</code> <p>Either 'train' or 'eval' to specify the type of data.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If data_type is not 'train' or 'eval'.</p>"},{"location":"bolts/openai/instruction_tuning/","title":"Instruction Tuning Fine Tuner","text":"<p>             Bases: <code>OpenAIFineTuner</code></p> <p>A bolt for fine-tuning OpenAI models on instruction following tasks.</p> <p>This bolt uses the OpenAI API to fine-tune a pre-trained model for instruction following tasks.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>The batch input data.</p> required <code>output</code> <code>BatchOutput</code> <p>The output data.</p> required <code>state</code> <code>State</code> <p>The state manager.</p> required <p>CLI Usage:</p> <pre><code>    genius HuggingFaceCommonsenseReasoningFineTuner rise \\\nbatch \\\n--input_s3_bucket geniusrise-test \\\n--input_s3_folder train \\\nbatch \\\n--output_s3_bucket geniusrise-test \\\n--output_s3_folder model \\\nfine_tune \\\n--args model_name=my_model tokenizer_name=my_tokenizer num_train_epochs=3 per_device_train_batch_size=8\n</code></pre> <p>YAML Configuration:</p> <pre><code>    version: \"1\"\nbolts:\nmy_fine_tuner:\nname: \"HuggingFaceCommonsenseReasoningFineTuner\"\nmethod: \"fine_tune\"\nargs:\nmodel_name: \"my_model\"\ntokenizer_name: \"my_tokenizer\"\nnum_train_epochs: 3\nper_device_train_batch_size: 8\ndata_max_length: 512\ninput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\nfolder: \"my_dataset\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\nfolder: \"my_model\"\ndeploy:\ntype: k8s\nargs:\nkind: deployment\nname: my_fine_tuner\ncontext_name: arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\nnamespace: geniusrise\nimage: geniusrise/geniusrise\nkube_config_path: ~/.kube/config\n</code></pre> Supported Data Formats <ul> <li>JSONL</li> <li>CSV</li> <li>Parquet</li> <li>JSON</li> <li>XML</li> <li>YAML</li> <li>TSV</li> <li>Excel (.xls, .xlsx)</li> <li>SQLite (.db)</li> <li>Feather</li> </ul>"},{"location":"bolts/openai/instruction_tuning/#geniusrise_openai.instruction_tuning.OpenAIInstructionFineTuner.load_dataset","title":"<code>load_dataset(dataset_path, **kwargs)</code>","text":"<p>Load an instruction following dataset from a directory.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the dataset directory.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Union[Dataset, DatasetDict, Optional[Dataset]]</code> <p>The loaded dataset.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there was an error loading the dataset.</p>"},{"location":"bolts/openai/instruction_tuning/#geniusrise_openai.instruction_tuning.OpenAIInstructionFineTuner.load_dataset--supported-data-formats-and-structures","title":"Supported Data Formats and Structures:","text":""},{"location":"bolts/openai/instruction_tuning/#geniusrise_openai.instruction_tuning.OpenAIInstructionFineTuner.load_dataset--hugging-face-dataset","title":"Hugging Face Dataset","text":"<p>Dataset files saved by the Hugging Face datasets library.</p>"},{"location":"bolts/openai/instruction_tuning/#geniusrise_openai.instruction_tuning.OpenAIInstructionFineTuner.load_dataset--jsonl","title":"JSONL","text":"<p>Each line is a JSON object representing an example.</p>"},{"location":"bolts/openai/instruction_tuning/#geniusrise_openai.instruction_tuning.OpenAIInstructionFineTuner.load_dataset--csv","title":"CSV","text":"<p>Should contain 'instruction' and 'output' columns.</p>"},{"location":"bolts/openai/instruction_tuning/#geniusrise_openai.instruction_tuning.OpenAIInstructionFineTuner.load_dataset--parquet","title":"Parquet","text":"<p>Should contain 'instruction' and 'output' columns.</p>"},{"location":"bolts/openai/instruction_tuning/#geniusrise_openai.instruction_tuning.OpenAIInstructionFineTuner.load_dataset--json","title":"JSON","text":"<p>An array of dictionaries with 'instruction' and 'output' keys.</p>"},{"location":"bolts/openai/instruction_tuning/#geniusrise_openai.instruction_tuning.OpenAIInstructionFineTuner.load_dataset--xml","title":"XML","text":"<p>Each 'record' element should contain 'instruction' and 'output' child elements.</p>"},{"location":"bolts/openai/instruction_tuning/#geniusrise_openai.instruction_tuning.OpenAIInstructionFineTuner.load_dataset--yaml","title":"YAML","text":"<p>Each document should be a dictionary with 'instruction' and 'output' keys.</p>"},{"location":"bolts/openai/instruction_tuning/#geniusrise_openai.instruction_tuning.OpenAIInstructionFineTuner.load_dataset--tsv","title":"TSV","text":"<p>Should contain 'instruction' and 'output' columns separated by tabs.</p>"},{"location":"bolts/openai/instruction_tuning/#geniusrise_openai.instruction_tuning.OpenAIInstructionFineTuner.load_dataset--excel-xls-xlsx","title":"Excel (.xls, .xlsx)","text":"<p>Should contain 'instruction' and 'output' columns.</p>"},{"location":"bolts/openai/instruction_tuning/#geniusrise_openai.instruction_tuning.OpenAIInstructionFineTuner.load_dataset--sqlite-db","title":"SQLite (.db)","text":"<p>Should contain a table with 'instruction' and 'output' columns.</p>"},{"location":"bolts/openai/instruction_tuning/#geniusrise_openai.instruction_tuning.OpenAIInstructionFineTuner.load_dataset--feather","title":"Feather","text":"<p>Should contain 'instruction' and 'output' columns.</p>"},{"location":"bolts/openai/instruction_tuning/#geniusrise_openai.instruction_tuning.OpenAIInstructionFineTuner.prepare_fine_tuning_data","title":"<code>prepare_fine_tuning_data(data, data_type)</code>","text":"<p>Prepare the given data for fine-tuning.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Dataset, DatasetDict, Optional[Dataset]]</code> <p>The dataset to prepare.</p> required <code>data_type</code> <code>str</code> <p>Either 'train' or 'eval' to specify the type of data.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If data_type is not 'train' or 'eval'.</p>"},{"location":"bolts/openai/language_model/","title":"Language Model Fine Tuner","text":"<p>             Bases: <code>OpenAIFineTuner</code></p> <p>A bolt for fine-tuning OpenAI models on language modeling tasks.</p> <p>This bolt uses the OpenAI API to fine-tune a pre-trained model for language modeling.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>The batch input data.</p> required <code>output</code> <code>BatchOutput</code> <p>The output data.</p> required <code>state</code> <code>State</code> <p>The state manager.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> required <p>CLI Usage:</p> <pre><code>    genius HuggingFaceCommonsenseReasoningFineTuner rise \\\nbatch \\\n--input_s3_bucket geniusrise-test \\\n--input_s3_folder train \\\nbatch \\\n--output_s3_bucket geniusrise-test \\\n--output_s3_folder model \\\nfine_tune \\\n--args model_name=my_model tokenizer_name=my_tokenizer num_train_epochs=3 per_device_train_batch_size=8\n</code></pre> <p>YAML Configuration:</p> <pre><code>    version: \"1\"\nbolts:\nmy_fine_tuner:\nname: \"HuggingFaceCommonsenseReasoningFineTuner\"\nmethod: \"fine_tune\"\nargs:\nmodel_name: \"my_model\"\ntokenizer_name: \"my_tokenizer\"\nnum_train_epochs: 3\nper_device_train_batch_size: 8\ndata_max_length: 512\ninput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\nfolder: \"my_dataset\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\nfolder: \"my_model\"\ndeploy:\ntype: k8s\nargs:\nkind: deployment\nname: my_fine_tuner\ncontext_name: arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\nnamespace: geniusrise\nimage: geniusrise/geniusrise\nkube_config_path: ~/.kube/config\n</code></pre> Supported Data Formats <ul> <li>JSONL</li> <li>CSV</li> <li>Parquet</li> <li>JSON</li> <li>XML</li> <li>YAML</li> <li>TSV</li> <li>Excel (.xls, .xlsx)</li> <li>SQLite (.db)</li> <li>Feather</li> </ul>"},{"location":"bolts/openai/language_model/#geniusrise_openai.language_model.OpenAILanguageModelFineTuner.load_dataset","title":"<code>load_dataset(dataset_path, **kwargs)</code>","text":"<p>Load a language modeling dataset from a directory.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the dataset directory.</p> required <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Union[Dataset, DatasetDict, Optional[Dataset]]</code> <p>The loaded dataset.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there was an error loading the dataset.</p>"},{"location":"bolts/openai/language_model/#geniusrise_openai.language_model.OpenAILanguageModelFineTuner.load_dataset--supported-data-formats-and-structures","title":"Supported Data Formats and Structures:","text":""},{"location":"bolts/openai/language_model/#geniusrise_openai.language_model.OpenAILanguageModelFineTuner.load_dataset--dataset-files-saved-by-hugging-face-datasets-library","title":"Dataset files saved by Hugging Face datasets library","text":"<p>The directory should contain 'dataset_info.json' and other related files.</p>"},{"location":"bolts/openai/language_model/#geniusrise_openai.language_model.OpenAILanguageModelFineTuner.load_dataset--jsonl","title":"JSONL","text":"<p>Each line is a JSON object representing an example. <pre><code>{\"text\": \"The text content\"}\n</code></pre></p>"},{"location":"bolts/openai/language_model/#geniusrise_openai.language_model.OpenAILanguageModelFineTuner.load_dataset--csv","title":"CSV","text":"<p>Should contain 'text' column. <pre><code>text\n\"The text content\"\n</code></pre></p>"},{"location":"bolts/openai/language_model/#geniusrise_openai.language_model.OpenAILanguageModelFineTuner.load_dataset--parquet","title":"Parquet","text":"<p>Should contain 'text' column.</p>"},{"location":"bolts/openai/language_model/#geniusrise_openai.language_model.OpenAILanguageModelFineTuner.load_dataset--json","title":"JSON","text":"<p>An array of dictionaries with 'text' key. <pre><code>[{\"text\": \"The text content\"}]\n</code></pre></p>"},{"location":"bolts/openai/language_model/#geniusrise_openai.language_model.OpenAILanguageModelFineTuner.load_dataset--xml","title":"XML","text":"<p>Each 'record' element should contain 'text' child element. <pre><code>&lt;record&gt;\n&lt;text&gt;The text content&lt;/text&gt;\n&lt;/record&gt;\n</code></pre></p>"},{"location":"bolts/openai/language_model/#geniusrise_openai.language_model.OpenAILanguageModelFineTuner.load_dataset--yaml","title":"YAML","text":"<p>Each document should be a dictionary with 'text' key. <pre><code>- text: \"The text content\"\n</code></pre></p>"},{"location":"bolts/openai/language_model/#geniusrise_openai.language_model.OpenAILanguageModelFineTuner.load_dataset--tsv","title":"TSV","text":"<p>Should contain 'text' column separated by tabs.</p>"},{"location":"bolts/openai/language_model/#geniusrise_openai.language_model.OpenAILanguageModelFineTuner.load_dataset--excel-xls-xlsx","title":"Excel (.xls, .xlsx)","text":"<p>Should contain 'text' column.</p>"},{"location":"bolts/openai/language_model/#geniusrise_openai.language_model.OpenAILanguageModelFineTuner.load_dataset--sqlite-db","title":"SQLite (.db)","text":"<p>Should contain a table with 'text' column.</p>"},{"location":"bolts/openai/language_model/#geniusrise_openai.language_model.OpenAILanguageModelFineTuner.load_dataset--feather","title":"Feather","text":"<p>Should contain 'text' column.</p>"},{"location":"bolts/openai/language_model/#geniusrise_openai.language_model.OpenAILanguageModelFineTuner.prepare_fine_tuning_data","title":"<code>prepare_fine_tuning_data(data, data_type)</code>","text":"<p>Load a language modeling dataset from a directory.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the dataset directory.</p> required <code>masked</code> <code>bool</code> <p>Whether to use masked language modeling. Defaults to True.</p> required <code>max_length</code> <code>int</code> <p>The maximum length for tokenization. Defaults to 512.</p> required <p>Returns:</p> Name Type Description <code>Dataset</code> <code>None</code> <p>The loaded dataset.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there was an error loading the dataset.</p>"},{"location":"bolts/openai/language_model/#geniusrise_openai.language_model.OpenAILanguageModelFineTuner.prepare_fine_tuning_data--supported-data-formats-and-structures","title":"Supported Data Formats and Structures:","text":""},{"location":"bolts/openai/language_model/#geniusrise_openai.language_model.OpenAILanguageModelFineTuner.prepare_fine_tuning_data--dataset-files-saved-by-hugging-face-datasets-library","title":"Dataset files saved by Hugging Face datasets library","text":"<p>The directory should contain 'dataset_info.json' and other related files.</p>"},{"location":"bolts/openai/language_model/#geniusrise_openai.language_model.OpenAILanguageModelFineTuner.prepare_fine_tuning_data--jsonl","title":"JSONL","text":"<p>Each line is a JSON object representing an example. <pre><code>{\"text\": \"The text content\"}\n</code></pre></p>"},{"location":"bolts/openai/language_model/#geniusrise_openai.language_model.OpenAILanguageModelFineTuner.prepare_fine_tuning_data--csv","title":"CSV","text":"<p>Should contain 'text' column. <pre><code>text\n\"The text content\"\n</code></pre></p>"},{"location":"bolts/openai/language_model/#geniusrise_openai.language_model.OpenAILanguageModelFineTuner.prepare_fine_tuning_data--parquet","title":"Parquet","text":"<p>Should contain 'text' column.</p>"},{"location":"bolts/openai/language_model/#geniusrise_openai.language_model.OpenAILanguageModelFineTuner.prepare_fine_tuning_data--json","title":"JSON","text":"<p>An array of dictionaries with 'text' key. <pre><code>[{\"text\": \"The text content\"}]\n</code></pre></p>"},{"location":"bolts/openai/language_model/#geniusrise_openai.language_model.OpenAILanguageModelFineTuner.prepare_fine_tuning_data--xml","title":"XML","text":"<p>Each 'record' element should contain 'text' child element. <pre><code>&lt;record&gt;\n&lt;text&gt;The text content&lt;/text&gt;\n&lt;/record&gt;\n</code></pre></p>"},{"location":"bolts/openai/language_model/#geniusrise_openai.language_model.OpenAILanguageModelFineTuner.prepare_fine_tuning_data--yaml","title":"YAML","text":"<p>Each document should be a dictionary with 'text' key. <pre><code>- text: \"The text content\"\n</code></pre></p>"},{"location":"bolts/openai/language_model/#geniusrise_openai.language_model.OpenAILanguageModelFineTuner.prepare_fine_tuning_data--tsv","title":"TSV","text":"<p>Should contain 'text' column separated by tabs.</p>"},{"location":"bolts/openai/language_model/#geniusrise_openai.language_model.OpenAILanguageModelFineTuner.prepare_fine_tuning_data--excel-xls-xlsx","title":"Excel (.xls, .xlsx)","text":"<p>Should contain 'text' column.</p>"},{"location":"bolts/openai/language_model/#geniusrise_openai.language_model.OpenAILanguageModelFineTuner.prepare_fine_tuning_data--sqlite-db","title":"SQLite (.db)","text":"<p>Should contain a table with 'text' column.</p>"},{"location":"bolts/openai/language_model/#geniusrise_openai.language_model.OpenAILanguageModelFineTuner.prepare_fine_tuning_data--feather","title":"Feather","text":"<p>Should contain 'text' column.</p>"},{"location":"bolts/openai/ner/","title":"Named Entity Recognition Fine Tuner","text":"<p>             Bases: <code>OpenAIFineTuner</code></p> <p>A bolt for fine-tuning OpenAI models on named entity recognition tasks.</p> <p>This bolt extends the OpenAIFineTuner to handle the specifics of named entity recognition tasks.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>The batch input data.</p> required <code>output</code> <code>BatchOutput</code> <p>The output data.</p> required <code>state</code> <code>State</code> <p>The state manager.</p> required <p>CLI Usage:</p> <pre><code>    genius HuggingFaceCommonsenseReasoningFineTuner rise \\\nbatch \\\n--input_s3_bucket geniusrise-test \\\n--input_s3_folder train \\\nbatch \\\n--output_s3_bucket geniusrise-test \\\n--output_s3_folder model \\\nfine_tune \\\n--args model_name=my_model tokenizer_name=my_tokenizer num_train_epochs=3 per_device_train_batch_size=8\n</code></pre> <p>YAML Configuration:</p> <pre><code>    version: \"1\"\nbolts:\nmy_fine_tuner:\nname: \"HuggingFaceCommonsenseReasoningFineTuner\"\nmethod: \"fine_tune\"\nargs:\nmodel_name: \"my_model\"\ntokenizer_name: \"my_tokenizer\"\nnum_train_epochs: 3\nper_device_train_batch_size: 8\ndata_max_length: 512\ninput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\nfolder: \"my_dataset\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\nfolder: \"my_model\"\ndeploy:\ntype: k8s\nargs:\nkind: deployment\nname: my_fine_tuner\ncontext_name: arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\nnamespace: geniusrise\nimage: geniusrise/geniusrise\nkube_config_path: ~/.kube/config\n</code></pre> Supported Data Formats <ul> <li>JSONL</li> <li>CSV</li> <li>Parquet</li> <li>JSON</li> <li>XML</li> <li>YAML</li> <li>TSV</li> <li>Excel (.xls, .xlsx)</li> <li>SQLite (.db)</li> <li>Feather</li> </ul>"},{"location":"bolts/openai/ner/#geniusrise_openai.ner.NamedEntityRecognitionFineTuner.load_dataset","title":"<code>load_dataset(dataset_path, **kwargs)</code>","text":"<p>Load a named entity recognition dataset from a directory.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the dataset directory.</p> required <p>Returns:</p> Name Type Description <code>DatasetDict</code> <code>Union[Dataset, DatasetDict, None]</code> <p>The loaded dataset.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there was an error loading the dataset.</p>"},{"location":"bolts/openai/ner/#geniusrise_openai.ner.NamedEntityRecognitionFineTuner.load_dataset--supported-data-formats-and-structures","title":"Supported Data Formats and Structures:","text":""},{"location":"bolts/openai/ner/#geniusrise_openai.ner.NamedEntityRecognitionFineTuner.load_dataset--hugging-face-dataset","title":"Hugging Face Dataset","text":"<p>Dataset files saved by the Hugging Face datasets library.</p>"},{"location":"bolts/openai/ner/#geniusrise_openai.ner.NamedEntityRecognitionFineTuner.load_dataset--jsonl","title":"JSONL","text":"<p>Each line is a JSON object representing an example. <pre><code>{\"tokens\": [\"token1\", \"token2\", ...], \"ner_tags\": [0, 1, ...]}\n</code></pre></p>"},{"location":"bolts/openai/ner/#geniusrise_openai.ner.NamedEntityRecognitionFineTuner.load_dataset--csv","title":"CSV","text":"<p>Should contain 'tokens' and 'ner_tags' columns. <pre><code>tokens,ner_tags\n\"['token1', 'token2', ...]\", \"[0, 1, ...]\"\n</code></pre></p>"},{"location":"bolts/openai/ner/#geniusrise_openai.ner.NamedEntityRecognitionFineTuner.load_dataset--parquet","title":"Parquet","text":"<p>Should contain 'tokens' and 'ner_tags' columns.</p>"},{"location":"bolts/openai/ner/#geniusrise_openai.ner.NamedEntityRecognitionFineTuner.load_dataset--json","title":"JSON","text":"<p>An array of dictionaries with 'tokens' and 'ner_tags' keys. <pre><code>[{\"tokens\": [\"token1\", \"token2\", ...], \"ner_tags\": [0, 1, ...]}]\n</code></pre></p>"},{"location":"bolts/openai/ner/#geniusrise_openai.ner.NamedEntityRecognitionFineTuner.load_dataset--xml","title":"XML","text":"<p>Each 'record' element should contain 'tokens' and 'ner_tags' child elements. <pre><code>&lt;record&gt;\n&lt;tokens&gt;token1 token2 ...&lt;/tokens&gt;\n&lt;ner_tags&gt;0 1 ...&lt;/ner_tags&gt;\n&lt;/record&gt;\n</code></pre></p>"},{"location":"bolts/openai/ner/#geniusrise_openai.ner.NamedEntityRecognitionFineTuner.load_dataset--yaml","title":"YAML","text":"<p>Each document should be a dictionary with 'tokens' and 'ner_tags' keys. <pre><code>- tokens: [\"token1\", \"token2\", ...]\nner_tags: [0, 1, ...]\n</code></pre></p>"},{"location":"bolts/openai/ner/#geniusrise_openai.ner.NamedEntityRecognitionFineTuner.load_dataset--tsv","title":"TSV","text":"<p>Should contain 'tokens' and 'ner_tags' columns separated by tabs.</p>"},{"location":"bolts/openai/ner/#geniusrise_openai.ner.NamedEntityRecognitionFineTuner.load_dataset--excel-xls-xlsx","title":"Excel (.xls, .xlsx)","text":"<p>Should contain 'tokens' and 'ner_tags' columns.</p>"},{"location":"bolts/openai/ner/#geniusrise_openai.ner.NamedEntityRecognitionFineTuner.load_dataset--sqlite-db","title":"SQLite (.db)","text":"<p>Should contain a table with 'tokens' and 'ner_tags' columns.</p>"},{"location":"bolts/openai/ner/#geniusrise_openai.ner.NamedEntityRecognitionFineTuner.load_dataset--feather","title":"Feather","text":"<p>Should contain 'tokens' and 'ner_tags' columns.</p>"},{"location":"bolts/openai/ner/#geniusrise_openai.ner.NamedEntityRecognitionFineTuner.prepare_fine_tuning_data","title":"<code>prepare_fine_tuning_data(data, data_type)</code>","text":"<p>Prepare the given data for fine-tuning.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Dataset, DatasetDict, Optional[Dataset]]</code> <p>The dataset to prepare.</p> required <code>data_type</code> <code>str</code> <p>Either 'train' or 'eval' to specify the type of data.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If data_type is not 'train' or 'eval'.</p>"},{"location":"bolts/openai/question_answering/","title":"Question Answering Fine Tuner","text":"<p>             Bases: <code>OpenAIFineTuner</code></p> <p>A bolt for fine-tuning OpenAI models on question answering tasks.</p> <p>CLI Usage:</p> <pre><code>    genius HuggingFaceCommonsenseReasoningFineTuner rise \\\nbatch \\\n--input_s3_bucket geniusrise-test \\\n--input_s3_folder train \\\nbatch \\\n--output_s3_bucket geniusrise-test \\\n--output_s3_folder model \\\nfine_tune \\\n--args model_name=my_model tokenizer_name=my_tokenizer num_train_epochs=3 per_device_train_batch_size=8\n</code></pre> <p>YAML Configuration:</p> <pre><code>    version: \"1\"\nbolts:\nmy_fine_tuner:\nname: \"HuggingFaceCommonsenseReasoningFineTuner\"\nmethod: \"fine_tune\"\nargs:\nmodel_name: \"my_model\"\ntokenizer_name: \"my_tokenizer\"\nnum_train_epochs: 3\nper_device_train_batch_size: 8\ndata_max_length: 512\ninput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\nfolder: \"my_dataset\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\nfolder: \"my_model\"\ndeploy:\ntype: k8s\nargs:\nkind: deployment\nname: my_fine_tuner\ncontext_name: arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\nnamespace: geniusrise\nimage: geniusrise/geniusrise\nkube_config_path: ~/.kube/config\n</code></pre> Supported Data Formats <ul> <li>JSONL</li> <li>CSV</li> <li>Parquet</li> <li>JSON</li> <li>XML</li> <li>YAML</li> <li>TSV</li> <li>Excel (.xls, .xlsx)</li> <li>SQLite (.db)</li> <li>Feather</li> </ul>"},{"location":"bolts/openai/question_answering/#geniusrise_openai.question_answering.OpenAIQuestionAnsweringFineTuner.load_dataset","title":"<code>load_dataset(dataset_path, **kwargs)</code>","text":"<p>Load a dataset from a directory.</p>"},{"location":"bolts/openai/question_answering/#geniusrise_openai.question_answering.OpenAIQuestionAnsweringFineTuner.load_dataset--supported-data-formats-and-structures","title":"Supported Data Formats and Structures:","text":""},{"location":"bolts/openai/question_answering/#geniusrise_openai.question_answering.OpenAIQuestionAnsweringFineTuner.load_dataset--jsonl","title":"JSONL","text":"<p>Each line is a JSON object representing an example. <pre><code>{\"context\": \"The context content\", \"question\": \"The question\", \"answers\": {\"answer_start\": [int], \"context\": [str]}}\n</code></pre></p>"},{"location":"bolts/openai/question_answering/#geniusrise_openai.question_answering.OpenAIQuestionAnsweringFineTuner.load_dataset--csv","title":"CSV","text":"<p>Should contain 'context', 'question', and 'answers' columns. <pre><code>context,question,answers\n\"The context content\",\"The question\",\"{'answer_start': [int], 'text': [str]}\"\n</code></pre></p>"},{"location":"bolts/openai/question_answering/#geniusrise_openai.question_answering.OpenAIQuestionAnsweringFineTuner.load_dataset--parquet","title":"Parquet","text":"<p>Should contain 'context', 'question', and 'answers' columns.</p>"},{"location":"bolts/openai/question_answering/#geniusrise_openai.question_answering.OpenAIQuestionAnsweringFineTuner.load_dataset--json","title":"JSON","text":"<p>An array of dictionaries with 'context', 'question', and 'answers' keys. <pre><code>[{\"context\": \"The context content\", \"question\": \"The question\", \"answers\": {\"answer_start\": [int], \"context\": [str]}}]\n</code></pre></p>"},{"location":"bolts/openai/question_answering/#geniusrise_openai.question_answering.OpenAIQuestionAnsweringFineTuner.load_dataset--xml","title":"XML","text":"<p>Each 'record' element should contain 'context', 'question', and 'answers' child elements. <pre><code>&lt;record&gt;\n&lt;context&gt;The context content&lt;/context&gt;\n&lt;question&gt;The question&lt;/question&gt;\n&lt;answers answer_start=\"int\" context=\"str\"&gt;&lt;/answers&gt;\n&lt;/record&gt;\n</code></pre></p>"},{"location":"bolts/openai/question_answering/#geniusrise_openai.question_answering.OpenAIQuestionAnsweringFineTuner.load_dataset--yaml","title":"YAML","text":"<p>Each document should be a dictionary with 'context', 'question', and 'answers' keys. <pre><code>- context: \"The context content\"\nquestion: \"The question\"\nanswers:\nanswer_start: [int]\ncontext: [str]\n</code></pre></p>"},{"location":"bolts/openai/question_answering/#geniusrise_openai.question_answering.OpenAIQuestionAnsweringFineTuner.load_dataset--tsv","title":"TSV","text":"<p>Should contain 'context', 'question', and 'answers' columns separated by tabs.</p>"},{"location":"bolts/openai/question_answering/#geniusrise_openai.question_answering.OpenAIQuestionAnsweringFineTuner.load_dataset--excel-xls-xlsx","title":"Excel (.xls, .xlsx)","text":"<p>Should contain 'context', 'question', and 'answers' columns.</p>"},{"location":"bolts/openai/question_answering/#geniusrise_openai.question_answering.OpenAIQuestionAnsweringFineTuner.load_dataset--sqlite-db","title":"SQLite (.db)","text":"<p>Should contain a table with 'context', 'question', and 'answers' columns.</p>"},{"location":"bolts/openai/question_answering/#geniusrise_openai.question_answering.OpenAIQuestionAnsweringFineTuner.load_dataset--feather","title":"Feather","text":"<p>Should contain 'context', 'question', and 'answers' columns.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the dataset directory.</p> required <code>pad_on_right</code> <code>bool</code> <p>Whether to pad on the right.</p> required <code>max_length</code> <code>int</code> <p>The maximum length of the sequences.</p> required <code>doc_stride</code> <code>int</code> <p>The document stride.</p> required <code>evaluate_squadv2</code> <code>bool</code> <p>Whether to evaluate using SQuAD v2 metrics.</p> required <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Union[Dataset, DatasetDict, Optional[Dataset]]</code> <p>The loaded dataset.</p>"},{"location":"bolts/openai/question_answering/#geniusrise_openai.question_answering.OpenAIQuestionAnsweringFineTuner.prepare_fine_tuning_data","title":"<code>prepare_fine_tuning_data(data, data_type)</code>","text":"<p>Prepare the given data for fine-tuning.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Dataset, DatasetDict, Optional[Dataset]]</code> <p>The dataset to prepare.</p> required <code>data_type</code> <code>str</code> <p>Either 'train' or 'eval' to specify the type of data.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If data_type is not 'train' or 'eval'.</p>"},{"location":"bolts/openai/sentiment_analysis/","title":"Sentiment Analysis Fine Tuner","text":"<p>             Bases: <code>OpenAIFineTuner</code></p> <p>A bolt for fine-tuning OpenAI models on sentiment analysis tasks.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>The batch input data.</p> required <code>output</code> <code>BatchOutput</code> <p>The output data.</p> required <code>state</code> <code>State</code> <p>The state manager.</p> required <p>CLI Usage:</p> <pre><code>    genius HuggingFaceCommonsenseReasoningFineTuner rise \\\nbatch \\\n--input_s3_bucket geniusrise-test \\\n--input_s3_folder train \\\nbatch \\\n--output_s3_bucket geniusrise-test \\\n--output_s3_folder model \\\nfine_tune \\\n--args model_name=my_model tokenizer_name=my_tokenizer num_train_epochs=3 per_device_train_batch_size=8\n</code></pre> <p>YAML Configuration:</p> <pre><code>    version: \"1\"\nbolts:\nmy_fine_tuner:\nname: \"HuggingFaceCommonsenseReasoningFineTuner\"\nmethod: \"fine_tune\"\nargs:\nmodel_name: \"my_model\"\ntokenizer_name: \"my_tokenizer\"\nnum_train_epochs: 3\nper_device_train_batch_size: 8\ndata_max_length: 512\ninput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\nfolder: \"my_dataset\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\nfolder: \"my_model\"\ndeploy:\ntype: k8s\nargs:\nkind: deployment\nname: my_fine_tuner\ncontext_name: arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\nnamespace: geniusrise\nimage: geniusrise/geniusrise\nkube_config_path: ~/.kube/config\n</code></pre> Supported Data Formats <ul> <li>JSONL</li> <li>CSV</li> <li>Parquet</li> <li>JSON</li> <li>XML</li> <li>YAML</li> <li>TSV</li> <li>Excel (.xls, .xlsx)</li> <li>SQLite (.db)</li> <li>Feather</li> </ul>"},{"location":"bolts/openai/sentiment_analysis/#geniusrise_openai.sentiment_analysis.OpenAISentimentAnalysisFineTuner.load_dataset","title":"<code>load_dataset(dataset_path, **kwargs)</code>","text":"<p>Load a dataset from a directory.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the dataset directory.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[Dataset, DatasetDict, Optional[Dataset]]</code> <p>Dataset | DatasetDict: The loaded dataset.</p>"},{"location":"bolts/openai/sentiment_analysis/#geniusrise_openai.sentiment_analysis.OpenAISentimentAnalysisFineTuner.load_dataset--supported-data-formats-and-structures","title":"Supported Data Formats and Structures:","text":""},{"location":"bolts/openai/sentiment_analysis/#geniusrise_openai.sentiment_analysis.OpenAISentimentAnalysisFineTuner.load_dataset--jsonl","title":"JSONL","text":"<p>Each line is a JSON object representing an example. <pre><code>{\"text\": \"The text content\", \"label\": \"The label\"}\n</code></pre></p>"},{"location":"bolts/openai/sentiment_analysis/#geniusrise_openai.sentiment_analysis.OpenAISentimentAnalysisFineTuner.load_dataset--csv","title":"CSV","text":"<p>Should contain 'text' and 'label' columns. <pre><code>text,label\n\"The text content\",\"The label\"\n</code></pre></p>"},{"location":"bolts/openai/sentiment_analysis/#geniusrise_openai.sentiment_analysis.OpenAISentimentAnalysisFineTuner.load_dataset--parquet","title":"Parquet","text":"<p>Should contain 'text' and 'label' columns.</p>"},{"location":"bolts/openai/sentiment_analysis/#geniusrise_openai.sentiment_analysis.OpenAISentimentAnalysisFineTuner.load_dataset--json","title":"JSON","text":"<p>An array of dictionaries with 'text' and 'label' keys. <pre><code>[{\"text\": \"The text content\", \"label\": \"The label\"}]\n</code></pre></p>"},{"location":"bolts/openai/sentiment_analysis/#geniusrise_openai.sentiment_analysis.OpenAISentimentAnalysisFineTuner.load_dataset--xml","title":"XML","text":"<p>Each 'record' element should contain 'text' and 'label' child elements. <pre><code>&lt;record&gt;\n&lt;text&gt;The text content&lt;/text&gt;\n&lt;label&gt;The label&lt;/label&gt;\n&lt;/record&gt;\n</code></pre></p>"},{"location":"bolts/openai/sentiment_analysis/#geniusrise_openai.sentiment_analysis.OpenAISentimentAnalysisFineTuner.load_dataset--yaml","title":"YAML","text":"<p>Each document should be a dictionary with 'text' and 'label' keys. <pre><code>- text: \"The text content\"\nlabel: \"The label\"\n</code></pre></p>"},{"location":"bolts/openai/sentiment_analysis/#geniusrise_openai.sentiment_analysis.OpenAISentimentAnalysisFineTuner.load_dataset--tsv","title":"TSV","text":"<p>Should contain 'text' and 'label' columns separated by tabs.</p>"},{"location":"bolts/openai/sentiment_analysis/#geniusrise_openai.sentiment_analysis.OpenAISentimentAnalysisFineTuner.load_dataset--excel-xls-xlsx","title":"Excel (.xls, .xlsx)","text":"<p>Should contain 'text' and 'label' columns.</p>"},{"location":"bolts/openai/sentiment_analysis/#geniusrise_openai.sentiment_analysis.OpenAISentimentAnalysisFineTuner.load_dataset--sqlite-db","title":"SQLite (.db)","text":"<p>Should contain a table with 'text' and 'label' columns.</p>"},{"location":"bolts/openai/sentiment_analysis/#geniusrise_openai.sentiment_analysis.OpenAISentimentAnalysisFineTuner.load_dataset--feather","title":"Feather","text":"<p>Should contain 'text' and 'label' columns.</p>"},{"location":"bolts/openai/sentiment_analysis/#geniusrise_openai.sentiment_analysis.OpenAISentimentAnalysisFineTuner.prepare_fine_tuning_data","title":"<code>prepare_fine_tuning_data(data, data_type)</code>","text":"<p>Prepare the given data for fine-tuning.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Dataset, DatasetDict, Optional[Dataset]]</code> <p>The dataset to prepare.</p> required <code>data_type</code> <code>str</code> <p>Either 'train' or 'eval' to specify the type of data.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If data_type is not 'train' or 'eval'.</p>"},{"location":"bolts/openai/summarization/","title":"Sentiment Analysis Fine Tuner","text":"<p>             Bases: <code>OpenAIFineTuner</code></p> <p>A bolt for fine-tuning OpenAI models for summarization tasks.</p> <p>This bolt uses the OpenAI API to fine-tune a pre-trained model for summarization.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>The batch input data.</p> required <code>output</code> <code>BatchOutput</code> <p>The output data.</p> required <code>state</code> <code>State</code> <p>The state manager.</p> required <p>CLI Usage:</p> <pre><code>    genius HuggingFaceCommonsenseReasoningFineTuner rise \\\nbatch \\\n--input_s3_bucket geniusrise-test \\\n--input_s3_folder train \\\nbatch \\\n--output_s3_bucket geniusrise-test \\\n--output_s3_folder model \\\nfine_tune \\\n--args model_name=my_model tokenizer_name=my_tokenizer num_train_epochs=3 per_device_train_batch_size=8\n</code></pre> <p>YAML Configuration:</p> <pre><code>    version: \"1\"\nbolts:\nmy_fine_tuner:\nname: \"HuggingFaceCommonsenseReasoningFineTuner\"\nmethod: \"fine_tune\"\nargs:\nmodel_name: \"my_model\"\ntokenizer_name: \"my_tokenizer\"\nnum_train_epochs: 3\nper_device_train_batch_size: 8\ndata_max_length: 512\ninput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\nfolder: \"my_dataset\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\nfolder: \"my_model\"\ndeploy:\ntype: k8s\nargs:\nkind: deployment\nname: my_fine_tuner\ncontext_name: arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\nnamespace: geniusrise\nimage: geniusrise/geniusrise\nkube_config_path: ~/.kube/config\n</code></pre> Supported Data Formats <ul> <li>JSONL</li> <li>CSV</li> <li>Parquet</li> <li>JSON</li> <li>XML</li> <li>YAML</li> <li>TSV</li> <li>Excel (.xls, .xlsx)</li> <li>SQLite (.db)</li> <li>Feather</li> </ul>"},{"location":"bolts/openai/summarization/#geniusrise_openai.summarization.OpenAISummarizationFineTuner.load_dataset","title":"<code>load_dataset(dataset_path, **kwargs)</code>","text":"<p>Load a dataset from a directory.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the dataset directory.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[DatasetDict]</code> <p>Dataset | DatasetDict: The loaded dataset.</p>"},{"location":"bolts/openai/summarization/#geniusrise_openai.summarization.OpenAISummarizationFineTuner.load_dataset--supported-data-formats-and-structures","title":"Supported Data Formats and Structures:","text":""},{"location":"bolts/openai/summarization/#geniusrise_openai.summarization.OpenAISummarizationFineTuner.load_dataset--jsonl","title":"JSONL","text":"<p>Each line is a JSON object representing an example. <pre><code>{\"text\": \"The text content\", \"summary\": \"The summary\"}\n</code></pre></p>"},{"location":"bolts/openai/summarization/#geniusrise_openai.summarization.OpenAISummarizationFineTuner.load_dataset--csv","title":"CSV","text":"<p>Should contain 'text' and 'summary' columns. <pre><code>text,summary\n\"The text content\",\"The summary\"\n</code></pre></p>"},{"location":"bolts/openai/summarization/#geniusrise_openai.summarization.OpenAISummarizationFineTuner.load_dataset--parquet","title":"Parquet","text":"<p>Should contain 'text' and 'summary' columns.</p>"},{"location":"bolts/openai/summarization/#geniusrise_openai.summarization.OpenAISummarizationFineTuner.load_dataset--json","title":"JSON","text":"<p>An array of dictionaries with 'text' and 'summary' keys. <pre><code>[{\"text\": \"The text content\", \"summary\": \"The summary\"}]\n</code></pre></p>"},{"location":"bolts/openai/summarization/#geniusrise_openai.summarization.OpenAISummarizationFineTuner.load_dataset--xml","title":"XML","text":"<p>Each 'record' element should contain 'text' and 'summary' child elements. <pre><code>&lt;record&gt;\n&lt;text&gt;The text content&lt;/text&gt;\n&lt;summary&gt;The summary&lt;/summary&gt;\n&lt;/record&gt;\n</code></pre></p>"},{"location":"bolts/openai/summarization/#geniusrise_openai.summarization.OpenAISummarizationFineTuner.load_dataset--yaml","title":"YAML","text":"<p>Each document should be a dictionary with 'text' and 'summary' keys. <pre><code>- text: \"The text content\"\nsummary: \"The summary\"\n</code></pre></p>"},{"location":"bolts/openai/summarization/#geniusrise_openai.summarization.OpenAISummarizationFineTuner.load_dataset--tsv","title":"TSV","text":"<p>Should contain 'text' and 'summary' columns separated by tabs.</p>"},{"location":"bolts/openai/summarization/#geniusrise_openai.summarization.OpenAISummarizationFineTuner.load_dataset--excel-xls-xlsx","title":"Excel (.xls, .xlsx)","text":"<p>Should contain 'text' and 'summary' columns.</p>"},{"location":"bolts/openai/summarization/#geniusrise_openai.summarization.OpenAISummarizationFineTuner.load_dataset--sqlite-db","title":"SQLite (.db)","text":"<p>Should contain a table with 'text' and 'summary' columns.</p>"},{"location":"bolts/openai/summarization/#geniusrise_openai.summarization.OpenAISummarizationFineTuner.load_dataset--feather","title":"Feather","text":"<p>Should contain 'text' and 'summary' columns.</p>"},{"location":"bolts/openai/summarization/#geniusrise_openai.summarization.OpenAISummarizationFineTuner.prepare_fine_tuning_data","title":"<code>prepare_fine_tuning_data(data, data_type)</code>","text":"<p>Prepare the given data for fine-tuning.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dataset | DatasetDict | Optional[Dataset]</code> <p>The dataset to prepare.</p> required <code>data_type</code> <code>str</code> <p>Either 'train' or 'eval' to specify the type of data.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If data_type is not 'train' or 'eval'.</p>"},{"location":"bolts/openai/translation/","title":"Sentiment Analysis Fine Tuner","text":"<p>             Bases: <code>OpenAIFineTuner</code></p> <p>A bolt for fine-tuning OpenAI models for translation tasks.</p> <p>This bolt uses the OpenAI API to fine-tune a pre-trained model for translation.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>The batch input data.</p> required <code>output</code> <code>BatchOutput</code> <p>The output data.</p> required <code>state</code> <code>State</code> <p>The state manager.</p> required <p>CLI Usage:</p> <pre><code>    genius HuggingFaceCommonsenseReasoningFineTuner rise \\\nbatch \\\n--input_s3_bucket geniusrise-test \\\n--input_s3_folder train \\\nbatch \\\n--output_s3_bucket geniusrise-test \\\n--output_s3_folder model \\\nfine_tune \\\n--args model_name=my_model tokenizer_name=my_tokenizer num_train_epochs=3 per_device_train_batch_size=8\n</code></pre> <p>YAML Configuration:</p> <pre><code>    version: \"1\"\nbolts:\nmy_fine_tuner:\nname: \"HuggingFaceCommonsenseReasoningFineTuner\"\nmethod: \"fine_tune\"\nargs:\nmodel_name: \"my_model\"\ntokenizer_name: \"my_tokenizer\"\nnum_train_epochs: 3\nper_device_train_batch_size: 8\ndata_max_length: 512\ninput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\nfolder: \"my_dataset\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\nfolder: \"my_model\"\ndeploy:\ntype: k8s\nargs:\nkind: deployment\nname: my_fine_tuner\ncontext_name: arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\nnamespace: geniusrise\nimage: geniusrise/geniusrise\nkube_config_path: ~/.kube/config\n</code></pre> Supported Data Formats <ul> <li>JSONL</li> <li>CSV</li> <li>Parquet</li> <li>JSON</li> <li>XML</li> <li>YAML</li> <li>TSV</li> <li>Excel (.xls, .xlsx)</li> <li>SQLite (.db)</li> <li>Feather</li> </ul>"},{"location":"bolts/openai/translation/#geniusrise_openai.translation.OpenAITranslationFineTuner.load_dataset","title":"<code>load_dataset(dataset_path, origin='en', target='fr', **kwargs)</code>","text":"<p>Load a dataset from a directory.</p>"},{"location":"bolts/openai/translation/#geniusrise_openai.translation.OpenAITranslationFineTuner.load_dataset--supported-data-formats-and-structures-for-translation-tasks","title":"Supported Data Formats and Structures for Translation Tasks:","text":""},{"location":"bolts/openai/translation/#geniusrise_openai.translation.OpenAITranslationFineTuner.load_dataset--jsonl","title":"JSONL","text":"<p>Each line is a JSON object representing an example. <pre><code>{\n\"translation\": {\n\"en\": \"English text\",\n\"fr\": \"French text\"\n}\n}\n</code></pre></p>"},{"location":"bolts/openai/translation/#geniusrise_openai.translation.OpenAITranslationFineTuner.load_dataset--csv","title":"CSV","text":"<p>Should contain 'en' and 'fr' columns. <pre><code>en,fr\n\"English text\",\"French text\"\n</code></pre></p>"},{"location":"bolts/openai/translation/#geniusrise_openai.translation.OpenAITranslationFineTuner.load_dataset--parquet","title":"Parquet","text":"<p>Should contain 'en' and 'fr' columns.</p>"},{"location":"bolts/openai/translation/#geniusrise_openai.translation.OpenAITranslationFineTuner.load_dataset--json","title":"JSON","text":"<p>An array of dictionaries with 'en' and 'fr' keys. <pre><code>[\n{\n\"en\": \"English text\",\n\"fr\": \"French text\"\n}\n]\n</code></pre></p>"},{"location":"bolts/openai/translation/#geniusrise_openai.translation.OpenAITranslationFineTuner.load_dataset--xml","title":"XML","text":"<p>Each 'record' element should contain 'en' and 'fr' child elements. <pre><code>&lt;record&gt;\n&lt;en&gt;English text&lt;/en&gt;\n&lt;fr&gt;French text&lt;/fr&gt;\n&lt;/record&gt;\n</code></pre></p>"},{"location":"bolts/openai/translation/#geniusrise_openai.translation.OpenAITranslationFineTuner.load_dataset--yaml","title":"YAML","text":"<p>Each document should be a dictionary with 'en' and 'fr' keys. <pre><code>- en: \"English text\"\nfr: \"French text\"\n</code></pre></p>"},{"location":"bolts/openai/translation/#geniusrise_openai.translation.OpenAITranslationFineTuner.load_dataset--tsv","title":"TSV","text":"<p>Should contain 'en' and 'fr' columns separated by tabs.</p>"},{"location":"bolts/openai/translation/#geniusrise_openai.translation.OpenAITranslationFineTuner.load_dataset--excel-xls-xlsx","title":"Excel (.xls, .xlsx)","text":"<p>Should contain 'en' and 'fr' columns.</p>"},{"location":"bolts/openai/translation/#geniusrise_openai.translation.OpenAITranslationFineTuner.load_dataset--sqlite-db","title":"SQLite (.db)","text":"<p>Should contain a table with 'en' and 'fr' columns.</p>"},{"location":"bolts/openai/translation/#geniusrise_openai.translation.OpenAITranslationFineTuner.load_dataset--feather","title":"Feather","text":"<p>Should contain 'en' and 'fr' columns.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the directory containing the dataset files.</p> required <code>max_length</code> <code>int</code> <p>The maximum length for tokenization. Defaults to 512.</p> required <code>origin</code> <code>str</code> <p>The origin language. Defaults to 'en'.</p> <code>'en'</code> <code>target</code> <code>str</code> <p>The target language. Defaults to 'fr'.</p> <code>'fr'</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>DatasetDict</code> <code>Dataset | DatasetDict | Optional[Dataset]</code> <p>The loaded dataset.</p>"},{"location":"bolts/openai/translation/#geniusrise_openai.translation.OpenAITranslationFineTuner.prepare_fine_tuning_data","title":"<code>prepare_fine_tuning_data(data, data_type)</code>","text":"<p>Prepare the given data for fine-tuning.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dataset | DatasetDict | Optional[Dataset]</code> <p>The dataset to prepare.</p> required <code>data_type</code> <code>str</code> <p>Either 'train' or 'eval' to specify the type of data.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If data_type is not 'train' or 'eval'.</p>"},{"location":"core/airflow/","title":"Airflow Deployment","text":"<p>AirflowRunner is a utility for managing and orchestrating Airflow DAGs. It is designed to provide a command-line interface (CLI) for creating, describing, showing, deleting, and getting the status of Airflow DAGs.</p> <p>This class uses the Airflow models to interact with DAGs and DockerOperator to run tasks in Docker containers. It is aimed to simplify the deployment and management of Airflow tasks, providing a straightforward way to deploy DAGs with Docker tasks from the command line.</p> CLI Usage <p>genius airflow sub-command</p> Sub-commands <ul> <li>create: Create a new DAG with the given parameters and Docker task.         <code>genius airflow create [options]</code></li> <li>describe: Describe a specific DAG by its ID.           <code>genius airflow describe --dag_id example_dag</code></li> <li>show: Show all available DAGs in the Airflow environment.       <code>genius airflow show</code></li> <li>delete: Delete a specific DAG by its ID.         <code>genius airflow delete --dag_id example_dag</code></li> <li>status: Get the status of a specific DAG by its ID.         <code>genius airflow status --dag_id example_dag --airflow_api_base_url http://localhost:8080/api/v1</code></li> </ul> <p>Each sub-command supports various options to specify the details of the DAG or the Docker task, such as the schedule interval, start date, owner, image, command, and more.</p> Example <p>Creating a new DAG: <pre><code>genius airflow create --dag_directory ~/airflow/dags \\\n--dag_id my_dag \\\n--image python:3.10-slim \\\n--command \"echo Hello World\"\n</code></pre></p> <p>Attributes:</p> Name Type Description <code>dag_directory</code> <code>str</code> <p>Directory where DAGs are stored. This path should be known to Airflow.</p> Methods <ul> <li>create: Method to create a new DAG based on the provided parameters and template.</li> <li>describe: Method to describe a specific DAG by its ID, showing details like tasks and schedule.</li> <li>show: Method to list all available DAGs.</li> <li>delete: Method to remove a specific DAG by its ID from the directory.</li> <li>status: Method to fetch and display the status of a specific DAG using Airflow's REST API.</li> </ul> Note <ul> <li>Ensure that the Airflow environment is properly configured and the specified DAG directory is correct.</li> <li>Make sure that the Airflow REST API base URL is accessible if using the status command.</li> </ul>"},{"location":"core/airflow/#runners.airflow.generate.AirflowRunner.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the AirflowRunner class for managing Airflow DAGs.</p>"},{"location":"core/airflow/#runners.airflow.generate.AirflowRunner.create","title":"<code>create(args)</code>","text":"<p>Create a new DAG with a Docker task using the provided arguments.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>Namespace containing all the arguments needed for creating the DAG.</p> required"},{"location":"core/airflow/#runners.airflow.generate.AirflowRunner.delete","title":"<code>delete(dag_id)</code>","text":"<p>Delete a specific DAG by removing its file from the DAG directory.</p> <p>Parameters:</p> Name Type Description Default <code>dag_id</code> <code>str</code> <p>The ID of the DAG to delete.</p> required"},{"location":"core/airflow/#runners.airflow.generate.AirflowRunner.describe","title":"<code>describe(dag_id)</code>","text":"<p>Describe the details of a specific DAG.</p> <p>Parameters:</p> Name Type Description Default <code>dag_id</code> <code>str</code> <p>The ID of the DAG to describe.</p> required <p>Returns:</p> Type Description <code>None</code> <p>The DAG object if found, None otherwise.</p>"},{"location":"core/airflow/#runners.airflow.generate.AirflowRunner.run","title":"<code>run(args)</code>","text":"<p>Execute the command based on the parsed arguments.</p>"},{"location":"core/airflow/#runners.airflow.generate.AirflowRunner.show","title":"<code>show()</code>","text":"<p>Show all available DAGs by listing their IDs.</p>"},{"location":"core/airflow/#runners.airflow.generate.AirflowRunner.status","title":"<code>status(dag_id, airflow_api_base_url)</code>","text":"<p>Get the status of a specific DAG using Airflow's REST API.</p> <p>Parameters:</p> Name Type Description Default <code>dag_id</code> <code>str</code> <p>The ID of the DAG to get the status of.</p> required <code>airflow_api_base_url</code> <code>str</code> <p>URL of airflow for calling its APIs.</p> required"},{"location":"core/cli_boltctl/","title":"Boltctl","text":"<p>The main bolt controller</p>"},{"location":"core/cli_boltctl/#cli.boltctl.BoltCtl","title":"<code>BoltCtl</code>","text":"<p>Class for managing bolts end-to-end from the command line.</p>"},{"location":"core/cli_boltctl/#cli.boltctl.BoltCtl.__init__","title":"<code>__init__(discovered_bolt)</code>","text":"<p>Initialize BoltCtl with a DiscoveredBolt object.</p> <p>Parameters:</p> Name Type Description Default <code>discovered_bolt</code> <code>DiscoveredBolt</code> <p>DiscoveredBolt object used to create and manage bolts.</p> required"},{"location":"core/cli_boltctl/#cli.boltctl.BoltCtl.create_bolt","title":"<code>create_bolt(input_type, output_type, state_type, id, **kwargs)</code>","text":"<p>Create a bolt of a specific type.</p> <p>Parameters:</p> Name Type Description Default <code>input_type</code> <code>str</code> <p>The type of input (\"batch\" or \"streaming\").</p> required <code>output_type</code> <code>str</code> <p>The type of output (\"batch\" or \"streaming\").</p> required <code>state_type</code> <code>str</code> <p>The type of state manager (\"none\", \"redis\", \"postgres\", or \"dynamodb\").</p> required <code>**kwargs</code> <p>Additional keyword arguments for initializing the bolt. <pre><code>Keyword Arguments:\n    Batch input:\n    - input_folder (str): The input folder argument.\n    - input_s3_bucket (str): The input bucket argument.\n    - input_s3_folder (str): The input S3 folder argument.\n    Batch output:\n    - output_folder (str): The output folder argument.\n    - output_s3_bucket (str): The output bucket argument.\n    - output_s3_folder (str): The output S3 folder argument.\n    Streaming input:\n    - input_kafka_cluster_connection_string (str): The input Kafka servers argument.\n    - input_kafka_topic (str): The input kafka topic argument.\n    - input_kafka_consumer_group_id (str): The Kafka consumer group id.\n    Streaming output:\n    - output_kafka_cluster_connection_string (str): The output Kafka servers argument.\n    - output_kafka_topic (str): The output kafka topic argument.\n    Redis state manager config:\n    - redis_host (str): The Redis host argument.\n    - redis_port (str): The Redis port argument.\n    - redis_db (str): The Redis database argument.\n    Postgres state manager config:\n    - postgres_host (str): The PostgreSQL host argument.\n    - postgres_port (str): The PostgreSQL port argument.\n    - postgres_user (str): The PostgreSQL user argument.\n    - postgres_password (str): The PostgreSQL password argument.\n    - postgres_database (str): The PostgreSQL database argument.\n    - postgres_table (str): The PostgreSQL table argument.\n    DynamoDB state manager config:\n    - dynamodb_table_name (str): The DynamoDB table name argument.\n    - dynamodb_region_name (str): The DynamoDB region name argument.\n</code></pre></p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Bolt</code> <code>Bolt</code> <p>The created bolt.</p>"},{"location":"core/cli_boltctl/#cli.boltctl.BoltCtl.create_parser","title":"<code>create_parser(parser)</code>","text":"<p>Add arguments to the command-line parser for managing the bolt.</p> <p>Parameters:</p> Name Type Description Default <code>parser</code> <code>argparse.ArgumentParser</code> <p>Command-line parser.</p> required"},{"location":"core/cli_boltctl/#cli.boltctl.BoltCtl.deploy_bolt","title":"<code>deploy_bolt(args)</code>","text":"<p>Deploy a spout of a specific type.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments for initializing the spout. <pre><code>Keyword Arguments:\n    Batch input:\n    - input_folder (str): The input folder argument.\n    - input_s3_bucket (str): The input bucket argument.\n    - input_s3_folder (str): The input S3 folder argument.\n    Batch outupt:\n    - output_folder (str): The output folder argument.\n    - output_s3_bucket (str): The output bucket argument.\n    - output_s3_folder (str): The output S3 folder argument.\n    Streaming input:\n    - input_kafka_cluster_connection_string (str): The input Kafka servers argument.\n    - input_kafka_topic (str): The input kafka topic argument.\n    - input_kafka_consumer_group_id (str): The Kafka consumer group id.\n    Streaming output:\n    - output_kafka_cluster_connection_string (str): The output Kafka servers argument.\n    - output_kafka_topic (str): The output kafka topic argument.\n    Redis state manager config:\n    - redis_host (str): The host address for the Redis server.\n    - redis_port (int): The port number for the Redis server.\n    - redis_db (int): The Redis database to be used.\n    Postgres state manager config:\n    - postgres_host (str): The host address for the PostgreSQL server.\n    - postgres_port (int): The port number for the PostgreSQL server.\n    - postgres_user (str): The username for the PostgreSQL server.\n    - postgres_password (str): The password for the PostgreSQL server.\n    - postgres_database (str): The PostgreSQL database to be used.\n    - postgres_table (str): The PostgreSQL table to be used.\n    DynamoDB state manager config:\n    - dynamodb_table_name (str): The name of the DynamoDB table.\n    - dynamodb_region_name (str): The AWS region for DynamoDB.\n    Deployment\n    - k8s_kind (str): Kind opf kubernetes resource to be deployed as, choices are \"deployment\", \"service\", \"job\", \"cron_job\"\n    - k8s_name (str): Name of the Kubernetes resource.\n    - k8s_image (str): Docker image for the Kubernetes resource.\n    - k8s_replicas (int): Number of replicas.\n    - k8s_env_vars (json): Environment variables as a JSON string.\n    - k8s_cpu (str): CPU requirements.\n    - k8s_memory (str): Memory requirements.\n    - k8s_storage (str): Storage requirements.\n    - k8s_gpu (str): GPU requirements.\n    - k8s_kube_config_path (str): Name of the Kubernetes cluster local config.\n    - k8s_api_key (str): GPU requirements.\n    - k8s_api_host (str): GPU requirements.\n    - k8s_verify_ssl (str): GPU requirements.\n    - k8s_ssl_ca_cert (str): GPU requirements.\n    - k8s_cluster_name (str): Name of the Kubernetes cluster.\n    - k8s_context_name (str): Name of the kubeconfig context.\n    - k8s_namespace (str): Kubernetes namespace.\", default=\"default\n    - k8s_labels (json): Labels for Kubernetes resources, as a JSON string.\n    - k8s_annotations (json): Annotations for Kubernetes resources, as a JSON string.\n    - k8s_port (int): Port to run the spout on as a service.\n    - k8s_target_port (int): Port to expose the spout on as a service.\n    - k8s_schedule (str): Schedule to run the spout on as a cron job.\n</code></pre></p> required"},{"location":"core/cli_boltctl/#cli.boltctl.BoltCtl.execute_bolt","title":"<code>execute_bolt(bolt, method_name, *args, **kwargs)</code>","text":"<p>Execute a method of a bolt.</p> <p>Parameters:</p> Name Type Description Default <code>bolt</code> <code>Bolt</code> <p>The bolt to execute.</p> required <code>method_name</code> <code>str</code> <p>The name of the method to execute.</p> required <code>*args</code> <p>Positional arguments to pass to the method.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments to pass to the method.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <p>The result of the method.</p>"},{"location":"core/cli_boltctl/#cli.boltctl.BoltCtl.run","title":"<code>run(args)</code>","text":"<p>Run the command-line interface.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>argparse.Namespace</code> <p>Parsed command-line arguments.</p> required"},{"location":"core/cli_discover/","title":"Discover","text":"<p>Module discovery</p>"},{"location":"core/cli_discover/#cli.discover.Discover","title":"<code>Discover</code>","text":""},{"location":"core/cli_discover/#cli.discover.Discover.__init__","title":"<code>__init__(directory=None)</code>","text":"<p>Initialize the Discover class.</p>"},{"location":"core/cli_discover/#cli.discover.Discover.discover_geniusrise_installed_modules","title":"<code>discover_geniusrise_installed_modules()</code>","text":"<p>Discover installed geniusrise modules from Python path directories.</p>"},{"location":"core/cli_discover/#cli.discover.Discover.find_classes","title":"<code>find_classes(module)</code>","text":"<p>Discover spout/bolt classes in a module.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Any</code> <p>Module to scan for spout/bolt classes.</p> required"},{"location":"core/cli_discover/#cli.discover.Discover.get_geniusignore_patterns","title":"<code>get_geniusignore_patterns(directory)</code>  <code>staticmethod</code>","text":"<p>Read the .geniusignore file and return a list of patterns to ignore.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Directory containing the .geniusignore file.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of patterns to ignore.</p>"},{"location":"core/cli_discover/#cli.discover.Discover.get_init_args","title":"<code>get_init_args(cls)</code>","text":"<p>Extract initialization arguments of a class.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>type</code> <p>Class to extract initialization arguments from.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Initialization arguments.</p>"},{"location":"core/cli_discover/#cli.discover.Discover.import_module","title":"<code>import_module(path)</code>","text":"<p>Import a module given its path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the module.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <p>Imported module.</p>"},{"location":"core/cli_discover/#cli.discover.Discover.scan_directory","title":"<code>scan_directory(directory=None)</code>","text":"<p>Scan for spouts/bolts in installed extensions and user's codebase.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Optional[str]</code> <p>Directory to scan for user-defined spouts/bolts.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Discovered spouts/bolts.</p>"},{"location":"core/cli_dockerctl/","title":"Dockerctl","text":""},{"location":"core/cli_dockerctl/#cli.dockerctl.DockerCtl","title":"<code>DockerCtl</code>","text":"<p>This class manages the creation and uploading of Docker containers.</p> <p>Attributes:</p> Name Type Description <code>base_image</code> <code>str</code> <p>The base image to use for the Docker container.</p> <code>workdir</code> <code>str</code> <p>The working directory in the Docker container.</p> <code>local_dir</code> <code>str</code> <p>The local directory to copy into the Docker container.</p> <code>packages</code> <code>List[str]</code> <p>List of packages to install in the Docker container.</p> <code>os_packages</code> <code>List[str]</code> <p>List of OS packages to install in the Docker container.</p> <code>env_vars</code> <code>Dict[str, str]</code> <p>Environment variables to set in the Docker container.</p> Command-Line Interface <p>genius docker package   [options] <p>Parameters:</p> Name Type Description Default <code>-</code> <code>&lt;image_name&gt;</code> <p>The name of the Docker image to build and upload.</p> required <code>-</code> <code>&lt;repository&gt;</code> <p>The container repository to upload to (e.g., \"ECR\", \"DockerHub\", \"Quay\", \"ACR\", \"GCR\").</p> required Options <ul> <li>--auth: Authentication credentials as a JSON string. Default is an empty JSON object.</li> <li>--base_image: The base image to use for the Docker container. Default is \"nvidia/cuda:12.2.0-runtime-ubuntu20.04\".</li> <li>--workdir: The working directory in the Docker container. Default is \"/app\".</li> <li>--local_dir: The local directory to copy into the Docker container. Default is \".\".</li> <li>--packages: List of Python packages to install in the Docker container. Default is an empty list.</li> <li>--os_packages: List of OS packages to install in the Docker container. Default is an empty list.</li> <li>--env_vars: Environment variables to set in the Docker container. Default is an empty dictionary.</li> </ul> Authentication Details <ul> <li>ECR: <code>{\"aws_region\": \"ap-south-1\", \"aws_secret_access_key\": \"aws_key\", \"aws_access_key_id\": \"aws_secret\"}</code></li> <li>DockerHub: <code>{\"dockerhub_username\": \"username\", \"dockerhub_password\": \"password\"}</code></li> <li>ACR: <code>{\"acr_username\": \"username\", \"acr_password\": \"password\", \"acr_login_server\": \"login_server\"}</code></li> <li>GCR: <code>{\"gcr_key_file_path\": \"/path/to/keyfile.json\", \"gcr_repository\": \"repository\"}</code></li> <li>Quay: <code>{\"quay_username\": \"username\", \"quay_password\": \"password\"}</code></li> </ul>"},{"location":"core/cli_dockerctl/#cli.dockerctl.DockerCtl--examples","title":"Examples","text":""},{"location":"core/cli_dockerctl/#cli.dockerctl.DockerCtl--uploading-to-ecr-amazon-elastic-container-registry","title":"Uploading to ECR (Amazon Elastic Container Registry)","text":"<pre><code>genius docker package geniusrise ecr --auth '{\"aws_region\": \"ap-south-1\"}'\n</code></pre>"},{"location":"core/cli_dockerctl/#cli.dockerctl.DockerCtl--uploading-to-dockerhub","title":"Uploading to DockerHub","text":"<pre><code>genius docker package geniusrise dockerhub --auth '{\"dockerhub_username\": \"username\", \"dockerhub_password\": \"password\"}'\n</code></pre> <p>This is how we upload to dockerhub:</p> <pre><code>export DOCKERHUB_USERNAME=\nexport DOCKERHUB_PASSWORD=\ngenius docker package geniusrise dockerhub \\\n--packages geniusrise-listeners geniusrise-databases geniusrise-huggingface geniusrise-openai \\\n--os_packages libmysqlclient-dev libldap2-dev libsasl2-dev libssl-dev\n</code></pre> <pre><code>genius docker package geniusrise-core dockerhub\n</code></pre>"},{"location":"core/cli_dockerctl/#cli.dockerctl.DockerCtl--uploading-to-acr-azure-container-registry","title":"Uploading to ACR (Azure Container Registry)","text":"<pre><code>genius docker package geniusrise acr --auth '{\"acr_username\": \"username\", \"acr_password\": \"password\", \"acr_login_server\": \"login_server\"}'\n</code></pre>"},{"location":"core/cli_dockerctl/#cli.dockerctl.DockerCtl--uploading-to-gcr-google-container-registry","title":"Uploading to GCR (Google Container Registry)","text":"<pre><code>genius docker package geniusrise gcr --auth '{\"gcr_key_file_path\": \"/path/to/keyfile.json\", \"gcr_repository\": \"repository\"}'\n</code></pre>"},{"location":"core/cli_dockerctl/#cli.dockerctl.DockerCtl.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the DockerContainerManager with logging.</p>"},{"location":"core/cli_dockerctl/#cli.dockerctl.DockerCtl.build_image","title":"<code>build_image(image_name, dockerfile_path)</code>","text":"<p>Build a Docker image based on the provided Dockerfile.</p> <p>Parameters:</p> Name Type Description Default <code>image_name</code> <code>str</code> <p>The name to give to the built Docker image.</p> required <code>dockerfile_path</code> <code>str</code> <p>The path to the Dockerfile to use for building the image.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the build was successful, False otherwise.</p>"},{"location":"core/cli_dockerctl/#cli.dockerctl.DockerCtl.create_dockerfile","title":"<code>create_dockerfile()</code>","text":"<p>Create a Dockerfile based on the class attributes.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The path to the created Dockerfile.</p>"},{"location":"core/cli_dockerctl/#cli.dockerctl.DockerCtl.create_parser","title":"<code>create_parser(parser)</code>","text":"<p>Add arguments to the command-line parser for managing Docker containers.</p> <p>Parameters:</p> Name Type Description Default <code>parser</code> <code>argparse.ArgumentParser</code> <p>Command-line parser.</p> required <p>Returns:</p> Type Description <code>argparse.ArgumentParser</code> <p>argparse.ArgumentParser: The updated parser.</p>"},{"location":"core/cli_dockerctl/#cli.dockerctl.DockerCtl.run","title":"<code>run(args)</code>","text":"<p>Run the command-line interface.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>argparse.Namespace</code> <p>Parsed command-line arguments.</p> required"},{"location":"core/cli_dockerctl/#cli.dockerctl.DockerCtl.upload_to_acr","title":"<code>upload_to_acr(image_name, auth)</code>","text":"<p>Upload the Docker image to Azure Container Registry (ACR).</p> <p>Parameters:</p> Name Type Description Default <code>image_name</code> <code>str</code> <p>The name of the Docker image to upload.</p> required <code>auth</code> <code>dict</code> <p>Authentication credentials for ACR.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the upload was successful, False otherwise.</p>"},{"location":"core/cli_dockerctl/#cli.dockerctl.DockerCtl.upload_to_dockerhub","title":"<code>upload_to_dockerhub(image_name, auth)</code>","text":"<p>Upload the Docker image to DockerHub.</p> <p>Parameters:</p> Name Type Description Default <code>image_name</code> <code>str</code> <p>The name of the Docker image to upload.</p> required <code>auth</code> <code>dict</code> <p>Authentication credentials for DockerHub.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the upload was successful, False otherwise.</p>"},{"location":"core/cli_dockerctl/#cli.dockerctl.DockerCtl.upload_to_ecr","title":"<code>upload_to_ecr(image_name, auth, ecr_repo=None)</code>","text":"<p>Upload the Docker image to Amazon Elastic Container Registry (ECR).</p> <p>Parameters:</p> Name Type Description Default <code>image_name</code> <code>str</code> <p>The name of the Docker image to upload.</p> required <code>auth</code> <code>dict</code> <p>Authentication credentials for ECR.</p> required <code>ecr_repo</code> <code>Optional[str]</code> <p>The ECR repository to upload to. If not provided, it will be generated.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the upload was successful, False otherwise.</p>"},{"location":"core/cli_dockerctl/#cli.dockerctl.DockerCtl.upload_to_gcr","title":"<code>upload_to_gcr(image_name, auth)</code>","text":"<p>Upload the Docker image to Google Container Registry (GCR).</p> <p>Parameters:</p> Name Type Description Default <code>image_name</code> <code>str</code> <p>The name of the Docker image to upload.</p> required <code>auth</code> <code>dict</code> <p>Authentication credentials for GCR.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the upload was successful, False otherwise.</p>"},{"location":"core/cli_dockerctl/#cli.dockerctl.DockerCtl.upload_to_quay","title":"<code>upload_to_quay(image_name, auth)</code>","text":"<p>Upload the Docker image to Quay.io.</p> <p>Parameters:</p> Name Type Description Default <code>image_name</code> <code>str</code> <p>The name of the Docker image to upload.</p> required <code>auth</code> <code>dict</code> <p>Authentication credentials for Quay.io.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the upload was successful, False otherwise.</p>"},{"location":"core/cli_dockerctl/#cli.dockerctl.DockerCtl.upload_to_repository","title":"<code>upload_to_repository(image_name, repository, auth={})</code>","text":"<p>Upload the Docker image to a specified container repository.</p> <p>Parameters:</p> Name Type Description Default <code>image_name</code> <code>str</code> <p>The name of the Docker image to upload.</p> required <code>repository</code> <code>str</code> <p>The container repository to upload to (e.g., \"ECR\", \"DockerHub\", \"Quay\").</p> required <code>auth</code> <code>dict</code> <p>Authentication credentials for the container repository. Defaults to None.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the upload was successful, False otherwise.</p>"},{"location":"core/cli_geniusctl/","title":"Geniusctl","text":"<p>The main command line application</p>"},{"location":"core/cli_geniusctl/#cli.geniusctl.GeniusCtl","title":"<code>GeniusCtl</code>","text":"<p>Main class for managing the geniusrise CLI application.</p>"},{"location":"core/cli_geniusctl/#cli.geniusctl.GeniusCtl.__init__","title":"<code>__init__()</code>","text":"<p>Initialize GeniusCtl.v</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>The directory to scan for spouts and bolts.</p> required"},{"location":"core/cli_geniusctl/#cli.geniusctl.GeniusCtl.cli","title":"<code>cli()</code>","text":"<p>Main function to be called when geniusrise is run from the command line.</p>"},{"location":"core/cli_geniusctl/#cli.geniusctl.GeniusCtl.create_parser","title":"<code>create_parser()</code>","text":"<p>Create a command-line parser with arguments for managing the application.</p> <p>Returns:</p> Type Description <p>argparse.ArgumentParser: Command-line parser.</p>"},{"location":"core/cli_geniusctl/#cli.geniusctl.GeniusCtl.list_spouts_and_bolts","title":"<code>list_spouts_and_bolts(verbose=False)</code>","text":"<p>List all discovered spouts and bolts in a table.</p>"},{"location":"core/cli_geniusctl/#cli.geniusctl.GeniusCtl.run","title":"<code>run(args)</code>","text":"<p>Run the command-line interface.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>argparse.Namespace</code> <p>Parsed command-line arguments.</p> required"},{"location":"core/cli_schema/","title":"YAML schema","text":"<p>YAML schema definition as pydantic</p>"},{"location":"core/cli_schema/#cli.schema.Bolt","title":"<code>Bolt</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This class defines a bolt. A bolt has a name, method, optional arguments, input, output, state, and deployment.</p>"},{"location":"core/cli_schema/#cli.schema.Deploy","title":"<code>Deploy</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This class defines the deployment of the spout or bolt. The deployment can be of type k8s or ecs.</p>"},{"location":"core/cli_schema/#cli.schema.DeployArgs","title":"<code>DeployArgs</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This class defines the arguments for the deployment. Depending on the type of deployment (k8s, ecs), different arguments are required.</p>"},{"location":"core/cli_schema/#cli.schema.ExtraKwargs","title":"<code>ExtraKwargs</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This class is used to handle any extra arguments that are not explicitly defined in the schema.</p>"},{"location":"core/cli_schema/#cli.schema.Geniusfile","title":"<code>Geniusfile</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This class defines the overall structure of the YAML file. It includes a version, spouts, and bolts.</p>"},{"location":"core/cli_schema/#cli.schema.Input","title":"<code>Input</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This class defines the input of the bolt. The input can be of type batch, streaming, spout, or bolt.</p>"},{"location":"core/cli_schema/#cli.schema.InputArgs","title":"<code>InputArgs</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This class defines the arguments for the input. Depending on the type of input (batch, streaming, spout, bolt), different arguments are required.</p>"},{"location":"core/cli_schema/#cli.schema.Output","title":"<code>Output</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This class defines the output of the spout or bolt. The output can be of type batch or streaming.</p>"},{"location":"core/cli_schema/#cli.schema.OutputArgs","title":"<code>OutputArgs</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This class defines the arguments for the output. Depending on the type of output (batch, streaming), different arguments are required.</p>"},{"location":"core/cli_schema/#cli.schema.Spout","title":"<code>Spout</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This class defines a spout. A spout has a name, method, optional arguments, output, state, and deployment.</p>"},{"location":"core/cli_schema/#cli.schema.State","title":"<code>State</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This class defines the state of the spout or bolt. The state can be of type none, redis, postgres, or dynamodb.</p>"},{"location":"core/cli_schema/#cli.schema.StateArgs","title":"<code>StateArgs</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This class defines the arguments for the state. Depending on the type of state (none, redis, postgres, dynamodb), different arguments are required.</p>"},{"location":"core/cli_spoutctl/","title":"Spoutctl","text":"<p>The main spout controller</p>"},{"location":"core/cli_spoutctl/#cli.spoutctl.SpoutCtl","title":"<code>SpoutCtl</code>","text":"<p>Class for managing spouts end-to-end from the command line.</p>"},{"location":"core/cli_spoutctl/#cli.spoutctl.SpoutCtl.__init__","title":"<code>__init__(discovered_spout)</code>","text":"<p>Initialize SpoutCtl with a DiscoveredSpout object.</p> <p>Parameters:</p> Name Type Description Default <code>discovered_spout</code> <code>DiscoveredSpout</code> <p>DiscoveredSpout object used to create and manage spouts.</p> required"},{"location":"core/cli_spoutctl/#cli.spoutctl.SpoutCtl.create_parser","title":"<code>create_parser(parser)</code>","text":"<p>Add arguments to the command-line parser for managing the spout.</p> <p>Parameters:</p> Name Type Description Default <code>parser</code> <code>argparse.ArgumentParser</code> <p>Command-line parser.</p> required"},{"location":"core/cli_spoutctl/#cli.spoutctl.SpoutCtl.create_spout","title":"<code>create_spout(output_type, state_type, id, **kwargs)</code>","text":"<p>Create a spout of a specific type.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>str</code> <p>The type of output (\"batch\" or \"streaming\").</p> required <code>state_type</code> <code>str</code> <p>The type of state manager (\"none\", \"redis\", \"postgres\", or \"dynamodb\").</p> required <code>**kwargs</code> <p>Additional keyword arguments for initializing the spout. <pre><code>Keyword Arguments:\n    Batch output:\n    - output_folder (str): The directory where output files should be stored temporarily.\n    - output_s3_bucket (str): The name of the S3 bucket for output storage.\n    - output_s3_folder (str): The S3 folder for output storage.\n    Streaming output:\n    - output_kafka_topic (str): Kafka output topic for streaming spouts.\n    - output_kafka_cluster_connection_string (str): Kafka connection string for streaming spouts.\n    Stream to Batch output:\n    - output_folder (str): The directory where output files should be stored temporarily.\n    - output_s3_bucket (str): The name of the S3 bucket for output storage.\n    - output_s3_folder (str): The S3 folder for output storage.\n    - buffer_size (int): Number of messages to buffer.\n    Redis state manager config:\n    - redis_host (str): The host address for the Redis server.\n    - redis_port (int): The port number for the Redis server.\n    - redis_db (int): The Redis database to be used.\n    Postgres state manager config:\n    - postgres_host (str): The host address for the PostgreSQL server.\n    - postgres_port (int): The port number for the PostgreSQL server.\n    - postgres_user (str): The username for the PostgreSQL server.\n    - postgres_password (str): The password for the PostgreSQL server.\n    - postgres_database (str): The PostgreSQL database to be used.\n    - postgres_table (str): The PostgreSQL table to be used.\n    DynamoDB state manager config:\n    - dynamodb_table_name (str): The name of the DynamoDB table.\n    - dynamodb_region_name (str): The AWS region for DynamoDB.\n</code></pre></p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Spout</code> <code>Spout</code> <p>The created spout.</p>"},{"location":"core/cli_spoutctl/#cli.spoutctl.SpoutCtl.deploy_spout","title":"<code>deploy_spout(args)</code>","text":"<p>Deploy a spout of a specific type.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments for initializing the spout. <pre><code>Keyword Arguments:\n    Batch output:\n    - output_folder (str): The directory where output files should be stored temporarily.\n    - output_s3_bucket (str): The name of the S3 bucket for output storage.\n    - output_s3_folder (str): The S3 folder for output storage.\n    Streaming output:\n    - output_kafka_topic (str): Kafka output topic for streaming spouts.\n    - output_kafka_cluster_connection_string (str): Kafka connection string for streaming spouts.\n    Stream to Batch output:\n    - output_folder (str): The directory where output files should be stored temporarily.\n    - output_s3_bucket (str): The name of the S3 bucket for output storage.\n    - output_s3_folder (str): The S3 folder for output storage.\n    - buffer_size (int): Number of messages to buffer.\n    Redis state manager config:\n    - redis_host (str): The host address for the Redis server.\n    - redis_port (int): The port number for the Redis server.\n    - redis_db (int): The Redis database to be used.\n    Postgres state manager config:\n    - postgres_host (str): The host address for the PostgreSQL server.\n    - postgres_port (int): The port number for the PostgreSQL server.\n    - postgres_user (str): The username for the PostgreSQL server.\n    - postgres_password (str): The password for the PostgreSQL server.\n    - postgres_database (str): The PostgreSQL database to be used.\n    - postgres_table (str): The PostgreSQL table to be used.\n    DynamoDB state manager config:\n    - dynamodb_table_name (str): The name of the DynamoDB table.\n    - dynamodb_region_name (str): The AWS region for DynamoDB.\n    Deployment\n    - k8s_kind (str): Kind opf kubernetes resource to be deployed as, choices are \"deployment\", \"service\", \"job\", \"cron_job\"\n    - k8s_name (str): Name of the Kubernetes resource.\n    - k8s_image (str): Docker image for the Kubernetes resource.\n    - k8s_replicas (int): Number of replicas.\n    - k8s_env_vars (json): Environment variables as a JSON string.\n    - k8s_cpu (str): CPU requirements.\n    - k8s_memory (str): Memory requirements.\n    - k8s_storage (str): Storage requirements.\n    - k8s_gpu (str): GPU requirements.\n    - k8s_kube_config_path (str): Name of the Kubernetes cluster local config.\n    - k8s_api_key (str): GPU requirements.\n    - k8s_api_host (str): GPU requirements.\n    - k8s_verify_ssl (str): GPU requirements.\n    - k8s_ssl_ca_cert (str): GPU requirements.\n    - k8s_cluster_name (str): Name of the Kubernetes cluster.\n    - k8s_context_name (str): Name of the kubeconfig context.\n    - k8s_namespace (str): Kubernetes namespace.\", default=\"default\n    - k8s_labels (json): Labels for Kubernetes resources, as a JSON string.\n    - k8s_annotations (json): Annotations for Kubernetes resources, as a JSON string.\n    - k8s_port (int): Port to run the spout on as a service.\n    - k8s_target_port (int): Port to expose the spout on as a service.\n    - k8s_schedule (str): Schedule to run the spout on as a cron job.\n</code></pre></p> required"},{"location":"core/cli_spoutctl/#cli.spoutctl.SpoutCtl.execute_spout","title":"<code>execute_spout(spout, method_name, *args, **kwargs)</code>","text":"<p>Execute a method of a spout.</p> <p>Parameters:</p> Name Type Description Default <code>spout</code> <code>Spout</code> <p>The spout to execute.</p> required <code>method_name</code> <code>str</code> <p>The name of the method to execute.</p> required <code>*args</code> <p>Positional arguments to pass to the method.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments to pass to the method.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <p>The result of the method.</p>"},{"location":"core/cli_spoutctl/#cli.spoutctl.SpoutCtl.run","title":"<code>run(args)</code>","text":"<p>Run the command-line interface.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>argparse.Namespace</code> <p>Parsed command-line arguments.</p> required"},{"location":"core/cli_yamlctl/","title":"YamlCtl","text":"<p>Control spouts and bolts defined in a YAML file</p>"},{"location":"core/cli_yamlctl/#cli.yamlctl.YamlCtl","title":"<code>YamlCtl</code>","text":"<p>Command-line interface for managing spouts and bolts based on a YAML configuration.</p> <p>The YamlCtl class provides methods to run specific or all spouts and bolts defined in a YAML file. The YAML file's structure is defined by the Geniusfile schema.</p> <p>Example YAML structures:</p> <pre><code>version: 1\nspouts:\nhttp_listener:\nname: WebhookListener\nmethod: listen\nargs:\nport: 8081\nstate:\ntype: redis\nargs:\nredis_host: \"127.0.0.1\"\nredis_port: 6379\nredis_db: 0\noutput:\ntype: batch\nargs:\nbucket: geniusrise-test\nfolder: train\ndeploy:\ntype: k8s\nargs:\nkind: deployment\nname: webhook-listener\ncontext_name: arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\nnamespace: geniusrise\nimage: geniusrise/geniusrise\nkube_config_path: ~/.kube/config\nbolts:\ntext_classifier:\nname: TextClassifier\nmethod: classify\nargs:\nmodel_name: bert-base-uncased\nstate:\ntype: none\ninput:\ntype: batch\nargs:\nbucket: geniusrise-test\nfolder: train\noutput:\ntype: batch\nargs:\nbucket: geniusrise-test\nfolder: model\ndeploy:\ntype: k8s\nargs:\nkind: deployment\nname: text-classifier\ncontext_name: arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\nnamespace: geniusrise\nimage: geniusrise/geniusrise\nkube_config_path: ~/.kube/config\n</code></pre> <pre><code>version: 1\nspouts:\ntwitter_stream:\nname: TwitterStream\nmethod: stream\nargs:\napi_key: \"your_twitter_api_key\"\nhashtags: [\"#AI\", \"#ML\"]\nstate:\ntype: postgres\nargs:\npostgres_host: \"127.0.0.1\"\npostgres_port: 5432\npostgres_user: \"postgres\"\npostgres_password: \"postgres\"\npostgres_database: \"geniusrise\"\npostgres_table: \"twitter_data\"\noutput:\ntype: streaming\nargs:\noutput_topic: twitter_topic\nkafka_servers: \"localhost:9092\"\ndeploy:\ntype: k8s\nargs:\nkind: deployment\nname: twitter-stream\ncontext_name: arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\nnamespace: geniusrise\nimage: geniusrise/geniusrise\nkube_config_path: ~/.kube/config\nbolts:\nsentiment_analyzer:\nname: SentimentAnalyzer\nmethod: analyze\nargs:\nmodel_name: \"sentiment-model\"\nstate:\ntype: dynamodb\nargs:\ndynamodb_table_name: \"SentimentAnalysis\"\ndynamodb_region_name: \"us-east-1\"\ninput:\ntype: streaming\nargs:\ninput_topic: twitter_topic\nkafka_servers: \"localhost:9092\"\ngroup_id: \"sentiment-group\"\noutput:\ntype: batch\nargs:\nbucket: geniusrise-test\nfolder: sentiment_results\ndeploy:\ntype: k8s\nargs:\nkind: deployment\nname: sentiment-analyzer\ncontext_name: arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\nnamespace: geniusrise\nimage: geniusrise/geniusrise\nkube_config_path: ~/.kube/config\n</code></pre> <p>Attributes:</p> Name Type Description <code>geniusfile</code> <code>Geniusfile</code> <p>Parsed YAML configuration.</p> <code>spout_ctls</code> <code>Dict[str, SpoutCtl]</code> <p>Dictionary of SpoutCtl instances.</p> <code>bolt_ctls</code> <code>Dict[str, BoltCtl]</code> <p>Dictionary of BoltCtl instances.</p>"},{"location":"core/cli_yamlctl/#cli.yamlctl.YamlCtl.__init__","title":"<code>__init__(spout_ctls, bolt_ctls)</code>","text":"<p>Initialize YamlCtl with the path to the YAML file and control instances for spouts and bolts.</p> <p>Parameters:</p> Name Type Description Default <code>spout_ctls</code> <code>Dict[str, SpoutCtl]</code> <p>Dictionary of SpoutCtl instances.</p> required <code>bolt_ctls</code> <code>Dict[str, BoltCtl]</code> <p>Dictionary of BoltCtl instances.</p> required"},{"location":"core/cli_yamlctl/#cli.yamlctl.YamlCtl.create_parser","title":"<code>create_parser(parser)</code>","text":"<p>Create and return the command-line parser for managing spouts and bolts.</p>"},{"location":"core/cli_yamlctl/#cli.yamlctl.YamlCtl.deploy_bolt","title":"<code>deploy_bolt(bolt_name)</code>","text":"<p>Deploy a specific bolt based on its name.</p> <p>Parameters:</p> Name Type Description Default <code>bolt_name</code> <code>str</code> <p>Name of the bolt to run.</p> required"},{"location":"core/cli_yamlctl/#cli.yamlctl.YamlCtl.deploy_bolts","title":"<code>deploy_bolts()</code>","text":"<p>Deploy all bolts defined in the YAML configuration.</p>"},{"location":"core/cli_yamlctl/#cli.yamlctl.YamlCtl.deploy_spout","title":"<code>deploy_spout(spout_name)</code>","text":"<p>Deploy a specific spout based on its name.</p> <p>Parameters:</p> Name Type Description Default <code>spout_name</code> <code>str</code> <p>Name of the spout to deploy.</p> required"},{"location":"core/cli_yamlctl/#cli.yamlctl.YamlCtl.deploy_spouts","title":"<code>deploy_spouts()</code>","text":"<p>Deploy all spouts defined in the YAML configuration.</p>"},{"location":"core/cli_yamlctl/#cli.yamlctl.YamlCtl.resolve_reference","title":"<code>resolve_reference(input_type, ref_name)</code>","text":"<p>Resolve the reference of a bolt's input based on the input type (spout or bolt).</p> <p>Parameters:</p> Name Type Description Default <code>input_type</code> <code>str</code> <p>Type of the input (\"spout\" or \"bolt\").</p> required <code>ref_name</code> <code>str</code> <p>Name of the spout or bolt to refer to.</p> required <p>Returns:</p> Name Type Description <code>Output</code> <p>The output data of the referred spout or bolt.</p>"},{"location":"core/cli_yamlctl/#cli.yamlctl.YamlCtl.run","title":"<code>run(args)</code>","text":"<p>Run the command-line interface for managing spouts and bolts based on provided arguments. Please note that there is no ordering of the spouts and bolts in the YAML configuration. Each spout and bolt is an independent entity even when connected together.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>argparse.Namespace</code> <p>Parsed command-line arguments.</p> required"},{"location":"core/cli_yamlctl/#cli.yamlctl.YamlCtl.run_bolt","title":"<code>run_bolt(bolt_name)</code>","text":"<p>Run a specific bolt based on its name.</p> <p>Parameters:</p> Name Type Description Default <code>bolt_name</code> <code>str</code> <p>Name of the bolt to run.</p> required"},{"location":"core/cli_yamlctl/#cli.yamlctl.YamlCtl.run_bolts","title":"<code>run_bolts(executor)</code>","text":"<p>Run all bolts defined in the YAML configuration.</p>"},{"location":"core/cli_yamlctl/#cli.yamlctl.YamlCtl.run_spout","title":"<code>run_spout(spout_name)</code>","text":"<p>Run a specific spout based on its name.</p> <p>Parameters:</p> Name Type Description Default <code>spout_name</code> <code>str</code> <p>Name of the spout to run.</p> required"},{"location":"core/cli_yamlctl/#cli.yamlctl.YamlCtl.run_spouts","title":"<code>run_spouts(executor)</code>","text":"<p>Run all spouts defined in the YAML configuration.</p>"},{"location":"core/core_bolt/","title":"Bolt","text":"<p>Core Bolt class</p>"},{"location":"core/core_bolt/#core.bolt.Bolt","title":"<code>Bolt</code>","text":"<p>             Bases: <code>Task</code></p> <p>Base class for all bolts.</p> <p>A bolt is a component that consumes streams of data, processes them, and possibly emits new data streams.</p>"},{"location":"core/core_bolt/#core.bolt.Bolt.__call__","title":"<code>__call__(method_name, *args, **kwargs)</code>","text":"<p>Execute a method locally and manage the state.</p> <p>Parameters:</p> Name Type Description Default <code>method_name</code> <code>str</code> <p>The name of the method to execute.</p> required <code>*args</code> <p>Positional arguments to pass to the method.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments to pass to the method. Keyword Arguments:     - Additional keyword arguments specific to the method.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The result of the method.</p>"},{"location":"core/core_bolt/#core.bolt.Bolt.__init__","title":"<code>__init__(input, output, state, id=None, **kwargs)</code>","text":"<p>The <code>Bolt</code> class is a base class for all bolts in the given context. It inherits from the <code>Task</code> class and provides methods for executing tasks both locally and remotely, as well as managing their state, with state management options including in-memory, Redis, PostgreSQL, and DynamoDB, and input and output data for  batch, streaming, stream-to-batch, and batch-to-streaming.</p> <p>The <code>Bolt</code> class uses the <code>Input</code>, <code>Output</code> and <code>State</code> classes, which are abstract base classes for managing input data, output data and states, respectively. The <code>Input</code> and <code>Output</code> classes each have two subclasses: <code>StreamingInput</code>, <code>BatchInput</code>, <code>StreamingOutput</code> and <code>BatchOutput</code>, which manage streaming and batch input and output data, respectively. The <code>State</code> class is used to get and set state, and it has several subclasses for different types of state managers.</p> <p>The <code>Bolt</code> class also uses the <code>ECSManager</code> and <code>K8sManager</code> classes in the <code>execute_remote</code> method, which are used to manage tasks on Amazon ECS and Kubernetes, respectively.</p> Usage <ul> <li>Create an instance of the Bolt class by providing an Input object, an Output object and a State object.</li> <li>The Input object specifies the input data for the bolt.</li> <li>The Output object specifies the output data for the bolt.</li> <li>The State object handles the management of the bolt's state.</li> </ul> Example <p>input = Input(...) output = Output(...) state = State(...) bolt = Bolt(input, output, state)</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Input</code> <p>The input data.</p> required <code>output</code> <code>Output</code> <p>The output data.</p> required <code>state</code> <code>State</code> <p>The state manager.</p> required"},{"location":"core/core_bolt/#core.bolt.Bolt.create","title":"<code>create(klass, input_type, output_type, state_type, id=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a bolt of a specific type.</p> <p>This static method is used to create a bolt of a specific type. It takes in an input type, an output type, a state type, and additional keyword arguments for initializing the bolt.</p> <p>The method creates the input, output, and state manager based on the provided types, and then creates and returns a bolt using these configurations.</p> <p>Parameters:</p> Name Type Description Default <code>klass</code> <code>type</code> <p>The Bolt class to create.</p> required <code>input_type</code> <code>str</code> <p>The type of input (\"batch\" or \"streaming\").</p> required <code>output_type</code> <code>str</code> <p>The type of output (\"batch\" or \"streaming\").</p> required <code>state_type</code> <code>str</code> <p>The type of state manager (\"none\", \"redis\", \"postgres\", or \"dynamodb\").</p> required <code>**kwargs</code> <p>Additional keyword arguments for initializing the bolt. <pre><code>Keyword Arguments:\n    Batch input:\n    - input_folder (str): The input folder argument.\n    - input_s3_bucket (str): The input bucket argument.\n    - input_s3_folder (str): The input S3 folder argument.\n    Batch output config:\n    - output_folder (str): The output folder argument.\n    - output_s3_bucket (str): The output bucket argument.\n    - output_s3_folder (str): The output S3 folder argument.\n    Streaming input:\n    - input_kafka_cluster_connection_string (str): The input Kafka servers argument.\n    - input_kafka_topic (str): The input kafka topic argument.\n    - input_kafka_consumer_group_id (str): The Kafka consumer group id.\n    Streaming output:\n    - output_kafka_cluster_connection_string (str): The output Kafka servers argument.\n    - output_kafka_topic (str): The output kafka topic argument.\n    Stream-to-Batch input:\n    - buffer_size (int): Number of messages to buffer.\n    - input_kafka_cluster_connection_string (str): The input Kafka servers argument.\n    - input_kafka_topic (str): The input kafka topic argument.\n    - input_kafka_consumer_group_id (str): The Kafka consumer group id.\n    Batch-to-Streaming input:\n    - buffer_size (int): Number of messages to buffer.\n    - input_folder (str): The input folder argument.\n    - input_s3_bucket (str): The input bucket argument.\n    - input_s3_folder (str): The input S3 folder argument.\n    Stream-to-Batch output:\n    - buffer_size (int): Number of messages to buffer.\n    - output_folder (str): The output folder argument.\n    - output_s3_bucket (str): The output bucket argument.\n    - output_s3_folder (str): The output S3 folder argument.\n    Redis state manager config:\n    - redis_host (str): The Redis host argument.\n    - redis_port (str): The Redis port argument.\n    - redis_db (str): The Redis database argument.\n    Postgres state manager config:\n    - postgres_host (str): The PostgreSQL host argument.\n    - postgres_port (str): The PostgreSQL port argument.\n    - postgres_user (str): The PostgreSQL user argument.\n    - postgres_password (str): The PostgreSQL password argument.\n    - postgres_database (str): The PostgreSQL database argument.\n    - postgres_table (str): The PostgreSQL table argument.\n    DynamoDB state manager config:\n    - dynamodb_table_name (str): The DynamoDB table name argument.\n    - dynamodb_region_name (str): The DynamoDB region name argument.\n</code></pre></p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Bolt</code> <code>Bolt</code> <p>The created bolt.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid input type, output type, or state type is provided.</p>"},{"location":"core/core_data_batch_input/","title":"Batch data input","text":"<p>Batch input manager</p>"},{"location":"core/core_data_batch_input/#core.data.batch_input.BatchInput","title":"<code>BatchInput</code>","text":"<p>             Bases: <code>Input</code></p> <p>\ud83d\udcc1 BatchInput: Manages batch input data.</p> <p>Attributes:</p> Name Type Description <code>input_folder</code> <code>str</code> <p>Folder to read input files.</p> <code>bucket</code> <code>str</code> <p>S3 bucket name.</p> <code>s3_folder</code> <code>str</code> <p>Folder within the S3 bucket.</p> <code>partition_scheme</code> <code>Optional[str]</code> <p>Partitioning scheme for S3, e.g., \"year/month/day\".</p> <p>Raises:</p> Type Description <code>FileNotExistError</code> <p>If the file does not exist.</p> <p>Parameters:</p> Name Type Description Default <code>input_folder</code> <code>str</code> <p>Folder to read input files from.</p> required <code>bucket</code> <code>str</code> <p>S3 bucket name.</p> required <code>s3_folder</code> <code>str</code> <p>Folder within the S3 bucket.</p> required <code>partition_scheme</code> <code>Optional[str]</code> <p>Partitioning scheme for S3, e.g., \"year/month/day\".</p> <code>None</code> Usage"},{"location":"core/core_data_batch_input/#core.data.batch_input.BatchInput--initialize-batchinput","title":"Initialize BatchInput","text":"<pre><code>input = BatchInput(\"/path/to/input\", \"my_bucket\", \"s3/folder\")\n</code></pre>"},{"location":"core/core_data_batch_input/#core.data.batch_input.BatchInput--get-the-input-folder","title":"Get the input folder","text":"<pre><code>folder = input.get()\n</code></pre>"},{"location":"core/core_data_batch_input/#core.data.batch_input.BatchInput--save-a-spark-dataframe-to-the-input-folder","title":"Save a Spark DataFrame to the input folder","text":"<pre><code>input.from_spark(my_dataframe)\n</code></pre>"},{"location":"core/core_data_batch_input/#core.data.batch_input.BatchInput--compose-multiple-batchinput-instances","title":"Compose multiple BatchInput instances","text":"<pre><code>composed = input.compose(input1, input2)\n</code></pre>"},{"location":"core/core_data_batch_input/#core.data.batch_input.BatchInput--copy-files-from-s3-to-the-input-folder","title":"Copy files from S3 to the input folder","text":"<pre><code>input.from_s3()\n</code></pre>"},{"location":"core/core_data_batch_input/#core.data.batch_input.BatchInput--collect-metrics","title":"Collect metrics","text":"<pre><code>metrics = input.collect_metrics()\n</code></pre>"},{"location":"core/core_data_batch_input/#core.data.batch_input.BatchInput.__init__","title":"<code>__init__(input_folder, bucket, s3_folder, partition_scheme=None)</code>","text":"<p>Initialize a new BatchInput instance.</p>"},{"location":"core/core_data_batch_input/#core.data.batch_input.BatchInput.collect_metrics","title":"<code>collect_metrics()</code>","text":"<p>Collect and return metrics, then clear them for future collection.</p> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: Dictionary containing metrics.</p>"},{"location":"core/core_data_batch_input/#core.data.batch_input.BatchInput.compose","title":"<code>compose(*inputs)</code>","text":"<p>Compose multiple BatchInput instances by merging their input folders.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Input</code> <p>Variable number of BatchInput instances.</p> <code>()</code> <p>Returns:</p> Type Description <code>Union[bool, str]</code> <p>Union[bool, str]: True if successful, error message otherwise.</p>"},{"location":"core/core_data_batch_input/#core.data.batch_input.BatchInput.from_kafka","title":"<code>from_kafka(input_topic, kafka_cluster_connection_string, nr_messages=1000, group_id='geniusrise', partition_scheme=None)</code>","text":"<p>Consume messages from a Kafka topic and save them as JSON files in the input folder. Stops consuming after reaching the latest message or the specified number of messages.</p> <p>Parameters:</p> Name Type Description Default <code>input_topic</code> <code>str</code> <p>Kafka topic to consume data from.</p> required <code>kafka_cluster_connection_string</code> <code>str</code> <p>Connection string for the Kafka cluster.</p> required <code>nr_messages</code> <code>int</code> <p>Number of messages to consume. Defaults to 1000.</p> <code>1000</code> <code>group_id</code> <code>str</code> <p>Kafka consumer group ID. Defaults to \"geniusrise\".</p> <code>'geniusrise'</code> <code>partition_scheme</code> <code>Optional[str]</code> <p>Optional partitioning scheme for Kafka, e.g., \"year/month/day\".</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The path to the folder where the consumed messages are saved as JSON files.</p> <p>Raises:</p> Type Description <code>KafkaConnectionError</code> <p>If unable to connect to Kafka.</p> <code>Exception</code> <p>If any other error occurs during processing.</p>"},{"location":"core/core_data_batch_input/#core.data.batch_input.BatchInput.from_s3","title":"<code>from_s3(bucket=None, s3_folder=None)</code>","text":"<p>Copy contents from a given S3 bucket and location to the input folder.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the input folder is not specified.</p>"},{"location":"core/core_data_batch_input/#core.data.batch_input.BatchInput.from_spark","title":"<code>from_spark(df)</code>","text":"<p>Save the contents of a Spark DataFrame to the input folder with optional partitioning.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Spark DataFrame to save.</p> required <p>Raises:</p> Type Description <code>FileNotExistError</code> <p>If the input folder does not exist.</p>"},{"location":"core/core_data_batch_input/#core.data.batch_input.BatchInput.get","title":"<code>get()</code>","text":"<p>Get the input folder path.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The path to the input folder.</p>"},{"location":"core/core_data_batch_input/#core.data.batch_input.FileNotExistError","title":"<code>FileNotExistError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>\u274c Custom exception for file not existing.</p>"},{"location":"core/core_data_batch_input/#core.data.batch_input.KafkaConnectionError","title":"<code>KafkaConnectionError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>\u274c Custom exception for kafka connection problems.</p>"},{"location":"core/core_data_batch_output/","title":"Batch data output","text":"<p>Batch output manager</p>"},{"location":"core/core_data_batch_output/#core.data.batch_output.BatchOutput","title":"<code>BatchOutput</code>","text":"<p>             Bases: <code>Output</code></p> <p>\ud83d\udcc1 BatchOutput: Manages batch output data.</p> <p>Attributes:</p> Name Type Description <code>output_folder</code> <code>str</code> <p>Folder to save output files.</p> <code>bucket</code> <code>str</code> <p>S3 bucket name.</p> <code>s3_folder</code> <code>str</code> <p>Folder within the S3 bucket.</p> <code>partition_scheme</code> <code>Optional[str]</code> <p>Partitioning scheme for S3, e.g., \"year/month/day\".</p> <p>Raises:</p> Type Description <code>FileNotExistError</code> <p>If the output folder does not exist.</p> <p>Parameters:</p> Name Type Description Default <code>output_folder</code> <code>str</code> <p>Folder to save output files.</p> required <code>bucket</code> <code>str</code> <p>S3 bucket name.</p> required <code>s3_folder</code> <code>str</code> <p>Folder within the S3 bucket.</p> required <code>partition_scheme</code> <code>Optional[str]</code> <p>Partitioning scheme for S3, e.g., \"year/month/day\".</p> <code>None</code> Usage <pre><code># Initialize the BatchOutput instance\nconfig = BatchOutput(\"/path/to/output\", \"my_bucket\", \"s3/folder\", partition_scheme=\"%Y/%m/%d\")\n# Save data to a file\nconfig.save({\"key\": \"value\"}, \"example.json\")\n# Compose multiple BatchOutput instances\nresult = config1.compose(config2, config3)\n# Convert output to a Spark DataFrame\nspark_df = config.to_spark(spark_session)\n# Copy files to a remote S3 bucket\nconfig.to_s3()\n# Flush the output to S3\nconfig.flush()\n# Collect metrics\nmetrics = config.collect_metrics()\n</code></pre>"},{"location":"core/core_data_batch_output/#core.data.batch_output.BatchOutput.__init__","title":"<code>__init__(output_folder, bucket, s3_folder, partition_scheme=None)</code>","text":"<p>Initialize a new batch output data.</p> <p>Parameters:</p> Name Type Description Default <code>output_folder</code> <code>str</code> <p>Folder to save output files.</p> required <code>bucket</code> <code>str</code> <p>S3 bucket name.</p> required <code>s3_folder</code> <code>str</code> <p>Folder within the S3 bucket.</p> required"},{"location":"core/core_data_batch_output/#core.data.batch_output.BatchOutput.collect_metrics","title":"<code>collect_metrics()</code>","text":"<p>Collect and return metrics, then clear them for future collection.</p> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: Dictionary containing metrics.</p>"},{"location":"core/core_data_batch_output/#core.data.batch_output.BatchOutput.compose","title":"<code>compose(*outputs)</code>","text":"<p>Compose multiple BatchOutput instances by merging their output folders.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>Output</code> <p>Variable number of BatchOutput instances.</p> <code>()</code> <p>Returns:</p> Type Description <code>Union[bool, str]</code> <p>Union[bool, str]: True if successful, error message otherwise.</p>"},{"location":"core/core_data_batch_output/#core.data.batch_output.BatchOutput.flush","title":"<code>flush()</code>","text":"<p>\ud83d\udd04 Flush the output by copying all files and directories from the output folder to a given S3 bucket and folder.</p>"},{"location":"core/core_data_batch_output/#core.data.batch_output.BatchOutput.save","title":"<code>save(data, filename=None, **kwargs)</code>","text":"<p>\ud83d\udcbe Save data to a file in the output folder.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The data to save.</p> required <code>filename</code> <code>Optional[str]</code> <p>The filename to use when saving the data to a file.</p> <code>None</code>"},{"location":"core/core_data_batch_output/#core.data.batch_output.BatchOutput.to_kafka","title":"<code>to_kafka(output_topic, kafka_cluster_connection_string)</code>","text":"<p>Produce messages to a Kafka topic from the files in the output folder.</p> <p>Parameters:</p> Name Type Description Default <code>output_topic</code> <code>str</code> <p>Kafka topic to produce data to.</p> required <code>kafka_cluster_connection_string</code> <code>str</code> <p>Connection string for the Kafka cluster.</p> required <code>key_serializer</code> <code>Optional[str]</code> <p>Serializer for message keys. Defaults to None.</p> required <p>Raises:</p> Type Description <code>KafkaConnectionError</code> <p>If unable to connect to Kafka.</p> <code>Exception</code> <p>If any other error occurs during processing.</p>"},{"location":"core/core_data_batch_output/#core.data.batch_output.BatchOutput.to_s3","title":"<code>to_s3()</code>","text":"<p>\u2601\ufe0f Recursively copy all files and directories from the output folder to a given S3 bucket and folder.</p>"},{"location":"core/core_data_batch_output/#core.data.batch_output.BatchOutput.to_spark","title":"<code>to_spark(spark)</code>","text":"<p>Get a Spark DataFrame from the output folder.</p> <p>Returns:</p> Type Description <code>pyspark.sql.DataFrame</code> <p>pyspark.sql.DataFrame: A Spark DataFrame where each row corresponds to a file in the output folder.</p> <p>Raises:</p> Type Description <code>FileNotExistError</code> <p>If the output folder does not exist.</p>"},{"location":"core/core_data_batch_output/#core.data.batch_output.FileNotExistError","title":"<code>FileNotExistError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>\u274c Custom exception for file not existing.</p>"},{"location":"core/core_data_batch_output/#core.data.batch_output.KafkaConnectionError","title":"<code>KafkaConnectionError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>\u274c Custom exception for Kafka connection problems.</p>"},{"location":"core/core_data_input/","title":"Data input","text":"<p>Input manager base class</p>"},{"location":"core/core_data_input/#core.data.input.Input","title":"<code>Input</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract class for managing input data.</p> <p>Attributes:</p> Name Type Description <code>log</code> <code>logging.Logger</code> <p>Logger instance.</p>"},{"location":"core/core_data_input/#core.data.input.Input.__add__","title":"<code>__add__(*inputs)</code>","text":"<p>Compose multiple inputs.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Input</code> <p>Variable number of Input instances.</p> <code>()</code> <p>Returns:</p> Type Description <code>Union[bool, str]</code> <p>Union[bool, str]: True if successful, error message otherwise.</p>"},{"location":"core/core_data_input/#core.data.input.Input.collect_metrics","title":"<code>collect_metrics()</code>  <code>abstractmethod</code>","text":"<p>Collect metrics like latency.</p> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: A dictionary containing metrics.</p>"},{"location":"core/core_data_input/#core.data.input.Input.compose","title":"<code>compose(*inputs)</code>  <code>abstractmethod</code>","text":"<p>Compose multiple inputs.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Input</code> <p>Variable number of Input instances.</p> <code>()</code> <p>Returns:</p> Type Description <code>Union[bool, str]</code> <p>Union[bool, str]: True if successful, error message otherwise.</p>"},{"location":"core/core_data_input/#core.data.input.Input.get","title":"<code>get()</code>  <code>abstractmethod</code>","text":"<p>Abstract method to get data from the input source.</p> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The data from the input source.</p>"},{"location":"core/core_data_input/#core.data.input.Input.retryable_get","title":"<code>retryable_get()</code>","text":"<p>Retryable get method.</p> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The data from the input source.</p>"},{"location":"core/core_data_output/","title":"Data output","text":"<p>Output manager base class</p>"},{"location":"core/core_data_output/#core.data.output.Output","title":"<code>Output</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for managing output data.</p>"},{"location":"core/core_data_output/#core.data.output.Output.flush","title":"<code>flush()</code>  <code>abstractmethod</code>","text":"<p>Flush the output. This method should be implemented by subclasses.</p>"},{"location":"core/core_data_output/#core.data.output.Output.save","title":"<code>save(data, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Save data to a file or ingest it into a Kafka topic.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The data to save or ingest.</p> required <code>filename</code> <code>str</code> <p>The filename to use when saving the data to a file.</p> required"},{"location":"core/core_data_streaming_input/","title":"Streaming data input","text":"<p>Streaming input manager</p>"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.KafkaConnectionError","title":"<code>KafkaConnectionError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>\u274c Custom exception for kafka connection problems.</p>"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.StreamingInput","title":"<code>StreamingInput</code>","text":"<p>             Bases: <code>Input</code></p> <p>\ud83d\udce1 StreamingInput: Manages streaming input data from Kafka and other streaming sources.</p> <p>Attributes:</p> Name Type Description <code>input_topic</code> <code>str</code> <p>Kafka topic to consume data from.</p> <code>kafka_cluster_connection_string</code> <code>str</code> <p>Connection string for the Kafka cluster.</p> <code>group_id</code> <code>str</code> <p>Kafka consumer group ID.</p> <code>consumer</code> <code>KafkaConsumer</code> <p>Kafka consumer instance.</p> Usage <p>input = StreamingInput(\"my_topic\", \"localhost:9094\") for message in input.get():     print(message.value)</p> <p>Parameters:</p> Name Type Description Default <code>input_topic</code> <code>str</code> <p>Kafka topic to consume data from.</p> required <code>kafka_cluster_connection_string</code> <code>str</code> <p>Connection string for the Kafka cluster.</p> required <code>group_id</code> <code>str</code> <p>Kafka consumer group ID. Defaults to \"geniusrise\".</p> <code>'geniusrise'</code> <code>**kwargs</code> <p>Additional keyword arguments for KafkaConsumer.</p> <code>{}</code> <p>Raises:</p> Type Description <code>KafkaConnectionError</code> <p>If unable to connect to Kafka.</p> Usage"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.StreamingInput--using-get-method-to-consume-from-kafka","title":"Using <code>get</code> method to consume from Kafka","text":"<pre><code>input = StreamingInput(\"my_topic\", \"localhost:9094\")\nconsumer = input.get()\nfor message in consumer:\nprint(message.value)\n</code></pre>"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.StreamingInput--using-from_streamz-method-to-process-streamz-dataframe","title":"Using <code>from_streamz</code> method to process streamz DataFrame","text":"<pre><code>input = StreamingInput(\"my_topic\", \"localhost:9094\")\nstreamz_df = ...  # Assume this is a streamz DataFrame\nfor row in input.from_streamz(streamz_df):\nprint(row)\n</code></pre>"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.StreamingInput--using-from_spark-method-to-process-spark-dataframe","title":"Using <code>from_spark</code> method to process Spark DataFrame","text":"<pre><code>input = StreamingInput(\"my_topic\", \"localhost:9094\")\nspark_df = ...  # Assume this is a Spark DataFrame\nmap_func = lambda row: {\"key\": row.key, \"value\": row.value}\nquery_or_rdd = input.from_spark(spark_df, map_func)\n</code></pre>"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.StreamingInput--using-compose-method-to-merge-multiple-streaminginput-instances","title":"Using <code>compose</code> method to merge multiple StreamingInput instances","text":"<pre><code>input1 = StreamingInput(\"topic1\", \"localhost:9094\")\ninput2 = StreamingInput(\"topic2\", \"localhost:9094\")\nresult = input1.compose(input2)\n</code></pre>"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.StreamingInput--using-close-method-to-close-the-kafka-consumer","title":"Using <code>close</code> method to close the Kafka consumer","text":"<pre><code>input = StreamingInput(\"my_topic\", \"localhost:9094\")\ninput.close()\n</code></pre>"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.StreamingInput--using-seek-method-to-seek-to-a-specific-offset","title":"Using <code>seek</code> method to seek to a specific offset","text":"<pre><code>input = StreamingInput(\"my_topic\", \"localhost:9094\")\ninput.seek(42)\n</code></pre>"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.StreamingInput--using-commit-method-to-manually-commit-offsets","title":"Using <code>commit</code> method to manually commit offsets","text":"<pre><code>input = StreamingInput(\"my_topic\", \"localhost:9094\")\ninput.commit()\n</code></pre>"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.StreamingInput--using-collect_metrics-method-to-collect-kafka-metrics","title":"Using <code>collect_metrics</code> method to collect Kafka metrics","text":"<pre><code>input = StreamingInput(\"my_topic\", \"localhost:9094\")\nmetrics = input.collect_metrics()\nprint(metrics)\n</code></pre>"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.StreamingInput.__init__","title":"<code>__init__(input_topic, kafka_cluster_connection_string, group_id='geniusrise', **kwargs)</code>","text":"<p>\ud83d\udca5 Initialize a new streaming input data.</p> <p>Parameters:</p> Name Type Description Default <code>input_topic</code> <code>str</code> <p>Kafka topic to consume data.</p> required <code>kafka_cluster_connection_string</code> <code>str</code> <p>Kafka cluster connection string.</p> required <code>group_id</code> <code>str</code> <p>Kafka consumer group id. Defaults to \"geniusrise\".</p> <code>'geniusrise'</code>"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.StreamingInput.close","title":"<code>close()</code>","text":"<p>\ud83d\udeaa Close the Kafka consumer.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If an error occurs while closing the consumer.</p>"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.StreamingInput.collect_metrics","title":"<code>collect_metrics()</code>","text":"<p>\ud83d\udcca Collect metrics related to the Kafka consumer.</p> <p>Returns:</p> Type Description <code>Dict[str, Union[int, float]]</code> <p>Dict[str, Union[int, float]]: A dictionary containing metrics like latency.</p>"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.StreamingInput.commit","title":"<code>commit()</code>","text":"<p>\u2705 Manually commit offsets.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If an error occurs while committing offsets.</p>"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.StreamingInput.compose","title":"<code>compose(*inputs)</code>","text":"<p>Compose multiple StreamingInput instances by merging their iterators.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>StreamingInput</code> <p>Variable number of StreamingInput instances.</p> <code>()</code> <p>Returns:</p> Type Description <code>Union[bool, str]</code> <p>Union[bool, str]: True if successful, error message otherwise.</p> Caveat <p>On merging different topics, other operations such as</p>"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.StreamingInput.from_spark","title":"<code>from_spark(spark_df, map_func)</code>","text":"<p>Process a Spark DataFrame as a stream, similar to Kafka processing.</p> <p>Parameters:</p> Name Type Description Default <code>spark_df</code> <code>DataFrame</code> <p>The Spark DataFrame to process.</p> required <code>map_func</code> <code>Callable[[Row], Any]</code> <p>Function to map each row of the DataFrame.</p> required <p>Returns:</p> Type Description <code>Union[StreamingQuery, RDD[Any]]</code> <p>Union[StreamingQuery, RDD[Any]]: Returns a StreamingQuery for streaming DataFrames, and an RDD for batch DataFrames.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If an error occurs during processing.</p>"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.StreamingInput.from_streamz","title":"<code>from_streamz(streamz_df, sentinel=None, timeout=5)</code>","text":"<p>Process a streamz DataFrame as a stream, similar to Kafka processing.</p> <p>Parameters:</p> Name Type Description Default <code>streamz_df</code> <code>ZDataFrame</code> <p>The streamz DataFrame to process.</p> required <code>sentinel</code> <code>Any</code> <p>The value that, when received, will stop the generator.</p> <code>None</code> <code>timeout</code> <code>int</code> <p>The time to wait for an item from the queue before raising an exception.</p> <code>5</code> <p>Yields:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Yields each row as a dictionary.</p>"},{"location":"core/core_data_streaming_input/#core.data.streaming_input.StreamingInput.get","title":"<code>get()</code>","text":"<p>\ud83d\udce5 Get data from the input topic.</p> <p>Returns:</p> Name Type Description <code>KafkaConsumer</code> <code>KafkaConsumer</code> <p>The Kafka consumer.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If no input source or consumer is specified.</p>"},{"location":"core/core_data_streaming_output/","title":"Streaming data output","text":"<p>Streaming output manager</p>"},{"location":"core/core_data_streaming_output/#core.data.streaming_output.StreamingOutput","title":"<code>StreamingOutput</code>","text":"<p>             Bases: <code>Output</code></p> <p>\ud83d\udce1 StreamingOutput: Manages streaming output data.</p> <p>Attributes:</p> Name Type Description <code>output_topic</code> <code>str</code> <p>Kafka topic to ingest data.</p> <code>producer</code> <code>KafkaProducer</code> <p>Kafka producer for ingesting data.</p> <p>Usage: <pre><code>config = StreamingOutput(\"my_topic\", \"localhost:9094\")\nconfig.save({\"key\": \"value\"}, \"ignored_filename\")\nconfig.flush()\n</code></pre></p> <p>Note: - Ensure the Kafka cluster is running and accessible.</p>"},{"location":"core/core_data_streaming_output/#core.data.streaming_output.StreamingOutput.__init__","title":"<code>__init__(output_topic, kafka_servers)</code>","text":"<p>Initialize a new streaming output data.</p> <p>Parameters:</p> Name Type Description Default <code>output_topic</code> <code>str</code> <p>Kafka topic to ingest data.</p> required <code>kafka_servers</code> <code>str</code> <p>Kafka bootstrap servers.</p> required"},{"location":"core/core_data_streaming_output/#core.data.streaming_output.StreamingOutput.close","title":"<code>close()</code>","text":"<p>\ud83d\udeaa Close the Kafka producer.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If no Kafka producer is available.</p>"},{"location":"core/core_data_streaming_output/#core.data.streaming_output.StreamingOutput.flush","title":"<code>flush()</code>","text":"<p>\ud83d\udd04 Flush the output by flushing the Kafka producer.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If no Kafka producer is available.</p>"},{"location":"core/core_data_streaming_output/#core.data.streaming_output.StreamingOutput.partition_available","title":"<code>partition_available(partition)</code>","text":"<p>\ud83e\uddd0 Check if a partition is available in the Kafka topic.</p> <p>Parameters:</p> Name Type Description Default <code>partition</code> <code>int</code> <p>The partition to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the partition is available, False otherwise.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If no Kafka producer is available.</p>"},{"location":"core/core_data_streaming_output/#core.data.streaming_output.StreamingOutput.save","title":"<code>save(data, **kwargs)</code>","text":"<p>\ud83d\udce4 Ingest data into the Kafka topic.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The data to ingest.</p> required <code>filename</code> <code>str</code> <p>This argument is ignored for streaming outputs.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If no Kafka producer is available or an error occurs.</p>"},{"location":"core/core_data_streaming_output/#core.data.streaming_output.StreamingOutput.save_to_partition","title":"<code>save_to_partition(value, partition)</code>","text":"<p>\ud83c\udfaf Send a message to a specific partition in the Kafka topic.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The value of the message.</p> required <code>partition</code> <code>int</code> <p>The partition to send the message to.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If no Kafka producer is available or an error occurs.</p>"},{"location":"core/core_spout/","title":"Spout","text":"<p>Core Spout class</p>"},{"location":"core/core_spout/#core.spout.Spout","title":"<code>Spout</code>","text":"<p>             Bases: <code>Task</code></p> <p>Base class for all spouts.</p>"},{"location":"core/core_spout/#core.spout.Spout.__call__","title":"<code>__call__(method_name, *args, **kwargs)</code>","text":"<p>Execute a method locally and manage the state.</p> <p>Parameters:</p> Name Type Description Default <code>method_name</code> <code>str</code> <p>The name of the method to execute.</p> required <code>*args</code> <p>Positional arguments to pass to the method.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments to pass to the method. Keyword Arguments:     - Additional keyword arguments specific to the method.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The result of the method.</p>"},{"location":"core/core_spout/#core.spout.Spout.__init__","title":"<code>__init__(output, state, id=None, **kwargs)</code>","text":"<p>The <code>Spout</code> class is a base class for all spouts in the given context. It inherits from the <code>Task</code> class and provides methods for executing tasks both locally and remotely, as well as managing their state, with state management options including in-memory, Redis, PostgreSQL, and DynamoDB, and output data for batch or streaming data.</p> <p>The <code>Spout</code> class uses the <code>Output</code> and <code>State</code> classes, which are abstract base  classes for managing output data and states, respectively. The <code>Output</code> class  has two subclasses: <code>StreamingOutput</code> and <code>BatchOutput</code>, which manage streaming and  batch output data, respectively. The <code>State</code> class is used to get and set state,  and it has several subclasses for different types of state managers.</p> <p>The <code>Spout</code> class also uses the <code>ECSManager</code> and <code>K8sManager</code> classes in the <code>execute_remote</code> method, which are used to manage tasks on Amazon ECS and Kubernetes, respectively.</p> Usage <ul> <li>Create an instance of the Spout class by providing an Output object and a State object.</li> <li>The Output object specifies the output data for the spout.</li> <li>The State object handles the management of the spout's state.</li> </ul> Example <p>output = Output(...) state = State(...) spout = Spout(output, state)</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Output</code> <p>The output data.</p> required <code>state</code> <code>State</code> <p>The state manager.</p> required"},{"location":"core/core_spout/#core.spout.Spout.create","title":"<code>create(klass, output_type, state_type, id=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a spout of a specific type.</p> <p>Parameters:</p> Name Type Description Default <code>klass</code> <code>type</code> <p>The Spout class to create.</p> required <code>output_type</code> <code>str</code> <p>The type of output (\"batch\" or \"streaming\").</p> required <code>state_type</code> <code>str</code> <p>The type of state manager (\"none\", \"redis\", \"postgres\", or \"dynamodb\").</p> required <code>**kwargs</code> <p>Additional keyword arguments for initializing the spout. <pre><code>Keyword Arguments:\n    Batch output:\n    - output_folder (str): The directory where output files should be stored temporarily.\n    - output_s3_bucket (str): The name of the S3 bucket for output storage.\n    - output_s3_folder (str): The S3 folder for output storage.\n    Streaming output:\n    - output_kafka_topic (str): Kafka output topic for streaming spouts.\n    - output_kafka_cluster_connection_string (str): Kafka connection string for streaming spouts.\n    Stream to Batch output:\n    - output_folder (str): The directory where output files should be stored temporarily.\n    - output_s3_bucket (str): The name of the S3 bucket for output storage.\n    - output_s3_folder (str): The S3 folder for output storage.\n    - buffer_size (int): Number of messages to buffer.\n    Redis state manager config:\n    - redis_host (str): The host address for the Redis server.\n    - redis_port (int): The port number for the Redis server.\n    - redis_db (int): The Redis database to be used.\n    Postgres state manager config:\n    - postgres_host (str): The host address for the PostgreSQL server.\n    - postgres_port (int): The port number for the PostgreSQL server.\n    - postgres_user (str): The username for the PostgreSQL server.\n    - postgres_password (str): The password for the PostgreSQL server.\n    - postgres_database (str): The PostgreSQL database to be used.\n    - postgres_table (str): The PostgreSQL table to be used.\n    DynamoDB state manager config:\n    - dynamodb_table_name (str): The name of the DynamoDB table.\n    - dynamodb_region_name (str): The AWS region for DynamoDB\n</code></pre></p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Spout</code> <code>Spout</code> <p>The created spout.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid output type or state type is provided.</p>"},{"location":"core/core_state_base/","title":"State","text":"<p>Base class for task state mnager</p>"},{"location":"core/core_state_base/#core.state.base.State","title":"<code>State</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for a state manager.</p> <p>This class is responsible for managing task states. It provides an interface for state management and captures task-related metrics.</p> <p>Attributes:</p> Name Type Description <code>buffer</code> <code>Dict[str, Any]</code> <p>Buffer for state data.</p> <code>log</code> <code>logging.Logger</code> <p>Logger for capturing logs.</p> <code>task_id</code> <code>str</code> <p>Identifier for the task.</p>"},{"location":"core/core_state_base/#core.state.base.State.__del__","title":"<code>__del__()</code>","text":"<p>Destructor to flush the buffer before object deletion.</p> <p>This ensures that any buffered state data is not lost when the object is deleted.</p>"},{"location":"core/core_state_base/#core.state.base.State.flush","title":"<code>flush()</code>","text":"<p>Flush the buffer to the state storage.</p> <p>This method is responsible for writing the buffered state data to the underlying storage mechanism.</p>"},{"location":"core/core_state_base/#core.state.base.State.get","title":"<code>get(task_id, key)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to get the state associated with a task and key.</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>str</code> <p>The task identifier.</p> required <code>key</code> <code>str</code> <p>The key to get the state for.</p> required <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>Optional[Dict[str, Any]]: The state associated with the task and key, if it exists.</p>"},{"location":"core/core_state_base/#core.state.base.State.get_state","title":"<code>get_state(key)</code>","text":"<p>Get the state associated with a key from the buffer or underlying storage.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to get the state for.</p> required <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>Optional[Dict[str, Any]]: The state associated with the key.</p>"},{"location":"core/core_state_base/#core.state.base.State.set","title":"<code>set(task_id, key, value)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to set the state associated with a task and key.</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>str</code> <p>The task identifier.</p> required <code>key</code> <code>str</code> <p>The key to set the state for.</p> required <code>value</code> <code>Dict[str, Any]</code> <p>The state to set.</p> required"},{"location":"core/core_state_base/#core.state.base.State.set_state","title":"<code>set_state(key, value)</code>","text":"<p>Set the state associated with a key in the buffer.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to set the state for.</p> required <code>value</code> <code>Dict[str, Any]</code> <p>The state to set.</p> required"},{"location":"core/core_state_dynamo/","title":"DynamoDB State","text":"<p>State manager using dynamoDB</p>"},{"location":"core/core_state_dynamo/#core.state.dynamo.DynamoDBState","title":"<code>DynamoDBState</code>","text":"<p>             Bases: <code>State</code></p> <p>DynamoDBState: A state manager that stores state in DynamoDB.</p> <p>Attributes:</p> Name Type Description <code>dynamodb</code> <code>boto3.resources.factory.dynamodb.ServiceResource</code> <p>The DynamoDB service resource.</p> <code>table</code> <code>boto3.resources.factory.dynamodb.Table</code> <p>The DynamoDB table.</p>"},{"location":"core/core_state_dynamo/#core.state.dynamo.DynamoDBState.__init__","title":"<code>__init__(task_id, table_name, region_name)</code>","text":"<p>Initialize a new DynamoDB state manager.</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>str</code> <p>The task identifier.</p> required <code>table_name</code> <code>str</code> <p>The name of the DynamoDB table.</p> required <code>region_name</code> <code>str</code> <p>The name of the AWS region.</p> required"},{"location":"core/core_state_dynamo/#core.state.dynamo.DynamoDBState.get","title":"<code>get(task_id, key)</code>","text":"<p>Get the state associated with a task and key.</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>str</code> <p>The task identifier.</p> required <code>key</code> <code>str</code> <p>The key to get the state for.</p> required <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>Optional[Dict[str, Any]]: The state associated with the task and key, if it exists.</p>"},{"location":"core/core_state_dynamo/#core.state.dynamo.DynamoDBState.set","title":"<code>set(task_id, key, value)</code>","text":"<p>Set the state associated with a task and key.</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>str</code> <p>The task identifier.</p> required <code>key</code> <code>str</code> <p>The key to set the state for.</p> required <code>value</code> <code>Dict[str, Any]</code> <p>The state to set.</p> required"},{"location":"core/core_state_memory/","title":"In-memory State","text":"<p>State manager using local memory</p>"},{"location":"core/core_state_memory/#core.state.memory.InMemoryState","title":"<code>InMemoryState</code>","text":"<p>             Bases: <code>State</code></p> <p>\ud83e\udde0 InMemoryState: A state manager that stores state in memory.</p> <p>This manager is useful for temporary storage or testing purposes. Since it's in-memory, the data will be lost once the application stops.</p>"},{"location":"core/core_state_memory/#core.state.memory.InMemoryState--attributes","title":"Attributes:","text":"<ul> <li><code>store</code> (Dict[str, Dict]): The in-memory store for states.</li> </ul>"},{"location":"core/core_state_memory/#core.state.memory.InMemoryState--usage","title":"Usage:","text":"<pre><code>manager = InMemoryState()\nmanager.set_state(\"user123\", {\"status\": \"active\"})\nstate = manager.get_state(\"user123\")\nprint(state)  # Outputs: {\"status\": \"active\"}\n</code></pre> <p>Remember, this is an in-memory store. Do not use it for persistent storage!</p>"},{"location":"core/core_state_memory/#core.state.memory.InMemoryState.__init__","title":"<code>__init__(task_id)</code>","text":"<p>\ud83d\udca5 Initialize a new in-memory state manager.</p>"},{"location":"core/core_state_memory/#core.state.memory.InMemoryState.get","title":"<code>get(task_id, key)</code>","text":"<p>\ud83d\udcd6 Get the state associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to get the state for.</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>Optional[Dict]</code> <p>The state associated with the key, or None if not found.</p>"},{"location":"core/core_state_memory/#core.state.memory.InMemoryState.set","title":"<code>set(task_id, key, value)</code>","text":"<p>\ud83d\udcdd Set the state associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to set the state for.</p> required <code>value</code> <code>Dict</code> <p>The state to set.</p> required <p>Example: <pre><code>manager.set_state(\"user123\", {\"status\": \"active\"})\n</code></pre></p>"},{"location":"core/core_state_postgres/","title":"Postgres State","text":"<p>State manager using postgres database</p>"},{"location":"core/core_state_postgres/#core.state.postgres.PostgresState","title":"<code>PostgresState</code>","text":"<p>             Bases: <code>State</code></p> <p>\ud83d\uddc4\ufe0f PostgresState: A state manager that stores state in a PostgreSQL database.</p> <p>This manager provides a persistent storage solution using a PostgreSQL database.</p> <p>Attributes:</p> Name Type Description <code>conn</code> <code>psycopg2.extensions.connection</code> <p>The PostgreSQL connection.</p> <code>table</code> <code>str</code> <p>The table to use for storing state data.</p>"},{"location":"core/core_state_postgres/#core.state.postgres.PostgresState.__init__","title":"<code>__init__(task_id, host, port, user, password, database, table='geniusrise_state')</code>","text":"<p>Initialize a new PostgreSQL state manager.</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>str</code> <p>The identifier for the task.</p> required <code>host</code> <code>str</code> <p>The host of the PostgreSQL server.</p> required <code>port</code> <code>int</code> <p>The port of the PostgreSQL server.</p> required <code>user</code> <code>str</code> <p>The user to connect as.</p> required <code>password</code> <code>str</code> <p>The user's password.</p> required <code>database</code> <code>str</code> <p>The database to connect to.</p> required <code>table</code> <code>str</code> <p>The table to use. Defaults to \"geniusrise_state\".</p> <code>'geniusrise_state'</code>"},{"location":"core/core_state_postgres/#core.state.postgres.PostgresState.get","title":"<code>get(task_id, key)</code>","text":"<p>Get the state associated with a task and key.</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>str</code> <p>The task identifier.</p> required <code>key</code> <code>str</code> <p>The key to get the state for.</p> required <p>Returns:</p> Type Description <code>Optional[Dict]</code> <p>Optional[Dict]: The state associated with the task and key, or None if not found.</p>"},{"location":"core/core_state_postgres/#core.state.postgres.PostgresState.set","title":"<code>set(task_id, key, value)</code>","text":"<p>Set the state associated with a task and key.</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>str</code> <p>The task identifier.</p> required <code>key</code> <code>str</code> <p>The key to set the state for.</p> required <code>value</code> <code>Dict</code> <p>The state to set.</p> required"},{"location":"core/core_state_redis/","title":"Redis State","text":"<p>State manager using redis</p>"},{"location":"core/core_state_redis/#core.state.redis.RedisState","title":"<code>RedisState</code>","text":"<p>             Bases: <code>State</code></p> <p>RedisState: A state manager that stores state in Redis.</p> <p>This manager provides a fast, in-memory storage solution using Redis.</p> <p>Attributes:</p> Name Type Description <code>redis</code> <code>redis.Redis</code> <p>The Redis connection.</p>"},{"location":"core/core_state_redis/#core.state.redis.RedisState.__init__","title":"<code>__init__(task_id, host, port, db)</code>","text":"<p>Initialize a new Redis state manager.</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>str</code> <p>The task identifier.</p> required <code>host</code> <code>str</code> <p>The host of the Redis server.</p> required <code>port</code> <code>int</code> <p>The port of the Redis server.</p> required <code>db</code> <code>int</code> <p>The database number to connect to.</p> required"},{"location":"core/core_state_redis/#core.state.redis.RedisState.get","title":"<code>get(task_id, key)</code>","text":"<p>Get the state associated with a task and key.</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>str</code> <p>The task identifier.</p> required <code>key</code> <code>str</code> <p>The key to get the state for.</p> required <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>Optional[Dict[str, Any]]: The state associated with the task and key, if it exists.</p>"},{"location":"core/core_state_redis/#core.state.redis.RedisState.set","title":"<code>set(task_id, key, value)</code>","text":"<p>Set the state associated with a task and key.</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>str</code> <p>The task identifier.</p> required <code>key</code> <code>str</code> <p>The key to set the state for.</p> required <code>value</code> <code>Dict[str, Any]</code> <p>The state to set.</p> required"},{"location":"core/core_task_base/","title":"Task","text":"<p>Base class for Task</p>"},{"location":"core/core_task_base/#core.task.base.Task","title":"<code>Task</code>","text":"<p>             Bases: <code>ABC</code></p> <p>\ud83d\udee0\ufe0f Task: Class for managing tasks.</p> <p>This class provides a foundation for creating and managing tasks. Each task has a unique identifier and can be associated with specific input and output data.</p>"},{"location":"core/core_task_base/#core.task.base.Task--attributes","title":"Attributes:","text":"<ul> <li><code>id</code> (uuid.UUID): Unique identifier for the task.</li> <li><code>input</code> (Input): Configuration for input data.</li> <li><code>output</code> (Output): Configuration for output data.</li> </ul>"},{"location":"core/core_task_base/#core.task.base.Task--usage","title":"Usage:","text":"<pre><code>task = Task()\ntask.execute(\"fetch_data\")\n</code></pre> <p>!!! note     Extend this class to implement specific task functionalities.</p>"},{"location":"core/core_task_base/#core.task.base.Task.__init__","title":"<code>__init__(id=None)</code>","text":"<p>Initialize a new task.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Input</code> <p>Configuration for input data.</p> required <code>output</code> <code>Output</code> <p>Configuration for output data.</p> required"},{"location":"core/core_task_base/#core.task.base.Task.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a string representation of the task.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string representation of the task.</p>"},{"location":"core/core_task_base/#core.task.base.Task.execute","title":"<code>execute(method_name, *args, **kwargs)</code>","text":"<p>\ud83d\ude80 Execute a given fetch_* method if it exists.</p> <p>Parameters:</p> Name Type Description Default <code>method_name</code> <code>str</code> <p>The name of the fetch_* method to execute.</p> required <code>*args</code> <p>Positional arguments to pass to the method.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments to pass to the method.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The result of the fetch_* method, or None if the method does not exist.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If the specified method doesn't exist.</p>"},{"location":"core/core_task_base/#core.task.base.Task.get_methods","title":"<code>get_methods()</code>  <code>staticmethod</code>","text":"<p>\ud83d\udcdc Get all the fetch_* methods and their parameters along with their default values and docstrings.</p> <p>Returns:</p> Type Description <code>List[Tuple[str, List[str], Optional[str]]]</code> <p>List[Tuple[str, List[str], str]]: A list of tuples, where each tuple contains the name of a fetch_* method,</p> <code>List[Tuple[str, List[str], Optional[str]]]</code> <p>a list of its parameters along with their default values, and its docstring.</p>"},{"location":"core/core_task_base/#core.task.base.Task.print_help","title":"<code>print_help()</code>  <code>staticmethod</code>","text":"<p>\ud83d\udda8\ufe0f Pretty print the fetch_* methods and their parameters along with their default values and docstrings. Also prints the class's docstring and init parameters.</p>"},{"location":"core/docker/","title":"Docker Deployment","text":"<p>DockerResourceManager is a utility for managing Docker resources, including containers and images. It provides a command-line interface (CLI) for various Docker operations, such as listing, inspecting, creating, starting, and stopping containers, as well as managing images.</p> <p>This class uses the Docker SDK for Python to interact with the Docker daemon, offering a convenient way to manage Docker containers and images from the command line.</p> CLI Usage <p>genius docker sub-command</p> Sub-commands <ul> <li>list_containers: List all containers, with an option to include stopped containers.                  <code>genius docker list_containers [--all]</code></li> <li>inspect_container: Inspect a specific container by its ID.                    <code>genius docker inspect_container &lt;container_id&gt;</code></li> <li>create_container: Create a new container with specified image, command, and other parameters.                   <code>genius docker create_container &lt;image&gt; [options]</code></li> <li>start_container: Start a container by its ID.                  <code>genius docker start_container &lt;container_id&gt;</code></li> <li>stop_container: Stop a container by its ID.                 <code>genius docker stop_container &lt;container_id&gt;</code></li> <li>list_images: List all Docker images available on the local system.              <code>genius docker list_images</code></li> <li>inspect_image: Inspect a specific image by its ID.                <code>genius docker inspect_image &lt;image_id&gt;</code></li> <li>pull_image: Pull an image from a Docker registry.             <code>genius docker pull_image &lt;image&gt;</code></li> <li>push_image: Push an image to a Docker registry.             <code>genius docker push_image &lt;image&gt;</code></li> </ul> <p>Each sub-command supports various options to specify the details of the container or image operation, such as environment variables, port mappings, volume mappings, and more.</p> <p>Attributes:</p> Name Type Description <code>client</code> <p>The Docker client connection to interact with the Docker daemon.</p> <code>log</code> <p>Logger for the class to log information, warnings, and errors.</p> <code>console</code> <p>Rich console object to print formatted and styled outputs.</p> Methods <ul> <li>connect: Method to establish a connection to the Docker daemon.</li> <li>list_containers: Method to list all containers, with an option to include stopped ones.</li> <li>inspect_container: Method to inspect details of a specific container.</li> <li>create_container: Method to create a new container with given parameters.</li> <li>start_container: Method to start a specific container.</li> <li>stop_container: Method to stop a specific container.</li> <li>list_images: Method to list all Docker images.</li> <li>inspect_image: Method to inspect a specific image.</li> <li>pull_image: Method to pull an image from a Docker registry.</li> <li>push_image: Method to push an image to a Docker registry.</li> </ul> Note <ul> <li>Ensure that the Docker daemon is running and accessible at the specified URL.</li> <li>Make sure to have the necessary permissions to interact with the Docker daemon and manage containers and images.</li> </ul>"},{"location":"core/docker/#runners.docker.base.DockerResourceManager.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the Docker Resource Manager.</p>"},{"location":"core/docker/#runners.docker.base.DockerResourceManager.connect","title":"<code>connect(base_url='unix://var/run/docker.sock')</code>","text":"<p>Connect to the Docker daemon.</p> <p>Parameters:</p> Name Type Description Default <code>base_url</code> <code>str</code> <p>URL to the Docker daemon.</p> <code>'unix://var/run/docker.sock'</code>"},{"location":"core/docker/#runners.docker.base.DockerResourceManager.create_container","title":"<code>create_container(image, command=None, name=None, env_vars=None, ports=None, volumes=None, **kwargs)</code>","text":"<p>Create a new container.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>Name of the image to create the container from.</p> required <code>command</code> <code>Optional[str]</code> <p>Command to run in the container.</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>Name of the container.</p> <code>None</code> <code>env_vars</code> <code>Optional[Dict[str, str]]</code> <p>Environment variables.</p> <code>None</code> <code>ports</code> <code>Optional[Dict[str, str]]</code> <p>Port mappings.</p> <code>None</code> <code>volumes</code> <code>Optional[Dict[str, Dict[str, str]]]</code> <p>Volume mappings.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>ID of the created container.</p>"},{"location":"core/docker/#runners.docker.base.DockerResourceManager.create_parser","title":"<code>create_parser(parser)</code>","text":"<p>Create a parser for CLI commands.</p> <p>Returns:</p> Name Type Description <code>ArgumentParser</code> <code>ArgumentParser</code> <p>The parser for Docker operations.</p>"},{"location":"core/docker/#runners.docker.base.DockerResourceManager.inspect_container","title":"<code>inspect_container(container_id)</code>","text":"<p>Inspect a specific container.</p> <p>Parameters:</p> Name Type Description Default <code>container_id</code> <code>str</code> <p>ID of the container to inspect.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Container details.</p>"},{"location":"core/docker/#runners.docker.base.DockerResourceManager.inspect_image","title":"<code>inspect_image(image_id)</code>","text":"<p>Inspect a specific image.</p> <p>Parameters:</p> Name Type Description Default <code>image_id</code> <code>str</code> <p>ID of the image to inspect.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Image details.</p>"},{"location":"core/docker/#runners.docker.base.DockerResourceManager.list_containers","title":"<code>list_containers(all_containers=False)</code>","text":"<p>List all containers.</p> <p>Parameters:</p> Name Type Description Default <code>all_containers</code> <code>bool</code> <p>Flag to list all containers, including stopped ones.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>List[Any]: List of containers.</p>"},{"location":"core/docker/#runners.docker.base.DockerResourceManager.list_images","title":"<code>list_images()</code>","text":"<p>List all Docker images.</p> <p>Returns:</p> Type Description <code>List[Any]</code> <p>List[Any]: List of images.</p>"},{"location":"core/docker/#runners.docker.base.DockerResourceManager.pull_image","title":"<code>pull_image(image)</code>","text":"<p>Pull an image from a Docker registry.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>Name of the image to pull.</p> required"},{"location":"core/docker/#runners.docker.base.DockerResourceManager.push_image","title":"<code>push_image(image)</code>","text":"<p>Push an image to a Docker registry.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>Name of the image to push.</p> required"},{"location":"core/docker/#runners.docker.base.DockerResourceManager.run","title":"<code>run(args)</code>","text":"<p>Run the Docker Resource Manager based on the parsed CLI arguments.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>The parsed CLI arguments.</p> required"},{"location":"core/docker/#runners.docker.base.DockerResourceManager.start_container","title":"<code>start_container(container_id)</code>","text":"<p>Start a container.</p> <p>Parameters:</p> Name Type Description Default <code>container_id</code> <code>str</code> <p>ID of the container to start.</p> required"},{"location":"core/docker/#runners.docker.base.DockerResourceManager.stop_container","title":"<code>stop_container(container_id)</code>","text":"<p>Stop a container.</p> <p>Parameters:</p> Name Type Description Default <code>container_id</code> <code>str</code> <p>ID of the container to stop.</p> required"},{"location":"core/docker_swarm/","title":"Docker Swarm Deployment","text":"<p>             Bases: <code>DockerResourceManager</code></p> <p>DockerSwarmManager is a utility for managing Docker Swarm services, including creating, inspecting, updating, and removing services. It extends DockerResourceManager to provide swarm-specific functionalities and commands via a command-line interface (CLI).</p> <p>The manager interacts with the Docker Swarm API, offering a convenient way to manage Swarm services, nodes, and other swarm-related tasks from the command line.</p> CLI Usage <p>genius docker swarm sub-command</p> Sub-commands <ul> <li>list_nodes: List all nodes in the Docker Swarm.             <code>genius docker swarm list_nodes</code></li> <li>inspect_node: Inspect a specific Swarm node by its ID.               <code>genius docker swarm inspect_node &lt;node_id&gt;</code></li> <li>create_service: Create a new service in the Docker Swarm with comprehensive specifications.                 <code>genius docker swarm create_service [options]</code></li> <li>list_services: List all services in the Docker Swarm.                <code>genius docker swarm list_services</code></li> <li>inspect_service: Inspect a specific service by its ID.                  <code>genius docker swarm inspect_service &lt;service_id&gt;</code></li> <li>update_service: Update an existing service with new parameters.                 <code>genius docker swarm update_service &lt;service_id&gt; [options]</code></li> <li>remove_service: Remove a service from the Docker Swarm.                 <code>genius docker swarm remove_service &lt;service_id&gt;</code></li> <li>service_logs: Retrieve logs of a Docker Swarm service.               <code>genius docker swarm service_logs &lt;service_id&gt; [--tail] [--follow]</code></li> <li>scale_service: Scale a service to a specified number of replicas.                <code>genius docker swarm scale_service &lt;service_id&gt; &lt;replicas&gt;</code></li> </ul> <p>Each sub-command supports various options to specify the details of the swarm node or service operation. These options include node and service IDs, image and command specifications for services, environment variables, resource limits, and much more.</p> <p>Attributes:</p> Name Type Description <code>swarm_client</code> <p>The Docker Swarm client connection to interact with the Docker Swarm API.</p> <code>log</code> <p>Logger for the class to log information, warnings, and errors.</p> <code>console</code> <p>Rich console object to print formatted and styled outputs.</p> Methods <ul> <li>connect_to_swarm: Method to establish a connection to the Docker Swarm.</li> <li>list_nodes: Method to list all nodes in the Docker Swarm.</li> <li>inspect_node: Method to inspect details of a specific Swarm node.</li> <li>create_service: Method to create a new service with given specifications.</li> <li>list_services: Method to list all services in the Docker Swarm.</li> <li>inspect_service: Method to inspect a specific service.</li> <li>update_service: Method to update an existing service with new parameters.</li> <li>remove_service: Method to remove a service from the Docker Swarm.</li> <li>get_service_logs: Method to retrieve logs of a Docker Swarm service.</li> <li>scale_service: Method to scale a service to a specified number of replicas.</li> </ul> Note <ul> <li>Ensure that the Docker Swarm is initialized and running.</li> <li>Make sure to have the necessary permissions to interact with the Docker Swarm and manage services and nodes.</li> </ul>"},{"location":"core/docker_swarm/#runners.docker.swarm.DockerSwarmManager.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the Docker Swarm Manager.</p>"},{"location":"core/docker_swarm/#runners.docker.swarm.DockerSwarmManager.connect_to_swarm","title":"<code>connect_to_swarm(base_url='unix://var/run/docker.sock')</code>","text":"<p>Connect to the Docker Swarm.</p> <p>Parameters:</p> Name Type Description Default <code>base_url</code> <code>str</code> <p>URL to the Docker daemon.</p> <code>'unix://var/run/docker.sock'</code>"},{"location":"core/docker_swarm/#runners.docker.swarm.DockerSwarmManager.create_parser","title":"<code>create_parser(parser)</code>","text":"<p>Extend the parser for CLI commands to include Docker Swarm operations.</p> <p>Parameters:</p> Name Type Description Default <code>parser</code> <code>ArgumentParser</code> <p>The existing parser.</p> required <p>Returns:</p> Name Type Description <code>ArgumentParser</code> <code>ArgumentParser</code> <p>The extended parser with Docker Swarm operations.</p>"},{"location":"core/docker_swarm/#runners.docker.swarm.DockerSwarmManager.create_service","title":"<code>create_service(image, command, args)</code>","text":"<p>Create a new service in the Docker Swarm with comprehensive specifications.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>Docker image to use for the service.</p> required <code>command</code> <code>Union[str, List[str]]</code> <p>Command to run in the service.</p> required <code>args</code> <code>Namespace</code> <p>Arguments from the CLI for service creation.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>ID of the created service.</p>"},{"location":"core/docker_swarm/#runners.docker.swarm.DockerSwarmManager.get_service_logs","title":"<code>get_service_logs(service_id, tail=100, follow=False)</code>","text":"<p>Retrieve logs of a Docker Swarm service.</p> <p>Parameters:</p> Name Type Description Default <code>service_id</code> <code>str</code> <p>ID of the service.</p> required <code>tail</code> <code>int</code> <p>Number of lines to tail from the end of the logs. Defaults to 100.</p> <code>100</code> <code>follow</code> <code>bool</code> <p>Follow log output. Defaults to False.</p> <code>False</code>"},{"location":"core/docker_swarm/#runners.docker.swarm.DockerSwarmManager.inspect_node","title":"<code>inspect_node(node_id)</code>","text":"<p>Inspect a specific Swarm node.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>str</code> <p>ID of the node to inspect.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Node details.</p>"},{"location":"core/docker_swarm/#runners.docker.swarm.DockerSwarmManager.inspect_service","title":"<code>inspect_service(service_id)</code>","text":"<p>Inspect a specific service in the Docker Swarm.</p> <p>Parameters:</p> Name Type Description Default <code>service_id</code> <code>str</code> <p>ID of the service to inspect.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Service details.</p>"},{"location":"core/docker_swarm/#runners.docker.swarm.DockerSwarmManager.list_nodes","title":"<code>list_nodes()</code>","text":"<p>List all nodes in the Docker Swarm.</p> <p>Returns:</p> Type Description <code>List[Any]</code> <p>List[Any]: List of Swarm nodes.</p>"},{"location":"core/docker_swarm/#runners.docker.swarm.DockerSwarmManager.list_services","title":"<code>list_services()</code>","text":"<p>List all services in the Docker Swarm.</p> <p>Returns:</p> Type Description <code>List[Any]</code> <p>List[Any]: List of services.</p>"},{"location":"core/docker_swarm/#runners.docker.swarm.DockerSwarmManager.remove_service","title":"<code>remove_service(service_id)</code>","text":"<p>Remove a service from the Docker Swarm.</p> <p>Parameters:</p> Name Type Description Default <code>service_id</code> <code>str</code> <p>ID of the service to remove.</p> required"},{"location":"core/docker_swarm/#runners.docker.swarm.DockerSwarmManager.run","title":"<code>run(args)</code>","text":"<p>Run the Docker Swarm Manager based on the parsed CLI arguments.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>The parsed CLI arguments.</p> required"},{"location":"core/docker_swarm/#runners.docker.swarm.DockerSwarmManager.scale_service","title":"<code>scale_service(service_id, replicas)</code>","text":"<p>Scale a Docker Swarm service to a specified number of replicas.</p> <p>Parameters:</p> Name Type Description Default <code>service_id</code> <code>str</code> <p>ID of the service to scale.</p> required <code>replicas</code> <code>int</code> <p>Desired number of replicas.</p> required"},{"location":"core/docker_swarm/#runners.docker.swarm.DockerSwarmManager.update_service","title":"<code>update_service(service_id, image, command, args)</code>","text":"<p>Update an existing service in the Docker Swarm.</p> <p>Parameters:</p> Name Type Description Default <code>service_id</code> <code>str</code> <p>ID of the service to update.</p> required <code>args</code> <code>Namespace</code> <p>Arguments from the CLI for service update.</p> required"},{"location":"core/k8s_base/","title":"Kubernetes","text":""},{"location":"core/k8s_base/#runners.k8s.base.K8sResourceManager.__create_image_pull_secret","title":"<code>__create_image_pull_secret(name, registry, username, password)</code>","text":"<p>\ud83d\udd11 Create an image pull secret for a Docker registry.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the secret.</p> required <code>registry</code> <code>str</code> <p>Docker registry URL.</p> required <code>username</code> <code>str</code> <p>Username for the registry.</p> required <code>password</code> <code>str</code> <p>Password for the registry.</p> required"},{"location":"core/k8s_base/#runners.k8s.base.K8sResourceManager.__init__","title":"<code>__init__()</code>","text":"<p>\ud83d\ude80 Initialize the Kubernetes Resource Manager.</p> <p>Attributes:</p> Name Type Description <code>api_instance</code> <p>Core API instance for Kubernetes</p> <code>apps_api_instance</code> <p>Apps API instance for Kubernetes</p> <code>cluster_name</code> <p>Name of the Kubernetes cluster</p> <code>context_name</code> <p>Name of the kubeconfig context</p> <code>namespace</code> <p>Kubernetes namespace</p> <code>labels</code> <p>Labels for Kubernetes resources</p> <code>annotations</code> <p>Annotations for Kubernetes resources</p>"},{"location":"core/k8s_base/#runners.k8s.base.K8sResourceManager.__wait_for_pod_completion","title":"<code>__wait_for_pod_completion(pod_name, timeout=600, poll_interval=5)</code>","text":"<p>\u23f3 Wait for a Pod to complete its execution.</p> <p>Parameters:</p> Name Type Description Default <code>pod_name</code> <code>str</code> <p>Name of the Pod.</p> required <code>timeout</code> <code>int</code> <p>Maximum time to wait in seconds.</p> <code>600</code> <code>poll_interval</code> <code>int</code> <p>Time between status checks in seconds.</p> <code>5</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the Pod succeeded, False otherwise.</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If waiting for the Pod times out.</p>"},{"location":"core/k8s_base/#runners.k8s.base.K8sResourceManager.connect","title":"<code>connect(kube_config_path, cluster_name, context_name, namespace='default', labels={}, annotations={}, api_key=None, api_host=None, verify_ssl=True, ssl_ca_cert=None)</code>","text":"<p>\ud83c\udf10 Connect to a Kubernetes cluster.</p> <p>Parameters:</p> Name Type Description Default <code>kube_config_path</code> <code>str</code> <p>Path to the kubeconfig file.</p> required <code>cluster_name</code> <code>str</code> <p>Name of the Kubernetes cluster.</p> required <code>context_name</code> <code>str</code> <p>Name of the kubeconfig context.</p> required <code>namespace</code> <code>str</code> <p>Kubernetes namespace.</p> <code>'default'</code> <code>labels</code> <code>dict</code> <p>Labels for Kubernetes resources.</p> <code>{}</code> <code>annotations</code> <code>dict</code> <p>Annotations for Kubernetes resources.</p> <code>{}</code> <code>api_key</code> <code>str</code> <p>API key for Kubernetes cluster.</p> <code>None</code> <code>api_host</code> <code>str</code> <p>API host for Kubernetes cluster.</p> <code>None</code> <code>verify_ssl</code> <code>bool</code> <p>Whether to verify SSL certificates.</p> <code>True</code> <code>ssl_ca_cert</code> <code>str</code> <p>Path to the SSL CA certificate.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither kube_config_path and context_name nor api_key and api_host are provided.</p>"},{"location":"core/k8s_base/#runners.k8s.base.K8sResourceManager.describe","title":"<code>describe(pod_name)</code>","text":"<p>\ud83d\udcdd Describe a Kubernetes pod.</p> <p>Parameters:</p> Name Type Description Default <code>pod_name</code> <code>str</code> <p>Name of the pod.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>V1Pod</code> <p>Description of the pod.</p>"},{"location":"core/k8s_base/#runners.k8s.base.K8sResourceManager.logs","title":"<code>logs(name, tail=10, follow=True)</code>","text":"<p>\ud83d\udcdc Get logs of a Kubernetes pod.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the pod.</p> required <code>tail</code> <code>int</code> <p>Number of lines to tail.</p> <code>10</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Logs of the pod.</p>"},{"location":"core/k8s_base/#runners.k8s.base.K8sResourceManager.run","title":"<code>run(args)</code>","text":"<p>\ud83d\ude80 Run the Kubernetes resource manager.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>The parsed command line arguments.</p> required"},{"location":"core/k8s_base/#runners.k8s.base.K8sResourceManager.show","title":"<code>show()</code>","text":"<p>\ud83d\udccb Show all pods in the namespace.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>List[V1Pod]</code> <p>List of pods.</p>"},{"location":"core/k8s_base/#runners.k8s.base.K8sResourceManager.status","title":"<code>status(pod_name)</code>","text":"<p>\ud83d\udcdc Get the status of a Pod.</p> <p>Parameters:</p> Name Type Description Default <code>pod_name</code> <code>str</code> <p>Name of the Pod.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>V1Pod</code> <p>The status of the Pod.</p>"},{"location":"core/k8s_cron_job/","title":"Kubernetes CronJob","text":"<p>             Bases: <code>Job</code></p> <p>\ud83d\ude80 The CronJob class is responsible for managing Kubernetes CronJobs. It extends the Job class and provides additional functionalities specific to Kubernetes CronJobs.</p> CLI Usage <p>genius cronjob sub-command Examples:     <pre><code>genius cronjob create_cronjob --name example-cronjob --image example-image --command \"echo hello\" --schedule \"*/5 * * * *\" --namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre></p> <pre><code>```bash\ngenius cronjob delete_cronjob --name example-cronjob --namespace geniusrise \\\n    --context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n```\n\n```bash\ngenius cronjob get_cronjob_status --name example-cronjob --namespace geniusrise \\\n    --context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n```\n</code></pre> <p>YAML Configuration: <pre><code>    version: \"1.0\"\ncronjobs:\n- name: \"example-cronjob\"\nimage: \"example-image\"\ncommand: \"example-command\"\nschedule: \"*/5 * * * *\"\nenv_vars:\nKEY: \"value\"\ncpu: \"100m\"\nmemory: \"256Mi\"\nstorage: \"1Gi\"\ngpu: \"1\"\n</code></pre></p> Extended CLI Examples <pre><code>genius cronjob create_cronjob \\\n--k8s_kind cronjob \\\n--k8s_namespace geniusrise \\\n--k8s_context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev \\\n--k8s_name example-cronjob \\\n--k8s_image \"genius-dev.dkr.ecr.ap-south-1.amazonaws.com/geniusrise\" \\\n--k8s_schedule \"*/5 * * * *\" \\\n--k8s_env_vars '{\"AWS_DEFAULT_REGION\": \"ap-south-1\", \"AWS_SECRET_ACCESS_KEY\": \"\", \"AWS_ACCESS_KEY_ID\": \"\"}' \\\n--k8s_cpu \"100m\" \\\n--k8s_memory \"256Mi\"\n</code></pre> <pre><code>genius cronjob delete_cronjob \\\nexample-cronjob \\\n--namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre> <pre><code>genius cronjob get_cronjob_status \\\nexample-cronjob \\\n--namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre>"},{"location":"core/k8s_cron_job/#runners.k8s.cron_job.CronJob.__create_cronjob_spec","title":"<code>__create_cronjob_spec(image, command, schedule, env_vars={}, cpu=None, memory=None, storage=None, gpu=None, image_pull_secret_name=None)</code>","text":"<p>\ud83d\udce6 Create a Kubernetes CronJob specification.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>Docker image for the CronJob.</p> required <code>command</code> <code>str</code> <p>Command to run in the container.</p> required <code>env_vars</code> <code>dict</code> <p>Environment variables for the CronJob.</p> <code>{}</code> <code>cpu</code> <code>Optional[str]</code> <p>CPU requirements.</p> <code>None</code> <code>memory</code> <code>Optional[str]</code> <p>Memory requirements.</p> <code>None</code> <code>storage</code> <code>Optional[str]</code> <p>Storage requirements.</p> <code>None</code> <code>gpu</code> <code>Optional[str]</code> <p>GPU requirements.</p> <code>None</code> <code>image_pull_secret_name</code> <code>Optional[str]</code> <p>Name of the image pull secret.</p> <code>None</code> <p>Returns:</p> Type Description <code>client.V1CronJobSpec</code> <p>client.V1CronJobSpec: The CronJob specification.</p>"},{"location":"core/k8s_cron_job/#runners.k8s.cron_job.CronJob.__init__","title":"<code>__init__()</code>","text":"<p>\ud83d\ude80 Initialize the CronJob class for managing Kubernetes Cron Jobs.</p>"},{"location":"core/k8s_cron_job/#runners.k8s.cron_job.CronJob.create","title":"<code>create(name, image, schedule, command, env_vars={}, cpu=None, memory=None, storage=None, gpu=None, image_pull_secret_name=None, **kwargs)</code>","text":"<p>\ud83d\udee0 Create a Kubernetes CronJob.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the CronJob.</p> required <code>image</code> <code>str</code> <p>Docker image for the CronJob.</p> required <code>command</code> <code>str</code> <p>Command to run in the container.</p> required <code>schedule</code> <code>str</code> <p>Cron schedule.</p> required <code>env_vars</code> <code>dict</code> <p>Environment variables for the CronJob.</p> <code>{}</code>"},{"location":"core/k8s_cron_job/#runners.k8s.cron_job.CronJob.create_parser","title":"<code>create_parser(parser)</code>","text":"<p>\ud83c\udf9b Create a parser for CLI commands related to Cron Job functionalities.</p> <p>Parameters:</p> Name Type Description Default <code>parser</code> <code>ArgumentParser</code> <p>The main parser.</p> required <p>Returns:</p> Name Type Description <code>ArgumentParser</code> <code>ArgumentParser</code> <p>The parser with subparsers for each command.</p>"},{"location":"core/k8s_cron_job/#runners.k8s.cron_job.CronJob.delete","title":"<code>delete(name)</code>","text":"<p>\ud83d\uddd1 Delete a Kubernetes CronJob.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the CronJob to delete.</p> required"},{"location":"core/k8s_cron_job/#runners.k8s.cron_job.CronJob.run","title":"<code>run(args)</code>","text":"<p>\ud83d\ude80 Run the Cron Job manager.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>The parsed command line arguments.</p> required"},{"location":"core/k8s_cron_job/#runners.k8s.cron_job.CronJob.status","title":"<code>status(name)</code>","text":"<p>\ud83d\udcca Get the status of a Kubernetes CronJob.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the CronJob.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>V1CronJob</code> <p>Status of the CronJob.</p>"},{"location":"core/k8s_deployment/","title":"Kubernetes Deployment","text":"<p>             Bases: <code>K8sResourceManager</code></p> <p>\ud83d\ude80 Initialize the Deployment class for managing Kubernetes Deployments.</p> CLI Usage <p>geniusrise deployment sub-command Examples:</p> <pre><code>genius deployment create --name example-deployment --image example-image --command \"echo hello\" --namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre> <pre><code>genius deployment scale --name example-deployment --replicas 3 --namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre> <pre><code>genius deployment describe --name example-deployment --namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre> <pre><code>genius deployment delete --name example-deployment --namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre> <pre><code>genius deployment status --name example-deployment --namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre> <p>YAML Configuration:</p> <pre><code>version: \"1.0\"\ndeployments:\n- name: \"example-deployment\"\nimage: \"example-image\"\ncommand: \"example-command\"\nreplicas: 3\nenv_vars:\nKEY: \"value\"\ncpu: \"100m\"\nmemory: \"256Mi\"\nstorage: \"1Gi\"\ngpu: \"1\"\n</code></pre>"},{"location":"core/k8s_deployment/#runners.k8s.deployment.Deployment.__create_deployment_spec","title":"<code>__create_deployment_spec(image, command, replicas, image_pull_secret_name, env_vars, cpu=None, memory=None, storage=None, gpu=None)</code>","text":"<p>\ud83d\udce6 Create a Kubernetes Deployment specification.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>Docker image for the Deployment.</p> required <code>command</code> <code>str</code> <p>Command to run in the container.</p> required <code>replicas</code> <code>int</code> <p>Number of replicas.</p> required <code>image_pull_secret_name</code> <code>str</code> <p>Name of the image pull secret.</p> required <code>env_vars</code> <code>dict</code> <p>Environment variables for the Deployment.</p> required <code>cpu</code> <code>str</code> <p>CPU requirements.</p> <code>None</code> <code>memory</code> <code>str</code> <p>Memory requirements.</p> <code>None</code> <code>storage</code> <code>str</code> <p>Storage requirements.</p> <code>None</code> <code>gpu</code> <code>str</code> <p>GPU requirements.</p> <code>None</code> <p>Returns:</p> Type Description <code>client.V1DeploymentSpec</code> <p>client.V1DeploymentSpec: The Deployment specification.</p>"},{"location":"core/k8s_deployment/#runners.k8s.deployment.Deployment.__init__","title":"<code>__init__()</code>","text":"<p>\ud83d\ude80 Initialize the Deployment class for managing Kubernetes Deployments.</p>"},{"location":"core/k8s_deployment/#runners.k8s.deployment.Deployment.create","title":"<code>create(name, image, command, registry_creds=None, replicas=1, env_vars={}, cpu=None, memory=None, storage=None, gpu=None, **kwargs)</code>","text":"<p>\ud83d\udee0 Create a Kubernetes resource Deployment.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the resource.</p> required <code>image</code> <code>str</code> <p>Docker image for the resource.</p> required <code>command</code> <code>str</code> <p>Command to run in the container.</p> required <code>registry_creds</code> <code>dict</code> <p>Credentials for Docker registry.</p> <code>None</code> <code>replicas</code> <code>int</code> <p>Number of replicas for Deployment.</p> <code>1</code> <code>env_vars</code> <code>dict</code> <p>Environment variables for the resource.</p> <code>{}</code> <code>cpu</code> <code>str</code> <p>CPU requirements.</p> <code>None</code> <code>memory</code> <code>str</code> <p>Memory requirements.</p> <code>None</code> <code>storage</code> <code>str</code> <p>Storage requirements.</p> <code>None</code> <code>gpu</code> <code>str</code> <p>GPU requirements.</p> <code>None</code>"},{"location":"core/k8s_deployment/#runners.k8s.deployment.Deployment.create_parser","title":"<code>create_parser(parser)</code>","text":"<p>\ud83c\udf9b Create a parser for CLI commands related to Deployment functionalities.</p> <p>Parameters:</p> Name Type Description Default <code>parser</code> <code>ArgumentParser</code> <p>The main parser.</p> required <p>Returns:</p> Name Type Description <code>ArgumentParser</code> <code>ArgumentParser</code> <p>The parser with subparsers for each command.</p>"},{"location":"core/k8s_deployment/#runners.k8s.deployment.Deployment.delete","title":"<code>delete(name)</code>","text":"<p>\ud83d\uddd1 Delete a Kubernetes resource (Pod/Deployment/Service).</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the resource to delete.</p> required"},{"location":"core/k8s_deployment/#runners.k8s.deployment.Deployment.describe","title":"<code>describe(deployment_name)</code>","text":"<p>\ud83d\uddc2 Describe a Kubernetes deployment.</p> <p>Parameters:</p> Name Type Description Default <code>deployment_name</code> <code>str</code> <p>Name of the deployment.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>V1Deployment</code> <p>Description of the deployment.</p>"},{"location":"core/k8s_deployment/#runners.k8s.deployment.Deployment.run","title":"<code>run(args)</code>","text":"<p>\ud83d\ude80 Run the Deployment manager.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>The parsed command line arguments.</p> required"},{"location":"core/k8s_deployment/#runners.k8s.deployment.Deployment.scale","title":"<code>scale(name, replicas)</code>","text":"<p>\ud83d\udcc8 Scale a Kubernetes deployment.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the deployment.</p> required <code>replicas</code> <code>int</code> <p>Number of replicas.</p> required"},{"location":"core/k8s_deployment/#runners.k8s.deployment.Deployment.show","title":"<code>show()</code>","text":"<p>\ud83d\uddc2 List all deployments in the namespace.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>List[V1Deployment]</code> <p>List of deployments.</p>"},{"location":"core/k8s_deployment/#runners.k8s.deployment.Deployment.status","title":"<code>status(name)</code>","text":"<p>\ud83d\udcca Get the status of a Kubernetes deployment.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the deployment.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>V1Deployment</code> <p>Status of the deployment.</p>"},{"location":"core/k8s_job/","title":"Kubernetes Job","text":"<p>             Bases: <code>Deployment</code></p> <p>\ud83d\ude80 The Job class is responsible for managing Kubernetes Jobs. It extends the Deployment class and provides additional functionalities specific to Kubernetes Jobs.</p> CLI Usage <p>genius job sub-command Examples:     <pre><code>genius job create --name example-job --image example-image --command \"echo hello\" --cpu \"100m\" --memory \"256Mi\" --namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre></p> <pre><code>```bash\ngenius job delete --name example-job --namespace geniusrise \\\n    --context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n```\n\n```bash\ngenius job status --name example-job --namespace geniusrise \\\n    --context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n```\n</code></pre> <p>YAML Configuration: <pre><code>    version: \"1.0\"\njobs:\n- name: \"example-job\"\nimage: \"example-image\"\ncommand: \"example-command\"\nenv_vars:\nKEY: \"value\"\ncpu: \"100m\"\nmemory: \"256Mi\"\nstorage: \"1Gi\"\ngpu: \"1\"\n</code></pre></p> <p>Extended CLI Examples:</p> <pre><code>    genius job create \\\n--k8s_kind job \\\n--k8s_namespace geniusrise \\\n--k8s_context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev \\\n--k8s_name example-job \\\n--k8s_image \"genius-dev.dkr.ecr.ap-south-1.amazonaws.com/geniusrise\" \\\n--k8s_env_vars '{\"AWS_DEFAULT_REGION\": \"ap-south-1\", \"AWS_SECRET_ACCESS_KEY\": \"\", \"AWS_ACCESS_KEY_ID\": \"\"}' \\\n--k8s_cpu \"100m\" \\\n--k8s_memory \"256Mi\"\n</code></pre> <pre><code>    genius job delete \\\nexample-job \\\n--namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre> <pre><code>    genius job status \\\nexample-job \\\n--namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre>"},{"location":"core/k8s_job/#runners.k8s.job.Job.__init__","title":"<code>__init__()</code>","text":"<p>\ud83d\ude80 Initialize the Job class for managing Kubernetes Jobs.</p>"},{"location":"core/k8s_job/#runners.k8s.job.Job.create","title":"<code>create(name, image, command, env_vars={}, cpu=None, memory=None, storage=None, gpu=None, image_pull_secret_name=None, **kwargs)</code>","text":"<p>\ud83d\udee0 Create a Kubernetes Job.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the Job.</p> required <code>image</code> <code>str</code> <p>Docker image for the Job.</p> required <code>command</code> <code>str</code> <p>Command to run in the container.</p> required <code>env_vars</code> <code>dict</code> <p>Environment variables for the Job.</p> <code>{}</code>"},{"location":"core/k8s_job/#runners.k8s.job.Job.create_parser","title":"<code>create_parser(parser)</code>","text":"<p>\ud83c\udf9b Create a parser for CLI commands related to Job functionalities.</p> <p>Parameters:</p> Name Type Description Default <code>parser</code> <code>ArgumentParser</code> <p>The main parser.</p> required <p>Returns:</p> Name Type Description <code>ArgumentParser</code> <code>ArgumentParser</code> <p>The parser with subparsers for each command.</p>"},{"location":"core/k8s_job/#runners.k8s.job.Job.delete","title":"<code>delete(name)</code>","text":"<p>\ud83d\uddd1 Delete a Kubernetes Job.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the Job to delete.</p> required"},{"location":"core/k8s_job/#runners.k8s.job.Job.run","title":"<code>run(args)</code>","text":"<p>\ud83d\ude80 Run the Job manager.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>The parsed command line arguments.</p> required"},{"location":"core/k8s_job/#runners.k8s.job.Job.status","title":"<code>status(name)</code>","text":"<p>\ud83d\udcca Get the status of a Kubernetes Job.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the Job.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>V1Job</code> <p>Status of the Job.</p>"},{"location":"core/k8s_service/","title":"Kubernetes Service","text":"<p>             Bases: <code>Deployment</code></p> <p>\ud83d\ude80 Initialize the Service class for managing Kubernetes Services.</p> CLI Usage <p>genius service sub-command Examples:</p> <pre><code>genius service create --name example-service --image example-image --command \"echo hello\" --port 8080 --target_port 8080 --namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre> <pre><code>genius service delete --name example-service --namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre> <pre><code>genius service describe --name example-service --namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre> <pre><code>genius service show --namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre> <p>YAML Configuration:</p> <pre><code>version: \"1.0\"\nservices:\n- name: \"example-service\"\nimage: \"example-image\"\ncommand: \"example-command\"\nreplicas: 3\nport: 8080\ntarget_port: 8080\nenv_vars:\nKEY: \"value\"\ncpu: \"100m\"\nmemory: \"256Mi\"\nstorage: \"1Gi\"\ngpu: \"1\"\n</code></pre> Extended CLI Examples <pre><code>    genius service deploy \\\n--k8s_kind service \\\n--k8s_namespace geniusrise \\\n--k8s_context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev \\\n--k8s_name webhook \\\n--k8s_image \"genius-dev.dkr.ecr.ap-south-1.amazonaws.com/geniusrise\" \\\n--k8s_env_vars '{\"AWS_DEFAULT_REGION\": \"ap-south-1\", \"AWS_SECRET_ACCESS_KEY\": \"\", \"AWS_ACCESS_KEY_ID\": \"\"}' \\\n--k8s_port 8080 \\\n--k8s_target_port 8080\n</code></pre> <pre><code>    genius service delete \\\nwebhook \\\n--namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre>"},{"location":"core/k8s_service/#runners.k8s.service.Service.__create_service_spec","title":"<code>__create_service_spec(node_port, port, target_port)</code>","text":"<p>\ud83d\udce6 Create a Kubernetes Service specification.</p> <p>Parameters:</p> Name Type Description Default <code>port</code> <code>int</code> <p>Service port.</p> required <code>target_port</code> <code>int</code> <p>Container target port.</p> required <p>Returns:</p> Type Description <code>client.V1ServiceSpec</code> <p>client.V1ServiceSpec: The Service specification.</p>"},{"location":"core/k8s_service/#runners.k8s.service.Service.__init__","title":"<code>__init__()</code>","text":"<p>\ud83d\ude80 Initialize the Service class for managing Kubernetes Services.</p>"},{"location":"core/k8s_service/#runners.k8s.service.Service.create","title":"<code>create(name, image, command, registry_creds=None, replicas=1, node_port=80, port=80, target_port=8080, env_vars={}, cpu=None, memory=None, storage=None, gpu=None, **kwargs)</code>","text":"<p>\ud83d\udee0 Create a Kubernetes resource Service.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the resource.</p> required <code>image</code> <code>str</code> <p>Docker image for the resource.</p> required <code>command</code> <code>str</code> <p>Command to run in the container.</p> required <code>registry_creds</code> <code>dict</code> <p>Credentials for Docker registry.</p> <code>None</code> <code>replicas</code> <code>int</code> <p>Number of replicas for Deployment.</p> <code>1</code> <code>node_port</code> <code>int</code> <p>Service port that is exposed.</p> <code>80</code> <code>port</code> <code>int</code> <p>Service port.</p> <code>80</code> <code>target_port</code> <code>int</code> <p>Container target port.</p> <code>8080</code> <code>env_vars</code> <code>dict</code> <p>Environment variables for the resource.</p> <code>{}</code> <code>cpu</code> <code>str</code> <p>CPU requirements.</p> <code>None</code> <code>memory</code> <code>str</code> <p>Memory requirements.</p> <code>None</code> <code>storage</code> <code>str</code> <p>Storage requirements.</p> <code>None</code> <code>gpu</code> <code>str</code> <p>GPU requirements.</p> <code>None</code>"},{"location":"core/k8s_service/#runners.k8s.service.Service.create_parser","title":"<code>create_parser(parser)</code>","text":"<p>\ud83c\udf9b Create a parser for CLI commands related to Service functionalities.</p> <p>Parameters:</p> Name Type Description Default <code>parser</code> <code>ArgumentParser</code> <p>The main parser.</p> required <p>Returns:</p> Name Type Description <code>ArgumentParser</code> <code>ArgumentParser</code> <p>The parser with subparsers for each command.</p>"},{"location":"core/k8s_service/#runners.k8s.service.Service.delete","title":"<code>delete(name)</code>","text":"<p>\ud83d\uddd1 Delete a Kubernetes resource (Pod/Deployment/Service).</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the resource to delete.</p> required"},{"location":"core/k8s_service/#runners.k8s.service.Service.describe","title":"<code>describe(service_name)</code>","text":"<p>\ud83c\udf10 Describe a Kubernetes service.</p> <p>Parameters:</p> Name Type Description Default <code>service_name</code> <code>str</code> <p>Name of the service.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>V1Service</code> <p>Description of the service.</p>"},{"location":"core/k8s_service/#runners.k8s.service.Service.run","title":"<code>run(args)</code>","text":"<p>\ud83d\ude80 Run the Service manager.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>The parsed command line arguments.</p> required"},{"location":"core/k8s_service/#runners.k8s.service.Service.show","title":"<code>show()</code>","text":"<p>\ud83c\udf10 Show all services in the namespace.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>List[V1Service]</code> <p>List of services.</p>"},{"location":"core/k8s_service/#runners.k8s.service.Service.status","title":"<code>status(name)</code>","text":"<p>\ud83d\udcca Get the status of a Kubernetes service.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the service.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>V1Deployment</code> <p>Status of the service.</p>"},{"location":"databases/arangodb/","title":"ArangoDB","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/arangodb/#geniusrise_databases.arangodb.ArangoDB.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the Arango class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/arangodb/#geniusrise_databases.arangodb.ArangoDB.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius Arango rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--args host=myarangodb.example.com username=myusername password=mypassword database=mydb collection=mycollection\n</code></pre>"},{"location":"databases/arangodb/#geniusrise_databases.arangodb.ArangoDB.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_arangodb_spout:\nname: \"Arango\"\nmethod: \"fetch\"\nargs:\nhost: \"myarangodb.example.com\"\nusername: \"myusername\"\npassword: \"mypassword\"\ndatabase: \"mydb\"\ncollection: \"mycollection\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/arangodb/#geniusrise_databases.arangodb.ArangoDB.fetch","title":"<code>fetch(host, username, password, database, collection)</code>","text":"<p>\ud83d\udcd6 Fetch data from an ArangoDB collection and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The ArangoDB host.</p> required <code>username</code> <code>str</code> <p>The ArangoDB username.</p> required <code>password</code> <code>str</code> <p>The ArangoDB password.</p> required <code>database</code> <code>str</code> <p>The ArangoDB database name.</p> required <code>collection</code> <code>str</code> <p>The name of the ArangoDB collection.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the ArangoDB server or execute the command.</p>"},{"location":"databases/athena/","title":"Athena","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/athena/#geniusrise_databases.athena.Athena.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the Athena class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/athena/#geniusrise_databases.athena.Athena.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius Athena rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--args region_name=us-east-1 output_location=s3://mybucket/output query=\"SELECT * FROM mytable\"\n</code></pre>"},{"location":"databases/athena/#geniusrise_databases.athena.Athena.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_athena_spout:\nname: \"Athena\"\nmethod: \"fetch\"\nargs:\nregion_name: \"us-east-1\"\noutput_location: \"s3://mybucket/output\"\nquery: \"SELECT * FROM mytable\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/athena/#geniusrise_databases.athena.Athena.fetch","title":"<code>fetch(region_name, output_location, query)</code>","text":"<p>\ud83d\udcd6 Fetch data from an AWS Athena table and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>region_name</code> <code>str</code> <p>The AWS region name.</p> required <code>output_location</code> <code>str</code> <p>The S3 output location for the query results.</p> required <code>query</code> <code>str</code> <p>The SQL query to execute.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the AWS Athena service or execute the query.</p>"},{"location":"databases/azure_table/","title":"Athena","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/azure_table/#geniusrise_databases.azure_table.AzureTableStorage.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the AzureTableStorage class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/azure_table/#geniusrise_databases.azure_table.AzureTableStorage.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius AzureTableStorage rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--args account_name=my_account account_key=my_key table_name=my_table\n</code></pre>"},{"location":"databases/azure_table/#geniusrise_databases.azure_table.AzureTableStorage.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_azure_table_spout:\nname: \"AzureTableStorage\"\nmethod: \"fetch\"\nargs:\naccount_name: \"my_account\"\naccount_key: \"my_key\"\ntable_name: \"my_table\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/azure_table/#geniusrise_databases.azure_table.AzureTableStorage.fetch","title":"<code>fetch(account_name, account_key, table_name)</code>","text":"<p>\ud83d\udcd6 Fetch data from Azure Table Storage and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>account_name</code> <code>str</code> <p>The Azure Storage account name.</p> required <code>account_key</code> <code>str</code> <p>The Azure Storage account key.</p> required <code>table_name</code> <code>str</code> <p>The Azure Table Storage table name.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to Azure Table Storage or fetch the data.</p>"},{"location":"databases/bigquery/","title":"Bigquery","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/bigquery/#geniusrise_databases.bigquery.BigQuery.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the BigQuery class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/bigquery/#geniusrise_databases.bigquery.BigQuery.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius BigQuery rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--project_id my_project_id dataset_id=my_dataset table_id=my_table\n</code></pre>"},{"location":"databases/bigquery/#geniusrise_databases.bigquery.BigQuery.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_bigquery_spout:\nname: \"BigQuery\"\nmethod: \"fetch\"\nargs:\nproject_id: \"my_project_id\"\ndataset_id: \"my_dataset\"\ntable_id: \"my_table\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/bigquery/#geniusrise_databases.bigquery.BigQuery.fetch","title":"<code>fetch(project_id, dataset_id, table_id)</code>","text":"<p>\ud83d\udcd6 Fetch data from a BigQuery table and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>project_id</code> <code>str</code> <p>The Google Cloud project ID.</p> required <code>dataset_id</code> <code>str</code> <p>The BigQuery dataset ID.</p> required <code>table_id</code> <code>str</code> <p>The BigQuery table ID.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the BigQuery server or execute the query.</p>"},{"location":"databases/bigtable/","title":"BigTable","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/bigtable/#geniusrise_databases.bigtable.Bigtable.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the Bigtable class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/bigtable/#geniusrise_databases.bigtable.Bigtable.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius Bigtable rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--args project_id=my_project instance_id=my_instance table_id=my_table\n</code></pre>"},{"location":"databases/bigtable/#geniusrise_databases.bigtable.Bigtable.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_bigtable_spout:\nname: \"Bigtable\"\nmethod: \"fetch\"\nargs:\nproject_id: \"my_project\"\ninstance_id: \"my_instance\"\ntable_id: \"my_table\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/bigtable/#geniusrise_databases.bigtable.Bigtable.fetch","title":"<code>fetch(project_id, instance_id, table_id)</code>","text":"<p>\ud83d\udcd6 Fetch data from a Google Cloud Bigtable and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>project_id</code> <code>str</code> <p>The Google Cloud Project ID.</p> required <code>instance_id</code> <code>str</code> <p>The Bigtable instance ID.</p> required <code>table_id</code> <code>str</code> <p>The Bigtable table ID.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the Bigtable server or fetch the data.</p>"},{"location":"databases/cassandra/","title":"Cassandra","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/cassandra/#geniusrise_databases.cassandra.Cassandra.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the Cassandra class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/cassandra/#geniusrise_databases.cassandra.Cassandra.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius Cassandra rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--args hosts=localhost keyspace=my_keyspace query=\"SELECT * FROM my_table\" page_size=100\n</code></pre>"},{"location":"databases/cassandra/#geniusrise_databases.cassandra.Cassandra.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_cassandra_spout:\nname: \"Cassandra\"\nmethod: \"fetch\"\nargs:\nhosts: \"localhost\"\nkeyspace: \"my_keyspace\"\nquery: \"SELECT * FROM my_table\"\npage_size: 100\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/cassandra/#geniusrise_databases.cassandra.Cassandra.fetch","title":"<code>fetch(hosts, keyspace, query, page_size=100)</code>","text":"<p>\ud83d\udcd6 Fetch data from a Cassandra database and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>hosts</code> <code>str</code> <p>Comma-separated list of Cassandra hosts.</p> required <code>keyspace</code> <code>str</code> <p>The Cassandra keyspace to use.</p> required <code>query</code> <code>str</code> <p>The CQL query to execute.</p> required <code>page_size</code> <code>int</code> <p>The number of rows to fetch per page. Defaults to 100.</p> <code>100</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the Cassandra cluster or execute the query.</p>"},{"location":"databases/cloud_sql/","title":"Google Cloud SQL","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/cloud_sql/#geniusrise_databases.cloud_sql.GoogleCloudSQL.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the GoogleCloudSQL class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/cloud_sql/#geniusrise_databases.cloud_sql.GoogleCloudSQL.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius GoogleCloudSQL rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--args host=127.0.0.1 port=3306 user=root password=root database=mydb query=\"SELECT * FROM table\" page_size=100\n</code></pre>"},{"location":"databases/cloud_sql/#geniusrise_databases.cloud_sql.GoogleCloudSQL.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_google_cloud_sql_spout:\nname: \"GoogleCloudSQL\"\nmethod: \"fetch\"\nargs:\nhost: \"127.0.0.1\"\nport: 3306\nuser: \"root\"\npassword: \"root\"\ndatabase: \"mydb\"\nquery: \"SELECT * FROM table\"\npage_size: 100\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/cloud_sql/#geniusrise_databases.cloud_sql.GoogleCloudSQL.fetch","title":"<code>fetch(host, port, user, password, database, query, page_size=100)</code>","text":"<p>\ud83d\udcd6 Fetch data from a Google Cloud SQL database and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The Google Cloud SQL host.</p> required <code>port</code> <code>int</code> <p>The Google Cloud SQL port.</p> required <code>user</code> <code>str</code> <p>The Google Cloud SQL user.</p> required <code>password</code> <code>str</code> <p>The Google Cloud SQL password.</p> required <code>database</code> <code>str</code> <p>The Google Cloud SQL database name.</p> required <code>query</code> <code>str</code> <p>The SQL query to execute.</p> required <code>page_size</code> <code>int</code> <p>The number of rows to fetch per page. Defaults to 100.</p> <code>100</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the Google Cloud SQL or fetch the data.</p>"},{"location":"databases/cockroach/","title":"CockroachDB","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/cockroach/#geniusrise_databases.cockroach.CockroachDB.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the CockroachDB class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/cockroach/#geniusrise_databases.cockroach.CockroachDB.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius CockroachDB rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--args host=localhost port=26257 user=root password=root database=mydb query=\"SELECT * FROM table\" page_size=100\n</code></pre>"},{"location":"databases/cockroach/#geniusrise_databases.cockroach.CockroachDB.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_cockroachdb_spout:\nname: \"CockroachDB\"\nmethod: \"fetch\"\nargs:\nhost: \"localhost\"\nport: 26257\nuser: \"root\"\npassword: \"root\"\ndatabase: \"mydb\"\nquery: \"SELECT * FROM table\"\npage_size: 100\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/cockroach/#geniusrise_databases.cockroach.CockroachDB.fetch","title":"<code>fetch(host, port, user, password, database, query, page_size=100)</code>","text":"<p>\ud83d\udcd6 Fetch data from a CockroachDB database and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The CockroachDB host.</p> required <code>port</code> <code>int</code> <p>The CockroachDB port.</p> required <code>user</code> <code>str</code> <p>The CockroachDB user.</p> required <code>password</code> <code>str</code> <p>The CockroachDB password.</p> required <code>database</code> <code>str</code> <p>The CockroachDB database name.</p> required <code>query</code> <code>str</code> <p>The SQL query to execute.</p> required <code>page_size</code> <code>int</code> <p>The number of rows to fetch per page. Defaults to 100.</p> <code>100</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the CockroachDB server or execute the query.</p>"},{"location":"databases/cosmosdb/","title":"CosmosDB","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/cosmosdb/#geniusrise_databases.cosmosdb.CosmosDB.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the Cosmos DB class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/cosmosdb/#geniusrise_databases.cosmosdb.CosmosDB.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius CosmosDB rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--args endpoint=https://mycosmosdb.documents.azure.com:443/ my_database my_collection\n</code></pre>"},{"location":"databases/cosmosdb/#geniusrise_databases.cosmosdb.CosmosDB.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_cosmosdb_spout:\nname: \"CosmosDB\"\nmethod: \"fetch\"\nargs:\nendpoint: \"https://mycosmosdb.documents.azure.com:443/\"\ndatabase: \"my_database\"\ncollection: \"my_collection\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/cosmosdb/#geniusrise_databases.cosmosdb.CosmosDB.fetch","title":"<code>fetch(endpoint, database, collection)</code>","text":"<p>\ud83d\udcd6 Fetch data from a Cosmos DB collection and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>The Cosmos DB endpoint URL.</p> required <code>database</code> <code>str</code> <p>The Cosmos DB database name.</p> required <code>collection</code> <code>str</code> <p>The Cosmos DB collection name.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the Cosmos DB server or execute the query.</p>"},{"location":"databases/couchbase/","title":"Couchbase","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/couchbase/#geniusrise_databases.couchbase.Couchbase.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the CouchbaseSpout class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/couchbase/#geniusrise_databases.couchbase.Couchbase.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius CouchbaseSpout rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--args host=localhost username=admin password=password bucket_name=my_bucket query=\"SELECT * FROM my_bucket\" page_size=100\n</code></pre>"},{"location":"databases/couchbase/#geniusrise_databases.couchbase.Couchbase.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_couchbase_spout:\nname: \"CouchbaseSpout\"\nmethod: \"fetch\"\nargs:\nhost: \"localhost\"\nusername: \"admin\"\npassword: \"password\"\nbucket_name: \"my_bucket\"\nquery: \"SELECT * FROM my_bucket\"\npage_size: 100\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/couchbase/#geniusrise_databases.couchbase.Couchbase.fetch","title":"<code>fetch(host, username, password, bucket_name, query, page_size=100)</code>","text":"<p>\ud83d\udcd6 Fetch data from a Couchbase bucket and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The Couchbase host.</p> required <code>username</code> <code>str</code> <p>The Couchbase username.</p> required <code>password</code> <code>str</code> <p>The Couchbase password.</p> required <code>bucket_name</code> <code>str</code> <p>The Couchbase bucket name.</p> required <code>query</code> <code>str</code> <p>The N1QL query to execute.</p> required <code>page_size</code> <code>int</code> <p>The number of documents to fetch per page. Defaults to 100.</p> <code>100</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the Couchbase cluster or execute the query.</p>"},{"location":"databases/db2/","title":"IBM DB2","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/db2/#geniusrise_databases.db2.DB2.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the DB2 class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/db2/#geniusrise_databases.db2.DB2.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius DB2 rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--args hostname=mydb2.example.com port=50000 username=myusername password=mypassword database=mydb\n</code></pre>"},{"location":"databases/db2/#geniusrise_databases.db2.DB2.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_db2_spout:\nname: \"DB2\"\nmethod: \"fetch\"\nargs:\nhostname: \"mydb2.example.com\"\nport: 50000\nusername: \"myusername\"\npassword: \"mypassword\"\ndatabase: \"mydb\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/db2/#geniusrise_databases.db2.DB2.fetch","title":"<code>fetch(hostname, port, username, password, database)</code>","text":"<p>\ud83d\udcd6 Fetch data from a DB2 database and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>hostname</code> <code>str</code> <p>The DB2 hostname.</p> required <code>port</code> <code>int</code> <p>The DB2 port.</p> required <code>username</code> <code>str</code> <p>The DB2 username.</p> required <code>password</code> <code>str</code> <p>The DB2 password.</p> required <code>database</code> <code>str</code> <p>The DB2 database name.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the DB2 server or execute the command.</p>"},{"location":"databases/documentdb/","title":"AWS DocumentDB","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/documentdb/#geniusrise_databases.documentdb.DocumentDB.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the DocumentDB class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/documentdb/#geniusrise_databases.documentdb.DocumentDB.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius DocumentDB rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--args host=localhost port=27017 user=myuser password=mypassword database=mydb collection=mycollection query=\"{}\" page_size=100\n</code></pre>"},{"location":"databases/documentdb/#geniusrise_databases.documentdb.DocumentDB.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_documentdb_spout:\nname: \"DocumentDB\"\nmethod: \"fetch\"\nargs:\nhost: \"localhost\"\nport: 27017\nuser: \"myuser\"\npassword: \"mypassword\"\ndatabase: \"mydb\"\ncollection: \"mycollection\"\nquery: \"{}\"\npage_size: 100\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/documentdb/#geniusrise_databases.documentdb.DocumentDB.fetch","title":"<code>fetch(host, port, user, password, database, collection, query, page_size=100)</code>","text":"<p>\ud83d\udcd6 Fetch data from a DocumentDB database and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The DocumentDB host.</p> required <code>port</code> <code>int</code> <p>The DocumentDB port.</p> required <code>user</code> <code>str</code> <p>The DocumentDB user.</p> required <code>password</code> <code>str</code> <p>The DocumentDB password.</p> required <code>database</code> <code>str</code> <p>The DocumentDB database name.</p> required <code>collection</code> <code>str</code> <p>The DocumentDB collection name.</p> required <code>query</code> <code>str</code> <p>The query to execute.</p> required <code>page_size</code> <code>int</code> <p>The number of documents to fetch per page. Defaults to 100.</p> <code>100</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the DocumentDB server or execute the query.</p>"},{"location":"databases/dynamodb/","title":"AWS DynamoDB","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/dynamodb/#geniusrise_databases.dynamodb.DynamoDB.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the DynamoDB class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/dynamodb/#geniusrise_databases.dynamodb.DynamoDB.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius DynamoDB rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--args table_name=my_table page_size=100\n</code></pre>"},{"location":"databases/dynamodb/#geniusrise_databases.dynamodb.DynamoDB.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_dynamodb_spout:\nname: \"DynamoDB\"\nmethod: \"fetch\"\nargs:\ntable_name: \"my_table\"\npage_size: 100\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/dynamodb/#geniusrise_databases.dynamodb.DynamoDB.fetch","title":"<code>fetch(table_name, page_size=100)</code>","text":"<p>\ud83d\udcd6 Fetch data from a DynamoDB table and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>The DynamoDB table name.</p> required <code>page_size</code> <code>int</code> <p>The number of rows to fetch per page. Defaults to 100.</p> <code>100</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the DynamoDB or fetch the data.</p>"},{"location":"databases/elasticsearch/","title":"Elasticsearch","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/elasticsearch/#geniusrise_databases.elasticsearch.Elasticsearch.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the Elasticsearch class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/elasticsearch/#geniusrise_databases.elasticsearch.Elasticsearch.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius Elasticsearch rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--args hosts=localhost:9200 index=my_index query='{\"query\": {\"match_all\": {}}}' page_size=100\n</code></pre>"},{"location":"databases/elasticsearch/#geniusrise_databases.elasticsearch.Elasticsearch.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_elasticsearch_spout:\nname: \"Elasticsearch\"\nmethod: \"fetch\"\nargs:\nhosts: \"localhost:9200\"\nindex: \"my_index\"\nquery: '{\"query\": {\"match_all\": {}}}'\npage_size: 100\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/elasticsearch/#geniusrise_databases.elasticsearch.Elasticsearch.fetch","title":"<code>fetch(hosts, index, query, page_size=100)</code>","text":"<p>\ud83d\udcd6 Fetch data from an Elasticsearch index and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>hosts</code> <code>str</code> <p>Comma-separated list of Elasticsearch hosts.</p> required <code>index</code> <code>str</code> <p>The Elasticsearch index to query.</p> required <code>query</code> <code>str</code> <p>The Elasticsearch query in JSON format.</p> required <code>page_size</code> <code>int</code> <p>The number of documents to fetch per page. Defaults to 100.</p> <code>100</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the Elasticsearch cluster or execute the query.</p>"},{"location":"databases/firestore/","title":"Firestore","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/firestore/#geniusrise_databases.firestore.Firestore.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the Firestore class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/firestore/#geniusrise_databases.firestore.Firestore.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius Firestore rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--args project_id=my-project collection_id=my-collection\n</code></pre>"},{"location":"databases/firestore/#geniusrise_databases.firestore.Firestore.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_firestore_spout:\nname: \"Firestore\"\nmethod: \"fetch\"\nargs:\nproject_id: \"my-project\"\ncollection_id: \"my-collection\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/firestore/#geniusrise_databases.firestore.Firestore.fetch","title":"<code>fetch(project_id, collection_id)</code>","text":"<p>\ud83d\udcd6 Fetch data from a Firestore collection and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>project_id</code> <code>str</code> <p>The Google Cloud project ID.</p> required <code>collection_id</code> <code>str</code> <p>The Firestore collection ID.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the Firestore server or execute the query.</p>"},{"location":"databases/graphite/","title":"Graphite","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/graphite/#geniusrise_databases.graphite.Graphite.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the Graphite class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/graphite/#geniusrise_databases.graphite.Graphite.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius Graphite rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--args url=http://localhost:8080 target=stats_counts.myapp output_format=json from=-1h until=now\n</code></pre>"},{"location":"databases/graphite/#geniusrise_databases.graphite.Graphite.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_graphite_spout:\nname: \"Graphite\"\nmethod: \"fetch\"\nargs:\nurl: \"http://localhost:8080\"\ntarget: \"stats_counts.myapp\"\noutput_format: \"json\"\nfrom: \"-1h\"\nuntil: \"now\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/graphite/#geniusrise_databases.graphite.Graphite.fetch","title":"<code>fetch(url, target, output_format='json', from_time='-1h', until='now')</code>","text":"<p>\ud83d\udcd6 Fetch data from a Graphite database and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The Graphite API URL.</p> required <code>target</code> <code>str</code> <p>The target metric to fetch.</p> required <code>output_format</code> <code>str</code> <p>The output format. Defaults to \"json\".</p> <code>'json'</code> <code>from_time</code> <code>str</code> <p>The start time for fetching data. Defaults to \"-1h\".</p> <code>'-1h'</code> <code>until</code> <code>str</code> <p>The end time for fetching data. Defaults to \"now\".</p> <code>'now'</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the Graphite server or fetch the data.</p>"},{"location":"databases/hbase/","title":"HBase","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/hbase/#geniusrise_databases.hbase.HBase.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the HBase class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/hbase/#geniusrise_databases.hbase.HBase.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius HBase rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--args host=localhost table=my_table row_start=start row_stop=stop batch_size=100\n</code></pre>"},{"location":"databases/hbase/#geniusrise_databases.hbase.HBase.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_hbase_spout:\nname: \"HBase\"\nmethod: \"fetch\"\nargs:\nhost: \"localhost\"\ntable: \"my_table\"\nrow_start: \"start\"\nrow_stop: \"stop\"\nbatch_size: 100\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/hbase/#geniusrise_databases.hbase.HBase.fetch","title":"<code>fetch(host, table, row_start, row_stop, batch_size=100)</code>","text":"<p>\ud83d\udcd6 Fetch data from an HBase table and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The HBase host.</p> required <code>table</code> <code>str</code> <p>The HBase table name.</p> required <code>row_start</code> <code>str</code> <p>The row key to start scanning from.</p> required <code>row_stop</code> <code>str</code> <p>The row key to stop scanning at.</p> required <code>batch_size</code> <code>int</code> <p>The number of rows to fetch per batch. Defaults to 100.</p> <code>100</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the HBase server or execute the scan.</p>"},{"location":"databases/influxdb/","title":"InfluxDB","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/influxdb/#geniusrise_databases.influxdb.InfluxDB.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the InfluxDB class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/influxdb/#geniusrise_databases.influxdb.InfluxDB.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius InfluxDB rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--args host=localhost port=8086 username=myusername password=mypassword database=mydatabase\n</code></pre>"},{"location":"databases/influxdb/#geniusrise_databases.influxdb.InfluxDB.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_influxdb_spout:\nname: \"InfluxDB\"\nmethod: \"fetch\"\nargs:\nhost: \"localhost\"\nport: 8086\nusername: \"myusername\"\npassword: \"mypassword\"\ndatabase: \"mydatabase\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/influxdb/#geniusrise_databases.influxdb.InfluxDB.fetch","title":"<code>fetch(host, port, username, password, database)</code>","text":"<p>\ud83d\udcd6 Fetch data from an InfluxDB database and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The InfluxDB host.</p> required <code>port</code> <code>int</code> <p>The InfluxDB port.</p> required <code>username</code> <code>str</code> <p>The InfluxDB username.</p> required <code>password</code> <code>str</code> <p>The InfluxDB password.</p> required <code>database</code> <code>str</code> <p>The InfluxDB database name.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the InfluxDB server or execute the query.</p>"},{"location":"databases/kairosdb/","title":"KairosDB","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/kairosdb/#geniusrise_databases.kairosdb.KairosDB.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the KairosDB class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/kairosdb/#geniusrise_databases.kairosdb.KairosDB.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius KairosDB rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--args url=http://mykairosdbhost:8080/api/v1/datapoints query=\"SELECT * FROM mymetric\"\n</code></pre>"},{"location":"databases/kairosdb/#geniusrise_databases.kairosdb.KairosDB.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_kairosdb_spout:\nname: \"KairosDB\"\nmethod: \"fetch\"\nargs:\nurl: \"http://mykairosdbhost:8080/api/v1/datapoints\"\nquery: \"SELECT * FROM mymetric\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/kairosdb/#geniusrise_databases.kairosdb.KairosDB.fetch","title":"<code>fetch(url, query)</code>","text":"<p>\ud83d\udcd6 Fetch data from a KairosDB metric and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the KairosDB API endpoint.</p> required <code>query</code> <code>str</code> <p>The SQL query to execute.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the KairosDB server or execute the query.</p>"},{"location":"databases/keyspaces/","title":"AWSKeyspaces","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/keyspaces/#geniusrise_databases.keyspaces.AWSKeyspaces.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the AWS Keyspaces class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/keyspaces/#geniusrise_databases.keyspaces.AWSKeyspaces.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius AWSKeyspaces rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--args region_name=us-east-1 cluster_name=mycluster table_name=mytable\n</code></pre>"},{"location":"databases/keyspaces/#geniusrise_databases.keyspaces.AWSKeyspaces.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_aws_keyspaces_spout:\nname: \"AWSKeyspaces\"\nmethod: \"fetch\"\nargs:\nregion_name: \"us-east-1\"\ncluster_name: \"mycluster\"\ntable_name: \"mytable\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/keyspaces/#geniusrise_databases.keyspaces.AWSKeyspaces.fetch","title":"<code>fetch(region_name, cluster_name, table_name)</code>","text":"<p>\ud83d\udcd6 Fetch data from an AWS Keyspaces table and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>region_name</code> <code>str</code> <p>The AWS region name.</p> required <code>cluster_name</code> <code>str</code> <p>The AWS Keyspaces cluster name.</p> required <code>table_name</code> <code>str</code> <p>The name of the AWS Keyspaces table.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the AWS Keyspaces cluster or execute the query.</p>"},{"location":"databases/ldap/","title":"LDAP","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/ldap/#geniusrise_databases.ldap.LDAP.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the LDAP class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/ldap/#geniusrise_databases.ldap.LDAP.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius LDAP rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--args url=ldap://myldap.example.com:389 bind_dn=\"cn=admin,dc=example,dc=com\" bind_password=\"password\" search_base=\"dc=example,dc=com\" search_filter=\"(objectClass=person)\" attributes=[\"cn\", \"givenName\", \"sn\"]\n</code></pre>"},{"location":"databases/ldap/#geniusrise_databases.ldap.LDAP.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_ldap_spout:\nname: \"LDAP\"\nmethod: \"fetch\"\nargs:\nurl: \"ldap://myldap.example.com:389\"\nbind_dn: \"cn=admin,dc=example,dc=com\"\nbind_password: \"password\"\nsearch_base: \"dc=example,dc=com\"\nsearch_filter: \"(objectClass=person)\"\nattributes: [\"cn\", \"givenName\", \"sn\"]\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/ldap/#geniusrise_databases.ldap.LDAP.fetch","title":"<code>fetch(url, bind_dn, bind_password, search_base, search_filter, attributes)</code>","text":"<p>\ud83d\udcd6 Fetch data from an LDAP server and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The LDAP URL.</p> required <code>bind_dn</code> <code>str</code> <p>The DN to bind as.</p> required <code>bind_password</code> <code>str</code> <p>The password for the DN.</p> required <code>search_base</code> <code>str</code> <p>The search base.</p> required <code>search_filter</code> <code>str</code> <p>The search filter.</p> required <code>attributes</code> <code>list</code> <p>The list of attributes to retrieve.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the LDAP server or execute the search.</p>"},{"location":"databases/memsql/","title":"MemSQL","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/memsql/#geniusrise_databases.memsql.MemSQL.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the MemSQL class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/memsql/#geniusrise_databases.memsql.MemSQL.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius MemSQL rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--args host=mymemsqlhost user=myuser password=&lt;PASSWORD&gt; database=mydatabase query=\"SELECT * FROM mytable\"\n</code></pre>"},{"location":"databases/memsql/#geniusrise_databases.memsql.MemSQL.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_memsql_spout:\nname: \"MemSQL\"\nmethod: \"fetch\"\nargs:\nhost: \"mymemsqlhost\"\nuser: \"myuser\"\npassword: \"&lt;PASSWORD&gt;\"\ndatabase: \"mydatabase\"\nquery: \"SELECT * FROM mytable\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/memsql/#geniusrise_databases.memsql.MemSQL.fetch","title":"<code>fetch(host, user, password, database, query)</code>","text":"<p>\ud83d\udcd6 Fetch data from a MemSQL database and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The MemSQL host.</p> required <code>user</code> <code>str</code> <p>The MemSQL user.</p> required <code>password</code> <code>str</code> <p>The MemSQL password.</p> required <code>database</code> <code>str</code> <p>The MemSQL database name.</p> required <code>query</code> <code>str</code> <p>The SQL query to execute.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the MemSQL server or execute the query.</p>"},{"location":"databases/mongodb/","title":"MongoDB","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/mongodb/#geniusrise_databases.mongodb.MongoDB.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the MongoDB class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/mongodb/#geniusrise_databases.mongodb.MongoDB.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius MongoDB rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--args host=localhost port=27017 username=myusername password=mypassword database=mydatabase collection=mycollection\n</code></pre>"},{"location":"databases/mongodb/#geniusrise_databases.mongodb.MongoDB.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_mongodb_spout:\nname: \"MongoDB\"\nmethod: \"fetch\"\nargs:\nhost: \"localhost\"\nport: 27017\nusername: \"myusername\"\npassword: \"mypassword\"\ndatabase: \"mydatabase\"\ncollection: \"mycollection\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/mongodb/#geniusrise_databases.mongodb.MongoDB.fetch","title":"<code>fetch(host, port, username, password, database, collection)</code>","text":"<p>\ud83d\udcd6 Fetch data from a MongoDB database and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The MongoDB host.</p> required <code>port</code> <code>int</code> <p>The MongoDB port.</p> required <code>username</code> <code>str</code> <p>The MongoDB username.</p> required <code>password</code> <code>str</code> <p>The MongoDB password.</p> required <code>database</code> <code>str</code> <p>The MongoDB database name.</p> required <code>collection</code> <code>str</code> <p>The MongoDB collection name.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the MongoDB server or execute the query.</p>"},{"location":"databases/mysql/","title":"MySQL","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/mysql/#geniusrise_databases.mysql.MySQL.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the MySQL class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/mysql/#geniusrise_databases.mysql.MySQL.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius MySQL rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--args host=localhost port=3306 user=root password=root database=mydb query=\"SELECT * FROM table\" page_size=100\n</code></pre>"},{"location":"databases/mysql/#geniusrise_databases.mysql.MySQL.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_mysql_spout:\nname: \"MySQL\"\nmethod: \"fetch\"\nargs:\nhost: \"localhost\"\nport: 3306\nuser: \"root\"\npassword: \"root\"\ndatabase: \"mydb\"\nquery: \"SELECT * FROM table\"\npage_size: 100\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/mysql/#geniusrise_databases.mysql.MySQL.fetch","title":"<code>fetch(host, port, user, password, database, query, page_size=100)</code>","text":"<p>\ud83d\udcd6 Fetch data from a MySQL database and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The MySQL host.</p> required <code>port</code> <code>int</code> <p>The MySQL port.</p> required <code>user</code> <code>str</code> <p>The MySQL user.</p> required <code>password</code> <code>str</code> <p>The MySQL password.</p> required <code>database</code> <code>str</code> <p>The MySQL database name.</p> required <code>query</code> <code>str</code> <p>The SQL query to execute.</p> required <code>page_size</code> <code>int</code> <p>The number of rows to fetch per page. Defaults to 100.</p> <code>100</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the MySQL server or execute the query.</p>"},{"location":"databases/neo4j/","title":"Neo4j","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/neo4j/#geniusrise_databases.neo4j.Neo4j.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the Neo4j class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/neo4j/#geniusrise_databases.neo4j.Neo4j.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius Neo4j rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--args host=localhost port=7687 username=myusername password=mypassword\n</code></pre>"},{"location":"databases/neo4j/#geniusrise_databases.neo4j.Neo4j.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_neo4j_spout:\nname: \"Neo4j\"\nmethod: \"fetch\"\nargs:\nhost: \"localhost\"\nport: 7687\nusername: \"myusername\"\npassword: \"mypassword\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/neo4j/#geniusrise_databases.neo4j.Neo4j.fetch","title":"<code>fetch(host, port, username, password)</code>","text":"<p>\ud83d\udcd6 Fetch data from a Neo4j database and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The Neo4j host.</p> required <code>port</code> <code>int</code> <p>The Neo4j port.</p> required <code>username</code> <code>str</code> <p>The Neo4j username.</p> required <code>password</code> <code>str</code> <p>The Neo4j password.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the Neo4j server or execute the query.</p>"},{"location":"databases/nuodb/","title":"NuoDB","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/nuodb/#geniusrise_databases.nuodb.NuoDB.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the NuoDB class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/nuodb/#geniusrise_databases.nuodb.NuoDB.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius NuoDB rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--args url=http://mynuodbhost:8080/v1/statement query=\"SELECT * FROM mytable\"\n</code></pre>"},{"location":"databases/nuodb/#geniusrise_databases.nuodb.NuoDB.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_nuodb_spout:\nname: \"NuoDB\"\nmethod: \"fetch\"\nargs:\nurl: \"http://mynuodbhost:8080/v1/statement\"\nquery: \"SELECT * FROM mytable\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/nuodb/#geniusrise_databases.nuodb.NuoDB.fetch","title":"<code>fetch(url, query)</code>","text":"<p>\ud83d\udcd6 Fetch data from a NuoDB table and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the NuoDB API endpoint.</p> required <code>query</code> <code>str</code> <p>The SQL query to execute.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the NuoDB server or execute the query.</p>"},{"location":"databases/opentsdb/","title":"OpenTSDB","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/opentsdb/#geniusrise_databases.opentsdb.OpenTSDB.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the OpenTSDB class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/opentsdb/#geniusrise_databases.opentsdb.OpenTSDB.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius OpenTSDB rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--args host=http://localhost:4242\n</code></pre>"},{"location":"databases/opentsdb/#geniusrise_databases.opentsdb.OpenTSDB.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_opentsdb_spout:\nname: \"OpenTSDB\"\nmethod: \"fetch\"\nargs:\nhost: \"http://localhost:4242\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/opentsdb/#geniusrise_databases.opentsdb.OpenTSDB.fetch","title":"<code>fetch(host)</code>","text":"<p>\ud83d\udcd6 Fetch data from an OpenTSDB database and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The URL of the OpenTSDB instance.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the OpenTSDB server or execute the query.</p>"},{"location":"databases/oracle/","title":"Oracle","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/oracle/#geniusrise_databases.oracle.Oracle.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the OracleSQL class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/oracle/#geniusrise_databases.oracle.Oracle.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius OracleSQL rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--args server=localhost port=1521 service_name=myservice user=myuser password=mypassword\n</code></pre>"},{"location":"databases/oracle/#geniusrise_databases.oracle.Oracle.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_oraclesql_spout:\nname: \"OracleSQL\"\nmethod: \"fetch\"\nargs:\nserver: \"localhost\"\nport: 1521\nservice_name: \"myservice\"\nuser: \"myuser\"\npassword: \"mypassword\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/oracle/#geniusrise_databases.oracle.Oracle.fetch","title":"<code>fetch(server, port, service_name, user, password, query)</code>","text":"<p>\ud83d\udcd6 Fetch data from an Oracle SQL database and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>server</code> <code>str</code> <p>The Oracle SQL server.</p> required <code>port</code> <code>int</code> <p>The Oracle SQL port.</p> required <code>service_name</code> <code>str</code> <p>The Oracle service name.</p> required <code>user</code> <code>str</code> <p>The Oracle user.</p> required <code>password</code> <code>str</code> <p>The Oracle password.</p> required <code>query</code> <code>str</code> <p>The SQL query to execute.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the Oracle SQL server or execute the query.</p>"},{"location":"databases/postgres/","title":"PostgreSQL","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/postgres/#geniusrise_databases.postgres.PostgreSQL.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the PostgreSQL class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/postgres/#geniusrise_databases.postgres.PostgreSQL.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius PostgreSQL rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--args host=localhost port=5432 user=postgres password=postgres database=mydb query=\"SELECT * FROM table\" page_size=100\n</code></pre>"},{"location":"databases/postgres/#geniusrise_databases.postgres.PostgreSQL.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_postgresql_spout:\nname: \"PostgreSQL\"\nmethod: \"fetch\"\nargs:\nhost: \"localhost\"\nport: 5432\nuser: \"postgres\"\npassword: \"postgres\"\ndatabase: \"mydb\"\nquery: \"SELECT * FROM table\"\npage_size: 100\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/postgres/#geniusrise_databases.postgres.PostgreSQL.fetch","title":"<code>fetch(host, port, user, password, database, query, page_size=100)</code>","text":"<p>\ud83d\udcd6 Fetch data from a PostgreSQL database and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The PostgreSQL host.</p> required <code>port</code> <code>int</code> <p>The PostgreSQL port.</p> required <code>user</code> <code>str</code> <p>The PostgreSQL user.</p> required <code>password</code> <code>str</code> <p>The PostgreSQL password.</p> required <code>database</code> <code>str</code> <p>The PostgreSQL database name.</p> required <code>query</code> <code>str</code> <p>The SQL query to execute.</p> required <code>page_size</code> <code>int</code> <p>The number of rows to fetch per page. Defaults to 100.</p> <code>100</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the PostgreSQL server or execute the query.</p>"},{"location":"databases/presto/","title":"Presto","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/presto/#geniusrise_databases.presto.Presto.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the Presto class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/presto/#geniusrise_databases.presto.Presto.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius Presto rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--args host=mypresto.example.com username=myusername password=mypassword catalog=mycatalog schema=myschema table=mytable\n</code></pre>"},{"location":"databases/presto/#geniusrise_databases.presto.Presto.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_presto_spout:\nname: \"Presto\"\nmethod: \"fetch\"\nargs:\nhost: \"mypresto.example.com\"\nusername: \"myusername\"\npassword: \"mypassword\"\ncatalog: \"mycatalog\"\nschema: \"myschema\"\ntable: \"mytable\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/presto/#geniusrise_databases.presto.Presto.fetch","title":"<code>fetch(host, username, password, catalog, schema, table)</code>","text":"<p>\ud83d\udcd6 Fetch data from a Presto table and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The Presto host.</p> required <code>username</code> <code>str</code> <p>The Presto username.</p> required <code>password</code> <code>str</code> <p>The Presto password.</p> required <code>catalog</code> <code>str</code> <p>The Presto catalog name.</p> required <code>schema</code> <code>str</code> <p>The Presto schema name.</p> required <code>table</code> <code>str</code> <p>The name of the Presto table.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the Presto server or execute the command.</p>"},{"location":"databases/redis/","title":"Redis","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/redis/#geniusrise_databases.redis.Redis.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the Redis class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/redis/#geniusrise_databases.redis.Redis.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius Redis rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--args host=localhost port=6379 password=mypassword database=0\n</code></pre>"},{"location":"databases/redis/#geniusrise_databases.redis.Redis.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_redis_spout:\nname: \"Redis\"\nmethod: \"fetch\"\nargs:\nhost: \"localhost\"\nport: 6379\npassword: \"mypassword\"\ndatabase: 0\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/redis/#geniusrise_databases.redis.Redis.fetch","title":"<code>fetch(host, port, password, database)</code>","text":"<p>\ud83d\udcd6 Fetch data from a Redis database and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The Redis host.</p> required <code>port</code> <code>int</code> <p>The Redis port.</p> required <code>password</code> <code>str</code> <p>The Redis password.</p> required <code>database</code> <code>int</code> <p>The Redis database number.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the Redis server or execute the command.</p>"},{"location":"databases/riak/","title":"Riak","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/riak/#geniusrise_databases.riak.Riak.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the Riak class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/riak/#geniusrise_databases.riak.Riak.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius Riak rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--args host=localhost port=8098\n</code></pre>"},{"location":"databases/riak/#geniusrise_databases.riak.Riak.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_riak_spout:\nname: \"Riak\"\nmethod: \"fetch\"\nargs:\nhost: \"localhost\"\nport: 8098\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/riak/#geniusrise_databases.riak.Riak.fetch","title":"<code>fetch(host, port)</code>","text":"<p>\ud83d\udcd6 Fetch data from a Riak database and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The Riak host.</p> required <code>port</code> <code>int</code> <p>The Riak port.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the Riak server or execute the query.</p>"},{"location":"databases/spanner/","title":"Couchbase","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/spanner/#geniusrise_databases.spanner.Spanner.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the Spanner class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/spanner/#geniusrise_databases.spanner.Spanner.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius Spanner rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--project_id my_project_id instance_id=my_instance database_id=my_database table_id=my_table\n</code></pre>"},{"location":"databases/spanner/#geniusrise_databases.spanner.Spanner.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_spanner_spout:\nname: \"Spanner\"\nmethod: \"fetch\"\nargs:\nproject_id: \"my_project_id\"\ninstance_id: \"my_instance\"\ndatabase_id: \"my_database\"\ntable_id: \"my_table\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/spanner/#geniusrise_databases.spanner.Spanner.fetch","title":"<code>fetch(project_id, instance_id, database_id, table_id)</code>","text":"<p>\ud83d\udcd6 Fetch data from a Spanner database and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>project_id</code> <code>str</code> <p>The Google Cloud project ID.</p> required <code>instance_id</code> <code>str</code> <p>The Spanner instance ID.</p> required <code>database_id</code> <code>str</code> <p>The Spanner database ID.</p> required <code>table_id</code> <code>str</code> <p>The Spanner table ID.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the Spanner database or execute the query.</p>"},{"location":"databases/sql_server/","title":"SQLServer","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/sql_server/#geniusrise_databases.sql_server.SQLServer.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the SQLServer class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/sql_server/#geniusrise_databases.sql_server.SQLServer.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius SQLServer rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--args server=localhost port=1433 user=myuser password=mypassword database=mydatabase query=\"SELECT * FROM mytable\"\n</code></pre>"},{"location":"databases/sql_server/#geniusrise_databases.sql_server.SQLServer.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_sqlserver_spout:\nname: \"SQLServer\"\nmethod: \"fetch\"\nargs:\nserver: \"localhost\"\nport: 1433\nuser: \"myuser\"\npassword: \"mypassword\"\ndatabase: \"mydatabase\"\nquery: \"SELECT * FROM mytable\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/sql_server/#geniusrise_databases.sql_server.SQLServer.fetch","title":"<code>fetch(server, port, user, password, database, query)</code>","text":"<p>\ud83d\udcd6 Fetch data from a SQL Server database and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>server</code> <code>str</code> <p>The SQL Server host.</p> required <code>port</code> <code>int</code> <p>The SQL Server port.</p> required <code>user</code> <code>str</code> <p>The SQL Server user.</p> required <code>password</code> <code>str</code> <p>The SQL Server password.</p> required <code>database</code> <code>str</code> <p>The SQL Server database name.</p> required <code>query</code> <code>str</code> <p>The SQL query to execute.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the SQL Server server or execute the query.</p>"},{"location":"databases/sqlite/","title":"SQLite","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/sqlite/#geniusrise_databases.sqlite.SQLite.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the SQLite class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/sqlite/#geniusrise_databases.sqlite.SQLite.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius SQLite rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--args s3_bucket=my_s3_bucket s3_key=mydb.sqlite query=\"SELECT * FROM table\" page_size=100\n</code></pre>"},{"location":"databases/sqlite/#geniusrise_databases.sqlite.SQLite.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_sqlite_spout:\nname: \"SQLite\"\nmethod: \"fetch\"\nargs:\ns3_bucket: \"my_s3_bucket\"\ns3_key: \"mydb.sqlite\"\nquery: \"SELECT * FROM table\"\npage_size: 100\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/sqlite/#geniusrise_databases.sqlite.SQLite.fetch","title":"<code>fetch(s3_bucket, s3_key, query, page_size=100)</code>","text":"<p>\ud83d\udcd6 Fetch data from an SQLite database and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>s3_bucket</code> <code>str</code> <p>The S3 bucket containing the SQLite database.</p> required <code>s3_key</code> <code>str</code> <p>The S3 key for the SQLite database.</p> required <code>query</code> <code>str</code> <p>The SQL query to execute.</p> required <code>page_size</code> <code>int</code> <p>The number of rows to fetch per page. Defaults to 100.</p> <code>100</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the SQLite database or execute the query.</p>"},{"location":"databases/sybase/","title":"Sybase","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/sybase/#geniusrise_databases.sybase.Sybase.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the Sybase class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/sybase/#geniusrise_databases.sybase.Sybase.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius Sybase rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--args host=localhost port=5000 user=sa password=secret database=mydb query=\"SELECT * FROM table\" page_size=100\n</code></pre>"},{"location":"databases/sybase/#geniusrise_databases.sybase.Sybase.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_sybase_spout:\nname: \"Sybase\"\nmethod: \"fetch\"\nargs:\nhost: \"localhost\"\nport: 5000\nuser: \"sa\"\npassword: \"secret\"\ndatabase: \"mydb\"\nquery: \"SELECT * FROM table\"\npage_size: 100\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/sybase/#geniusrise_databases.sybase.Sybase.fetch","title":"<code>fetch(host, port, user, password, database, query, page_size=100)</code>","text":"<p>\ud83d\udcd6 Fetch data from a Sybase database and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The Sybase host.</p> required <code>port</code> <code>int</code> <p>The Sybase port.</p> required <code>user</code> <code>str</code> <p>The Sybase user.</p> required <code>password</code> <code>str</code> <p>The Sybase password.</p> required <code>database</code> <code>str</code> <p>The Sybase database name.</p> required <code>query</code> <code>str</code> <p>The SQL query to execute.</p> required <code>page_size</code> <code>int</code> <p>The number of rows to fetch per page. Defaults to 100.</p> <code>100</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the Sybase server or execute the query.</p>"},{"location":"databases/teradata/","title":"Teradata","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/teradata/#geniusrise_databases.teradata.Teradata.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the Teradata class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/teradata/#geniusrise_databases.teradata.Teradata.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius Teradata rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--args host=myteradata.example.com username=myusername password=mypassword database=mydb\n</code></pre>"},{"location":"databases/teradata/#geniusrise_databases.teradata.Teradata.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_teradata_spout:\nname: \"Teradata\"\nmethod: \"fetch\"\nargs:\nhost: \"myteradata.example.com\"\nusername: \"myusername\"\npassword: \"mypassword\"\ndatabase: \"mydb\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/teradata/#geniusrise_databases.teradata.Teradata.fetch","title":"<code>fetch(host, username, password, database)</code>","text":"<p>\ud83d\udcd6 Fetch data from a Teradata database and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The Teradata host.</p> required <code>username</code> <code>str</code> <p>The Teradata username.</p> required <code>password</code> <code>str</code> <p>The Teradata password.</p> required <code>database</code> <code>str</code> <p>The Teradata database name.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the Teradata server or execute the command.</p>"},{"location":"databases/tidb/","title":"TiDB","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/tidb/#geniusrise_databases.tidb.TiDB.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the TiDB class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/tidb/#geniusrise_databases.tidb.TiDB.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius TiDB rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--args host=localhost port=4000 user=root password=root database=mydb query=\"SELECT * FROM table\" page_size=100\n</code></pre>"},{"location":"databases/tidb/#geniusrise_databases.tidb.TiDB.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_tidb_spout:\nname: \"TiDB\"\nmethod: \"fetch\"\nargs:\nhost: \"localhost\"\nport: 4000\nuser: \"root\"\npassword: \"root\"\ndatabase: \"mydb\"\nquery: \"SELECT * FROM table\"\npage_size: 100\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/tidb/#geniusrise_databases.tidb.TiDB.fetch","title":"<code>fetch(host, port, user, password, database, query, page_size=100)</code>","text":"<p>\ud83d\udcd6 Fetch data from a TiDB database and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The TiDB host.</p> required <code>port</code> <code>int</code> <p>The TiDB port.</p> required <code>user</code> <code>str</code> <p>The TiDB user.</p> required <code>password</code> <code>str</code> <p>The TiDB password.</p> required <code>database</code> <code>str</code> <p>The TiDB database name.</p> required <code>query</code> <code>str</code> <p>The SQL query to execute.</p> required <code>page_size</code> <code>int</code> <p>The number of rows to fetch per page. Defaults to 100.</p> <code>100</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the TiDB server or execute the query.</p>"},{"location":"databases/timescaledb/","title":"TimescaleDB","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/timescaledb/#geniusrise_databases.timescaledb.TimescaleDB.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the TimescaleDB class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/timescaledb/#geniusrise_databases.timescaledb.TimescaleDB.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius TimescaleDB rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--args host=localhost port=5432 user=postgres password=postgres database=mydb query=\"SELECT * FROM hypertable\" page_size=100\n</code></pre>"},{"location":"databases/timescaledb/#geniusrise_databases.timescaledb.TimescaleDB.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_timescaledb_spout:\nname: \"TimescaleDB\"\nmethod: \"fetch\"\nargs:\nhost: \"localhost\"\nport: 5432\nuser: \"postgres\"\npassword: \"postgres\"\ndatabase: \"mydb\"\nquery: \"SELECT * FROM hypertable\"\npage_size: 100\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/timescaledb/#geniusrise_databases.timescaledb.TimescaleDB.fetch","title":"<code>fetch(host, port, user, password, database, query, page_size=100)</code>","text":"<p>\ud83d\udcd6 Fetch data from a TimescaleDB hypertable and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The TimescaleDB host.</p> required <code>port</code> <code>int</code> <p>The TimescaleDB port.</p> required <code>user</code> <code>str</code> <p>The TimescaleDB user.</p> required <code>password</code> <code>str</code> <p>The TimescaleDB password.</p> required <code>database</code> <code>str</code> <p>The TimescaleDB database name.</p> required <code>query</code> <code>str</code> <p>The SQL query to execute.</p> required <code>page_size</code> <code>int</code> <p>The number of rows to fetch per page. Defaults to 100.</p> <code>100</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the TimescaleDB server or execute the query.</p>"},{"location":"databases/vertica/","title":"Vertica","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/vertica/#geniusrise_databases.vertica.Vertica.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the Vertica class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/vertica/#geniusrise_databases.vertica.Vertica.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius Vertica rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--host my_host port=5433 user=my_user password=my_password database=my_database query=\"SELECT * FROM my_table\"\n</code></pre>"},{"location":"databases/vertica/#geniusrise_databases.vertica.Vertica.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_vertica_spout:\nname: \"Vertica\"\nmethod: \"fetch\"\nargs:\nhost: \"my_host\"\nport: 5433\nuser: \"my_user\"\npassword: \"my_password\"\ndatabase: \"my_database\"\nquery: \"SELECT * FROM my_table\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/vertica/#geniusrise_databases.vertica.Vertica.fetch","title":"<code>fetch(host, port, user, password, database, query)</code>","text":"<p>\ud83d\udcd6 Fetch data from a Vertica database and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The Vertica host.</p> required <code>port</code> <code>int</code> <p>The Vertica port.</p> required <code>user</code> <code>str</code> <p>The Vertica user.</p> required <code>password</code> <code>str</code> <p>The Vertica password.</p> required <code>database</code> <code>str</code> <p>The Vertica database name.</p> required <code>query</code> <code>str</code> <p>The SQL query to execute.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the Vertica server or execute the query.</p>"},{"location":"databases/voltdb/","title":"VoltDB","text":"<p>             Bases: <code>Spout</code></p>"},{"location":"databases/voltdb/#geniusrise_databases.voltdb.VoltDB.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the VoltDB class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"databases/voltdb/#geniusrise_databases.voltdb.VoltDB.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius VoltDB rise \\\nbatch \\\n--output_s3_bucket my_bucket \\\n--output_s3_folder s3/folder \\\nnone \\\nfetch \\\n--args host=localhost port=21212 username=myuser password=&lt;PASSWORD&gt;\n</code></pre>"},{"location":"databases/voltdb/#geniusrise_databases.voltdb.VoltDB.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_voltdb_spout:\nname: \"VoltDB\"\nmethod: \"fetch\"\nargs:\nhost: \"localhost\"\nport: 21212\nusername: \"myuser\"\npassword: \"&lt;PASSWORD&gt;\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/folder\"\n</code></pre>"},{"location":"databases/voltdb/#geniusrise_databases.voltdb.VoltDB.fetch","title":"<code>fetch(host, port, username, password)</code>","text":"<p>\ud83d\udcd6 Fetch data from a VoltDB database and save it in batch.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The VoltDB host.</p> required <code>port</code> <code>int</code> <p>The VoltDB port.</p> required <code>username</code> <code>str</code> <p>The VoltDB username.</p> required <code>password</code> <code>str</code> <p>The VoltDB password.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the VoltDB server or execute the query.</p>"},{"location":"guides/404/","title":"WIP","text":"<p>Working on it! \ud83d\ude05</p>"},{"location":"guides/architecture/","title":"Architecture","text":""},{"location":"guides/architecture/#introduction","title":"Introduction","text":"<p>The Geniusrise framework is designed to provide a modular, scalable, and interoperable system for orchestrating machine learning workflows, particularly in the context of Large Language Models (LLMs). The architecture is built around the core concept of a <code>Task</code>, which represents a discrete unit of work. This document provides an overview of the architecture, detailing the primary components and their interactions.</p>"},{"location":"guides/architecture/#system-overview","title":"System Overview","text":"<p>The Geniusrise framework is composed of several key components:</p> <ol> <li>Tasks: The fundamental units of work.</li> <li>State Managers: Responsible for monitoring and managing the state of tasks.</li> <li>Data Managers: Oversee the input and output data associated with tasks.</li> <li>Model Managers: Handle model operations, ensuring efficient management.</li> <li>Runners: Wrappers for executing tasks on various platforms.</li> <li>Spouts and Bolts: Specialized tasks for data ingestion and processing.</li> </ol>"},{"location":"guides/architecture/#tasks","title":"Tasks","text":"<p>A task is the fundamental unit of work in the Geniusrise framework. It represents a specific operation or computation and can run for an arbitrary amount of time, performing any amount of work.</p> <pre>8065e727-64f5-48c8-95e1-4f00d330bbfb</pre>"},{"location":"guides/architecture/#state-managers","title":"State Managers","text":"<p>State Managers play a pivotal role in maintaining the state of tasks. They ensure that the progress and status of tasks are tracked, especially in distributed environments. Geniusrise offers various types of State Managers:</p> <ol> <li>DynamoDBStateManager: Interfaces with Amazon DynamoDB.</li> <li>InMemoryStateManager: Maintains state within the application's memory.</li> <li>PostgresStateManager: Interfaces with PostgreSQL databases.</li> <li>RedisStateManager: Interfaces with Redis in-memory data structure store.</li> </ol> <p>State Managers store data in various locations, allowing organizations to connect dashboards to these storage systems for real-time monitoring and analytics. This centralized storage and reporting mechanism ensures that stakeholders have a unified view of task states.</p> <pre>c91ecff4-99a8-4fdd-bb6d-2ad42c09db81</pre>"},{"location":"guides/architecture/#data-managers","title":"Data Managers","text":"<p>Data Managers are responsible for handling the input and output data for tasks. They implement various data operations methods that tasks can leverage to ingest or save data during their runs. Data Managers can be categorized based on their function and data processing type:</p> <ol> <li>BatchInputConfig: Manages batch input data.</li> <li>BatchOutputConfig: Manages batch output data.</li> <li>StreamingInputConfig: Manages streaming input data.</li> <li>StreamingOutputConfig: Manages streaming output data.</li> </ol> <p>Data Managers manage data partitioning for both batch and streaming data. By adhering to common data patterns, they enable the system's components to operate independently, fostering the creation of intricate networks of tasks. This independence, while allowing for flexibility and scalability, ensures that cascading failures in one component don't necessarily compromise the entire system.</p> <pre>e4d56acd-b644-45bc-9ade-bc3bc40e9867</pre>"},{"location":"guides/architecture/#model-managers","title":"Model Managers","text":"<p>Model Managers oversee model operations, ensuring that models are saved, loaded, and managed. They can be of two primary types:</p> <ol> <li>S3ModelManager: Interfaces with Amazon S3 for model storage.</li> <li>WANDBModelManager: Interfaces with Weights &amp; Biases for model versioning.</li> <li>GitModelManager: Interfaces with Git repositories for versioning of models.</li> </ol> <pre>ff8949d8-8492-4777-82ce-9bf7d1c68196</pre>"},{"location":"guides/architecture/#spouts-and-bolts","title":"Spouts and Bolts","text":"<p>At the heart of the Geniusrise framework are two primary component types: spouts and bolts.</p> <ol> <li> <p>Spouts: These are tasks responsible for ingesting data from various sources. Depending on the output type, spouts can either produce streaming output or batch output.</p> <ol> <li>Batch: Runs periodically, Produces data as a batch output.</li> <li>Stream: Runs forever, produces data into a streaming output.</li> </ol> </li> <li> <p>Bolts: Bolts are tasks that take in data, process it, and produce output. They can be categorized based on their input and output types:</p> <ol> <li>Stream-Stream: Reads streaming data and produces streaming output.</li> <li>Stream-Batch: Reads streaming data and produces batch output.</li> <li>Batch-Stream: Reads batch data and produces streaming output.</li> <li>Batch-Batch: Reads batch data and produces batch output.</li> </ol> </li> </ol> <pre>308b50c8-3c96-4a5f-8d76-a51543a63bb1</pre>"},{"location":"guides/architecture/#runners","title":"Runners","text":"<p>Runners are the backbone of the Geniusrise framework, ensuring that tasks are executed seamlessly across various platforms. They encapsulate the environment and resources required for task execution, abstracting away the underlying complexities. Geniusrise offers the following runners:</p> <ol> <li>Local Runner: Executes tasks directly on a local machine, ideal for development and testing.</li> <li>Docker Runner: Runs tasks within Docker containers, ensuring a consistent and isolated environment.</li> <li>Kubernetes Runner: Deploys tasks on Kubernetes clusters, leveraging its scalability and orchestration capabilities.</li> <li>Airflow Runner: Integrates with Apache Airflow, allowing for complex workflow orchestration and scheduling.</li> <li>ECS Runner: Executes tasks on AWS ECS, providing a managed container service.</li> <li>Batch Runner: Optimized for batch computing workloads on platforms like AWS Batch.</li> </ol>"},{"location":"guides/cli/","title":"CLI","text":"<ul> <li>DESCRIPTION</li> <li>Spouts<ul> <li>Command: genius TestSpoutCtlSpout</li> <li>Command: genius TestSpoutCtlSpout rise</li> <li>Command: genius TestSpoutCtlSpout deploy</li> <li>Command: genius TestSpoutCtlSpout help</li> </ul> </li> <li>Bolts<ul> <li>Command: genius TestBoltCtlBolt</li> <li>Command: genius TestBoltCtlBolt rise</li> <li>Command: genius TestBoltCtlBolt deploy</li> <li>Command: genius TestBoltCtlBolt help</li> </ul> </li> <li>Deployment<ul> <li>Command: genius rise</li> <li>Command: genius rise up</li> </ul> </li> <li>Kubernetes Pods<ul> <li>Command: genius pod</li> <li>Command: genius pod status</li> <li>Command: genius pod show</li> <li>Command: genius pod describe</li> <li>Command: genius pod logs</li> </ul> </li> <li>Kubernetes Deployments<ul> <li>Command: genius deployment</li> <li>Command: genius deployment create</li> <li>Command: genius deployment scale</li> <li>Command: genius deployment describe</li> <li>Command: genius deployment show</li> <li>Command: genius deployment delete</li> <li>Command: genius deployment status</li> </ul> </li> <li>Kubernetes Services<ul> <li>Command: genius service</li> <li>Command: genius service create</li> <li>Command: genius service delete</li> <li>Command: genius service describe</li> <li>Command: genius service show</li> </ul> </li> <li>Kubernetes Jobs<ul> <li>Command: genius job</li> <li>Command: genius job create</li> <li>Command: genius job delete</li> <li>Command: genius job status</li> <li>Kubernetes Cron Jobs</li> <li>Command: genius cron_job</li> <li>Command: genius cron_job create_cronjob</li> <li>Command: genius cron_job delete_cronjob</li> <li>Command: genius cron_job get_cronjob_status</li> </ul> </li> <li>Packaging<ul> <li>Command: genius docker package</li> </ul> </li> <li>Miscellaneous<ul> <li>Command: genius plugins</li> <li>Command: genius list</li> </ul> </li> </ul>"},{"location":"guides/cli/#description","title":"DESCRIPTION","text":"<p>Geniusrise</p> <p>POSITIONAL ARGUMENTS</p> <p>genius TestSpoutCtlSpout</p> <p>:   Manage spout TestSpoutCtlSpout.</p> <p>genius TestBoltCtlBolt</p> <p>:   Manage bolt TestBoltCtlBolt.</p> <p>genius rise</p> <p>:   Manage spouts and bolts with a YAML file.</p> <p>genius docker</p> <p>:   Package this application into a Docker image.</p> <p>genius pod</p> <p>:   Manage spouts and bolts as kubernetes pod</p> <p>genius deployment</p> <p>:   Manage spouts and bolts as kubernetes deployment</p> <p>genius service</p> <p>:   Manage spouts and bolts as kubernetes service</p> <p>genius job</p> <p>:   Manage spouts and bolts as kubernetes job</p> <p>genius cron_job</p> <p>:   Manage spouts and bolts as kubernetes cron_job</p> <p>genius plugins</p> <p>:   Print help for all spouts and bolts.</p> <p>genius list</p> <p>:   List all discovered spouts and bolts.</p>"},{"location":"guides/cli/#spouts","title":"Spouts","text":""},{"location":"guides/cli/#command-genius-testspoutctlspout","title":"Command: genius TestSpoutCtlSpout","text":"<p>Usage: genius TestSpoutCtlSpout [-h] {rise,deploy,help} ...</p> <p>POSITIONAL ARGUMENTS genius TestSpoutCtlSpout</p> <p>genius TestSpoutCtlSpout rise</p> <p>:   Run a spout locally.</p> <p>genius TestSpoutCtlSpout deploy</p> <p>:   Run a spout remotely.</p> <p>genius TestSpoutCtlSpout help</p> <p>:   Print help for the spout.</p>"},{"location":"guides/cli/#command-genius-testspoutctlspout-rise","title":"Command: genius TestSpoutCtlSpout rise","text":"<p>Usage: genius TestSpoutCtlSpout rise [-h] [--buffer_size BUFFER_SIZE] [--output_folder OUTPUT_FOLDER] [--output_kafka_topic OUTPUT_KAFKA_TOPIC] [--output_kafka_cluster_connection_string OUTPUT_KAFKA_CLUSTER_CONNECTION_STRING] [--output_s3_bucket OUTPUT_S3_BUCKET] [--output_s3_folder OUTPUT_S3_FOLDER] [--redis_host REDIS_HOST] [--redis_port REDIS_PORT] [--redis_db REDIS_DB] [--postgres_host POSTGRES_HOST] [--postgres_port POSTGRES_PORT] [--postgres_user POSTGRES_USER] [--postgres_password POSTGRES_PASSWORD] [--postgres_database POSTGRES_DATABASE] [--postgres_table POSTGRES_TABLE] [--dynamodb_table_name DYNAMODB_TABLE_NAME] [--dynamodb_region_name DYNAMODB_REGION_NAME] [--prometheus_gateway PROMETHEUS_GATEWAY] [--args ...] {batch,streaming,stream_to_batch} {none,redis,postgres,dynamodb,prometheus} method_name</p> <p>{batch,streaming,stream_to_batch}</p> <p>:   Choose the type of output data: batch or streaming.</p> <p>{none,redis,postgres,dynamodb,prometheus}</p> <p>:   Select the type of state manager: none, redis, postgres, or     dynamodb.</p> <p>method_name</p> <p>:   The name of the method to execute on the spout.</p> <p>Options genius TestSpoutCtlSpout rise</p> <p>--buffer_size BUFFER_SIZE:   Specify the size of the buffer. --output_folder OUTPUT_FOLDER:   Specify the directory where output files should be stored     temporarily</p> <p>--output_kafka_topic OUTPUT_KAFKA_TOPIC:   Kafka output topic for streaming spouts. --output_kafka_cluster_connection_string OUTPUT_KAFKA_CLUSTER_CONNECTION_STRING:   Kafka connection string for streaming spouts. --output_s3_bucket OUTPUT_S3_BUCKET:   Provide the name of the S3 bucket for output storage. --output_s3_folder OUTPUT_S3_FOLDER:   Indicate the S3 folder for output storage. --redis_host REDIS_HOST:   Enter the host address for the Redis server. --redis_port REDIS_PORT:   Enter the port number for the Redis server. --redis_db REDIS_DB:   Specify the Redis database to be used. --postgres_host POSTGRES_HOST:   Enter the host address for the PostgreSQL server. --postgres_port POSTGRES_PORT:   Enter the port number for the PostgreSQL server. --postgres_user POSTGRES_USER:   Provide the username for the PostgreSQL server. --postgres_password POSTGRES_PASSWORD:   Provide the password for the PostgreSQL server. --postgres_database POSTGRES_DATABASE:   Specify the PostgreSQL database to be used. --postgres_table POSTGRES_TABLE:   Specify the PostgreSQL table to be used. --dynamodb_table_name DYNAMODB_TABLE_NAME:   Provide the name of the DynamoDB table. --dynamodb_region_name DYNAMODB_REGION_NAME:   Specify the AWS region for DynamoDB. --prometheus_gateway PROMETHEUS_GATEWAY:   Specify the prometheus gateway URL. --args ...:   Additional keyword arguments to pass to the spout.</p>"},{"location":"guides/cli/#command-genius-testspoutctlspout-deploy","title":"Command: genius TestSpoutCtlSpout deploy","text":"<p>Usage: genius TestSpoutCtlSpout deploy [-h] [--buffer_size BUFFER_SIZE] [--output_folder OUTPUT_FOLDER] [--output_kafka_topic OUTPUT_KAFKA_TOPIC] [--output_kafka_cluster_connection_string OUTPUT_KAFKA_CLUSTER_CONNECTION_STRING] [--output_s3_bucket OUTPUT_S3_BUCKET] [--output_s3_folder OUTPUT_S3_FOLDER] [--redis_host REDIS_HOST] [--redis_port REDIS_PORT] [--redis_db REDIS_DB] [--postgres_host POSTGRES_HOST] [--postgres_port POSTGRES_PORT] [--postgres_user POSTGRES_USER] [--postgres_password POSTGRES_PASSWORD] [--postgres_database POSTGRES_DATABASE] [--postgres_table POSTGRES_TABLE] [--dynamodb_table_name DYNAMODB_TABLE_NAME] [--dynamodb_region_name DYNAMODB_REGION_NAME] [--prometheus_gateway PROMETHEUS_GATEWAY] [--k8s_kind {deployment,service,job,cron_job}] [--k8s_name K8S_NAME] [--k8s_image K8S_IMAGE] [--k8s_replicas K8S_REPLICAS] [--k8s_env_vars K8S_ENV_VARS] [--k8s_cpu K8S_CPU] [--k8s_memory K8S_MEMORY] [--k8s_storage K8S_STORAGE] [--k8s_gpu K8S_GPU] [--k8s_kube_config_path K8S_KUBE_CONFIG_PATH] [--k8s_api_key K8S_API_KEY] [--k8s_api_host K8S_API_HOST] [--k8s_verify_ssl K8S_VERIFY_SSL] [--k8s_ssl_ca_cert K8S_SSL_CA_CERT] [--k8s_cluster_name K8S_CLUSTER_NAME] [--k8s_context_name K8S_CONTEXT_NAME] [--k8s_namespace K8S_NAMESPACE] [--k8s_labels K8S_LABELS] [--k8s_annotations K8S_ANNOTATIONS] [--k8s_port K8S_PORT] [--k8s_target_port K8S_TARGET_PORT] [--k8s_schedule K8S_SCHEDULE] [--args ...] {batch,streaming,stream_to_batch} {none,redis,postgres,dynamodb,prometheus} {k8s} method_name</p> <p>{batch,streaming,stream_to_batch}</p> <p>:   Choose the type of output data: batch or streaming.</p> <p>{none,redis,postgres,dynamodb,prometheus}</p> <p>:   Select the type of state manager: none, redis, postgres, or     dynamodb.</p> <p>{k8s}</p> <p>:   Choose the type of deployment.</p> <p>method_name</p> <p>:   The name of the method to execute on the spout.</p> <p>Options genius TestSpoutCtlSpout deploy</p> <p>--buffer_size BUFFER_SIZE:   Specify the size of the buffer. --output_folder OUTPUT_FOLDER:   Specify the directory where output files should be stored     temporarily</p> <p>--output_kafka_topic OUTPUT_KAFKA_TOPIC:   Kafka output topic for streaming spouts. --output_kafka_cluster_connection_string OUTPUT_KAFKA_CLUSTER_CONNECTION_STRING:   Kafka connection string for streaming spouts. --output_s3_bucket OUTPUT_S3_BUCKET:   Provide the name of the S3 bucket for output storage. --output_s3_folder OUTPUT_S3_FOLDER:   Indicate the S3 folder for output storage. --redis_host REDIS_HOST:   Enter the host address for the Redis server. --redis_port REDIS_PORT:   Enter the port number for the Redis server. --redis_db REDIS_DB:   Specify the Redis database to be used. --postgres_host POSTGRES_HOST:   Enter the host address for the PostgreSQL server. --postgres_port POSTGRES_PORT:   Enter the port number for the PostgreSQL server. --postgres_user POSTGRES_USER:   Provide the username for the PostgreSQL server. --postgres_password POSTGRES_PASSWORD:   Provide the password for the PostgreSQL server. --postgres_database POSTGRES_DATABASE:   Specify the PostgreSQL database to be used. --postgres_table POSTGRES_TABLE:   Specify the PostgreSQL table to be used. --dynamodb_table_name DYNAMODB_TABLE_NAME:   Provide the name of the DynamoDB table. --dynamodb_region_name DYNAMODB_REGION_NAME:   Specify the AWS region for DynamoDB. --prometheus_gateway PROMETHEUS_GATEWAY:   Specify the prometheus gateway URL. --k8s_kind {deployment,service,job,cron_job}:   Choose the type of kubernetes resource. --k8s_name K8S_NAME:   Name of the Kubernetes resource. --k8s_image K8S_IMAGE:   Docker image for the Kubernetes resource. --k8s_replicas K8S_REPLICAS:   Number of replicas. --k8s_env_vars K8S_ENV_VARS:   Environment variables as a JSON string. --k8s_cpu K8S_CPU:   CPU requirements. --k8s_memory K8S_MEMORY:   Memory requirements. --k8s_storage K8S_STORAGE:   Storage requirements. --k8s_gpu K8S_GPU:   GPU requirements. --k8s_kube_config_path K8S_KUBE_CONFIG_PATH:   Name of the Kubernetes cluster local config. --k8s_api_key K8S_API_KEY:   GPU requirements. --k8s_api_host K8S_API_HOST:   GPU requirements. --k8s_verify_ssl K8S_VERIFY_SSL:   GPU requirements. --k8s_ssl_ca_cert K8S_SSL_CA_CERT:   GPU requirements. --k8s_cluster_name K8S_CLUSTER_NAME:   Name of the Kubernetes cluster. --k8s_context_name K8S_CONTEXT_NAME:   Name of the kubeconfig context. --k8s_namespace K8S_NAMESPACE:   Kubernetes namespace. --k8s_labels K8S_LABELS:   Labels for Kubernetes resources, as a JSON string. --k8s_annotations K8S_ANNOTATIONS:   Annotations for Kubernetes resources, as a JSON string. --k8s_port K8S_PORT:   Port to run the spout on as a service. --k8s_target_port K8S_TARGET_PORT:   Port to expose the spout on as a service. --k8s_schedule K8S_SCHEDULE:   Schedule to run the spout on as a cron job. --args ...:   Additional keyword arguments to pass to the spout.</p>"},{"location":"guides/cli/#command-genius-testspoutctlspout-help","title":"Command: genius TestSpoutCtlSpout help","text":"<p>Usage: genius TestSpoutCtlSpout help [-h] method</p> <p>method</p> <p>:   The method to execute.</p>"},{"location":"guides/cli/#bolts","title":"Bolts","text":""},{"location":"guides/cli/#command-genius-testboltctlbolt","title":"Command: genius TestBoltCtlBolt","text":"<p>Usage: genius TestBoltCtlBolt [-h] {rise,deploy,help} ...</p> <p>POSITIONAL ARGUMENTS genius TestBoltCtlBolt</p> <p>genius TestBoltCtlBolt rise</p> <p>:   Run a bolt locally.</p> <p>genius TestBoltCtlBolt deploy</p> <p>:   Run a spout remotely.</p> <p>genius TestBoltCtlBolt help</p> <p>:   Print help for the bolt.</p>"},{"location":"guides/cli/#command-genius-testboltctlbolt-rise","title":"Command: genius TestBoltCtlBolt rise","text":"<p>Usage: genius TestBoltCtlBolt rise [-h] [--buffer_size BUFFER_SIZE] [--input_folder INPUT_FOLDER] [--input_kafka_topic INPUT_KAFKA_TOPIC] [--input_kafka_cluster_connection_string INPUT_KAFKA_CLUSTER_CONNECTION_STRING] [--input_kafka_consumer_group_id INPUT_KAFKA_CONSUMER_GROUP_ID] [--input_s3_bucket INPUT_S3_BUCKET] [--input_s3_folder INPUT_S3_FOLDER] [--output_folder OUTPUT_FOLDER] [--output_kafka_topic OUTPUT_KAFKA_TOPIC] [--output_kafka_cluster_connection_string OUTPUT_KAFKA_CLUSTER_CONNECTION_STRING] [--output_s3_bucket OUTPUT_S3_BUCKET] [--output_s3_folder OUTPUT_S3_FOLDER] [--redis_host REDIS_HOST] [--redis_port REDIS_PORT] [--redis_db REDIS_DB] [--postgres_host POSTGRES_HOST] [--postgres_port POSTGRES_PORT] [--postgres_user POSTGRES_USER] [--postgres_password POSTGRES_PASSWORD] [--postgres_database POSTGRES_DATABASE] [--postgres_table POSTGRES_TABLE] [--dynamodb_table_name DYNAMODB_TABLE_NAME] [--dynamodb_region_name DYNAMODB_REGION_NAME] [--prometheus_gateway PROMETHEUS_GATEWAY] [--args ...] {batch,streaming,batch_to_stream,stream_to_batch} {batch,streaming,stream_to_batch} {none,redis,postgres,dynamodb,prometheus} method_name</p> <p>{batch,streaming,batch_to_stream,stream_to_batch}</p> <p>:   Choose the type of input data: batch or streaming.</p> <p>{batch,streaming,stream_to_batch}</p> <p>:   Choose the type of output data: batch or streaming.</p> <p>{none,redis,postgres,dynamodb,prometheus}</p> <p>:   Select the type of state manager: none, redis, postgres, or     dynamodb.</p> <p>method_name</p> <p>:   The name of the method to execute on the bolt.</p> <p>Options genius TestBoltCtlBolt rise</p> <p>--buffer_size BUFFER_SIZE:   Specify the size of the buffer. --input_folder INPUT_FOLDER:   Specify the directory where output files should be stored     temporarily</p> <p>--input_kafka_topic INPUT_KAFKA_TOPIC:   Kafka output topic for streaming spouts. --input_kafka_cluster_connection_string INPUT_KAFKA_CLUSTER_CONNECTION_STRING:   Kafka connection string for streaming spouts. --input_kafka_consumer_group_id INPUT_KAFKA_CONSUMER_GROUP_ID:   Kafka consumer group id to use. --input_s3_bucket INPUT_S3_BUCKET:   Provide the name of the S3 bucket for output storage. --input_s3_folder INPUT_S3_FOLDER:   Indicate the S3 folder for output storage. --output_folder OUTPUT_FOLDER:   Specify the directory where output files should be stored     temporarily</p> <p>--output_kafka_topic OUTPUT_KAFKA_TOPIC:   Kafka output topic for streaming spouts. --output_kafka_cluster_connection_string OUTPUT_KAFKA_CLUSTER_CONNECTION_STRING:   Kafka connection string for streaming spouts. --output_s3_bucket OUTPUT_S3_BUCKET:   Provide the name of the S3 bucket for output storage. --output_s3_folder OUTPUT_S3_FOLDER:   Indicate the S3 folder for output storage. --redis_host REDIS_HOST:   Enter the host address for the Redis server. --redis_port REDIS_PORT:   Enter the port number for the Redis server. --redis_db REDIS_DB:   Specify the Redis database to be used. --postgres_host POSTGRES_HOST:   Enter the host address for the PostgreSQL server. --postgres_port POSTGRES_PORT:   Enter the port number for the PostgreSQL server. --postgres_user POSTGRES_USER:   Provide the username for the PostgreSQL server. --postgres_password POSTGRES_PASSWORD:   Provide the password for the PostgreSQL server. --postgres_database POSTGRES_DATABASE:   Specify the PostgreSQL database to be used. --postgres_table POSTGRES_TABLE:   Specify the PostgreSQL table to be used. --dynamodb_table_name DYNAMODB_TABLE_NAME:   Provide the name of the DynamoDB table. --dynamodb_region_name DYNAMODB_REGION_NAME:   Specify the AWS region for DynamoDB. --prometheus_gateway PROMETHEUS_GATEWAY:   Specify the prometheus gateway URL. --args ...:   Additional keyword arguments to pass to the bolt.</p>"},{"location":"guides/cli/#command-genius-testboltctlbolt-deploy","title":"Command: genius TestBoltCtlBolt deploy","text":"<p>Usage: genius TestBoltCtlBolt deploy [-h] [--buffer_size BUFFER_SIZE] [--input_folder INPUT_FOLDER] [--input_kafka_topic INPUT_KAFKA_TOPIC] [--input_kafka_cluster_connection_string INPUT_KAFKA_CLUSTER_CONNECTION_STRING] [--input_kafka_consumer_group_id INPUT_KAFKA_CONSUMER_GROUP_ID] [--input_s3_bucket INPUT_S3_BUCKET] [--input_s3_folder INPUT_S3_FOLDER] [--output_folder OUTPUT_FOLDER] [--output_kafka_topic OUTPUT_KAFKA_TOPIC] [--output_kafka_cluster_connection_string OUTPUT_KAFKA_CLUSTER_CONNECTION_STRING] [--output_s3_bucket OUTPUT_S3_BUCKET] [--output_s3_folder OUTPUT_S3_FOLDER] [--redis_host REDIS_HOST] [--redis_port REDIS_PORT] [--redis_db REDIS_DB] [--postgres_host POSTGRES_HOST] [--postgres_port POSTGRES_PORT] [--postgres_user POSTGRES_USER] [--postgres_password POSTGRES_PASSWORD] [--postgres_database POSTGRES_DATABASE] [--postgres_table POSTGRES_TABLE] [--dynamodb_table_name DYNAMODB_TABLE_NAME] [--dynamodb_region_name DYNAMODB_REGION_NAME] [--prometheus_gateway PROMETHEUS_GATEWAY] [--k8s_kind {deployment,service,job,cron_job}] [--k8s_name K8S_NAME] [--k8s_image K8S_IMAGE] [--k8s_replicas K8S_REPLICAS] [--k8s_env_vars K8S_ENV_VARS] [--k8s_cpu K8S_CPU] [--k8s_memory K8S_MEMORY] [--k8s_storage K8S_STORAGE] [--k8s_gpu K8S_GPU] [--k8s_kube_config_path K8S_KUBE_CONFIG_PATH] [--k8s_api_key K8S_API_KEY] [--k8s_api_host K8S_API_HOST] [--k8s_verify_ssl K8S_VERIFY_SSL] [--k8s_ssl_ca_cert K8S_SSL_CA_CERT] [--k8s_cluster_name K8S_CLUSTER_NAME] [--k8s_context_name K8S_CONTEXT_NAME] [--k8s_namespace K8S_NAMESPACE] [--k8s_labels K8S_LABELS] [--k8s_annotations K8S_ANNOTATIONS] [--k8s_port K8S_PORT] [--k8s_target_port K8S_TARGET_PORT] [--k8s_schedule K8S_SCHEDULE] [--args ...] {batch,streaming,batch_to_stream,stream_to_batch} {batch,streaming,stream_to_batch} {none,redis,postgres,dynamodb,prometheus} {k8s} method_name</p> <p>{batch,streaming,batch_to_stream,stream_to_batch}</p> <p>:   Choose the type of input data: batch or streaming.</p> <p>{batch,streaming,stream_to_batch}</p> <p>:   Choose the type of output data: batch or streaming.</p> <p>{none,redis,postgres,dynamodb,prometheus}</p> <p>:   Select the type of state manager: none, redis, postgres, or     dynamodb.</p> <p>{k8s}</p> <p>:   Choose the type of deployment.</p> <p>method_name</p> <p>:   The name of the method to execute on the spout.</p> <p>Options genius TestBoltCtlBolt deploy</p> <p>--buffer_size BUFFER_SIZE:   Specify the size of the buffer. --input_folder INPUT_FOLDER:   Specify the directory where output files should be stored     temporarily</p> <p>--input_kafka_topic INPUT_KAFKA_TOPIC:   Kafka output topic for streaming spouts. --input_kafka_cluster_connection_string INPUT_KAFKA_CLUSTER_CONNECTION_STRING:   Kafka connection string for streaming spouts. --input_kafka_consumer_group_id INPUT_KAFKA_CONSUMER_GROUP_ID:   Kafka consumer group id to use. --input_s3_bucket INPUT_S3_BUCKET:   Provide the name of the S3 bucket for output storage. --input_s3_folder INPUT_S3_FOLDER:   Indicate the S3 folder for output storage. --output_folder OUTPUT_FOLDER:   Specify the directory where output files should be stored     temporarily</p> <p>--output_kafka_topic OUTPUT_KAFKA_TOPIC:   Kafka output topic for streaming spouts. --output_kafka_cluster_connection_string OUTPUT_KAFKA_CLUSTER_CONNECTION_STRING:   Kafka connection string for streaming spouts. --output_s3_bucket OUTPUT_S3_BUCKET:   Provide the name of the S3 bucket for output storage. --output_s3_folder OUTPUT_S3_FOLDER:   Indicate the S3 folder for output storage. --redis_host REDIS_HOST:   Enter the host address for the Redis server. --redis_port REDIS_PORT:   Enter the port number for the Redis server. --redis_db REDIS_DB:   Specify the Redis database to be used. --postgres_host POSTGRES_HOST:   Enter the host address for the PostgreSQL server. --postgres_port POSTGRES_PORT:   Enter the port number for the PostgreSQL server. --postgres_user POSTGRES_USER:   Provide the username for the PostgreSQL server. --postgres_password POSTGRES_PASSWORD:   Provide the password for the PostgreSQL server. --postgres_database POSTGRES_DATABASE:   Specify the PostgreSQL database to be used. --postgres_table POSTGRES_TABLE:   Specify the PostgreSQL table to be used. --dynamodb_table_name DYNAMODB_TABLE_NAME:   Provide the name of the DynamoDB table. --dynamodb_region_name DYNAMODB_REGION_NAME:   Specify the AWS region for DynamoDB. --prometheus_gateway PROMETHEUS_GATEWAY:   Specify the prometheus gateway URL. --k8s_kind {deployment,service,job,cron_job}:   Choose the type of kubernetes resource. --k8s_name K8S_NAME:   Name of the Kubernetes resource. --k8s_image K8S_IMAGE:   Docker image for the Kubernetes resource. --k8s_replicas K8S_REPLICAS:   Number of replicas. --k8s_env_vars K8S_ENV_VARS:   Environment variables as a JSON string. --k8s_cpu K8S_CPU:   CPU requirements. --k8s_memory K8S_MEMORY:   Memory requirements. --k8s_storage K8S_STORAGE:   Storage requirements. --k8s_gpu K8S_GPU:   GPU requirements. --k8s_kube_config_path K8S_KUBE_CONFIG_PATH:   Name of the Kubernetes cluster local config. --k8s_api_key K8S_API_KEY:   GPU requirements. --k8s_api_host K8S_API_HOST:   GPU requirements. --k8s_verify_ssl K8S_VERIFY_SSL:   GPU requirements. --k8s_ssl_ca_cert K8S_SSL_CA_CERT:   GPU requirements. --k8s_cluster_name K8S_CLUSTER_NAME:   Name of the Kubernetes cluster. --k8s_context_name K8S_CONTEXT_NAME:   Name of the kubeconfig context. --k8s_namespace K8S_NAMESPACE:   Kubernetes namespace. --k8s_labels K8S_LABELS:   Labels for Kubernetes resources, as a JSON string. --k8s_annotations K8S_ANNOTATIONS:   Annotations for Kubernetes resources, as a JSON string. --k8s_port K8S_PORT:   Port to run the spout on as a service. --k8s_target_port K8S_TARGET_PORT:   Port to expose the spout on as a service. --k8s_schedule K8S_SCHEDULE:   Schedule to run the spout on as a cron job. --args ...:   Additional keyword arguments to pass to the spout.</p>"},{"location":"guides/cli/#command-genius-testboltctlbolt-help","title":"Command: genius TestBoltCtlBolt help","text":"<p>Usage: genius TestBoltCtlBolt help [-h] method</p> <p>method</p> <p>:   The method to execute.</p>"},{"location":"guides/cli/#deployment","title":"Deployment","text":""},{"location":"guides/cli/#command-genius-rise","title":"Command: genius rise","text":"<p>Usage: genius rise [-h] [--spout SPOUT] [--bolt BOLT] [--file FILE] {up} ...</p> <p>POSITIONAL ARGUMENTS genius rise</p> <p>genius rise up</p> <p>:   Deploy according to the genius.yml file.</p>"},{"location":"guides/cli/#command-genius-rise-up","title":"Command: genius rise up","text":"<p>Usage: genius rise up [-h] [--spout SPOUT] [--bolt BOLT] [--file FILE]</p> <p>Options genius rise up</p> <p>--spout SPOUT:   Name of the specific spout to run. --bolt BOLT:   Name of the specific bolt to run. --file FILE:   Path of the genius.yml file, default to .</p> <p>Options genius rise</p> <p>--spout SPOUT:   Name of the specific spout to run. --bolt BOLT:   Name of the specific bolt to run. --file FILE:   Path of the genius.yml file, default to .</p>"},{"location":"guides/cli/#kubernetes-pods","title":"Kubernetes Pods","text":""},{"location":"guides/cli/#command-genius-pod","title":"Command: genius pod","text":"<p>usage: genius pod [-h] {status,show,describe,logs} ...</p> <p>POSITIONAL ARGUMENTS genius pod</p> <p>genius pod status</p> <p>:   Get the status of the Kubernetes pod.</p> <p>genius pod show</p> <p>:   List all pods.</p> <p>genius pod describe</p> <p>:   Describe a pod.</p> <p>genius pod logs</p> <p>:   Get the logs of a pod.</p>"},{"location":"guides/cli/#command-genius-pod-status","title":"Command: genius pod status","text":"<p>usage: genius pod status [-h] [--kube_config_path KUBE_CONFIG_PATH] [--cluster_name CLUSTER_NAME] [--context_name CONTEXT_NAME] [--namespace NAMESPACE] [--labels LABELS] [--annotations ANNOTATIONS] [--api_key API_KEY] [--api_host API_HOST] [--verify_ssl VERIFY_SSL] [--ssl_ca_cert SSL_CA_CERT] name</p> <p>name</p> <p>:   Name of the Kubernetes pod.</p> <p>Options genius pod status</p> <p>--kube_config_path KUBE_CONFIG_PATH:   Path to the kubeconfig file. --cluster_name CLUSTER_NAME:   Name of the Kubernetes cluster. --context_name CONTEXT_NAME:   Name of the kubeconfig context. --namespace NAMESPACE:   Kubernetes namespace. --labels LABELS:   Labels for Kubernetes resources, as a JSON string. --annotations ANNOTATIONS:   Annotations for Kubernetes resources, as a JSON string. --api_key API_KEY:   API key for Kubernetes cluster. --api_host API_HOST:   API host for Kubernetes cluster. --verify_ssl VERIFY_SSL:   Whether to verify SSL certificates. --ssl_ca_cert SSL_CA_CERT:   Path to the SSL CA certificate.</p>"},{"location":"guides/cli/#command-genius-pod-show","title":"Command: genius pod show","text":"<p>usage: genius pod show [-h] [--kube_config_path KUBE_CONFIG_PATH] [--cluster_name CLUSTER_NAME] [--context_name CONTEXT_NAME] [--namespace NAMESPACE] [--labels LABELS] [--annotations ANNOTATIONS] [--api_key API_KEY] [--api_host API_HOST] [--verify_ssl VERIFY_SSL] [--ssl_ca_cert SSL_CA_CERT]</p> <p>Options genius pod show</p> <p>--kube_config_path KUBE_CONFIG_PATH:   Path to the kubeconfig file. --cluster_name CLUSTER_NAME:   Name of the Kubernetes cluster. --context_name CONTEXT_NAME:   Name of the kubeconfig context. --namespace NAMESPACE:   Kubernetes namespace. --labels LABELS:   Labels for Kubernetes resources, as a JSON string. --annotations ANNOTATIONS:   Annotations for Kubernetes resources, as a JSON string. --api_key API_KEY:   API key for Kubernetes cluster. --api_host API_HOST:   API host for Kubernetes cluster. --verify_ssl VERIFY_SSL:   Whether to verify SSL certificates. --ssl_ca_cert SSL_CA_CERT:   Path to the SSL CA certificate.</p>"},{"location":"guides/cli/#command-genius-pod-describe","title":"Command: genius pod describe","text":"<p>usage: genius pod describe [-h] [--kube_config_path KUBE_CONFIG_PATH] [--cluster_name CLUSTER_NAME] [--context_name CONTEXT_NAME] [--namespace NAMESPACE] [--labels LABELS] [--annotations ANNOTATIONS] [--api_key API_KEY] [--api_host API_HOST] [--verify_ssl VERIFY_SSL] [--ssl_ca_cert SSL_CA_CERT] name</p> <p>name</p> <p>:   Name of the pod.</p> <p>Options genius pod describe</p> <p>--kube_config_path KUBE_CONFIG_PATH:   Path to the kubeconfig file. --cluster_name CLUSTER_NAME:   Name of the Kubernetes cluster. --context_name CONTEXT_NAME:   Name of the kubeconfig context. --namespace NAMESPACE:   Kubernetes namespace. --labels LABELS:   Labels for Kubernetes resources, as a JSON string. --annotations ANNOTATIONS:   Annotations for Kubernetes resources, as a JSON string. --api_key API_KEY:   API key for Kubernetes cluster. --api_host API_HOST:   API host for Kubernetes cluster. --verify_ssl VERIFY_SSL:   Whether to verify SSL certificates. --ssl_ca_cert SSL_CA_CERT:   Path to the SSL CA certificate.</p>"},{"location":"guides/cli/#command-genius-pod-logs","title":"Command: genius pod logs","text":"<p>usage: genius pod logs [-h] [--follow FOLLOW] [--tail TAIL] [--kube_config_path KUBE_CONFIG_PATH] [--cluster_name CLUSTER_NAME] [--context_name CONTEXT_NAME] [--namespace NAMESPACE] [--labels LABELS] [--annotations ANNOTATIONS] [--api_key API_KEY] [--api_host API_HOST] [--verify_ssl VERIFY_SSL] [--ssl_ca_cert SSL_CA_CERT] name</p> <p>name</p> <p>:   Name of the pod.</p> <p>Options genius pod logs</p> <p>--follow FOLLOW:   Whether to follow the logs. --tail TAIL:   Number of lines to show from the end of the logs. --kube_config_path KUBE_CONFIG_PATH:   Path to the kubeconfig file. --cluster_name CLUSTER_NAME:   Name of the Kubernetes cluster. --context_name CONTEXT_NAME:   Name of the kubeconfig context. --namespace NAMESPACE:   Kubernetes namespace. --labels LABELS:   Labels for Kubernetes resources, as a JSON string. --annotations ANNOTATIONS:   Annotations for Kubernetes resources, as a JSON string. --api_key API_KEY:   API key for Kubernetes cluster. --api_host API_HOST:   API host for Kubernetes cluster. --verify_ssl VERIFY_SSL:   Whether to verify SSL certificates. --ssl_ca_cert SSL_CA_CERT:   Path to the SSL CA certificate.</p>"},{"location":"guides/cli/#kubernetes-deployments","title":"Kubernetes Deployments","text":""},{"location":"guides/cli/#command-genius-deployment","title":"Command: genius deployment","text":"<p>usage: genius deployment [-h] {create,scale,describe,show,delete,status} ...</p> <p>POSITIONAL ARGUMENTS genius deployment</p> <p>genius deployment create</p> <p>:   Create a new deployment.</p> <p>genius deployment scale</p> <p>:   Scale a deployment.</p> <p>genius deployment describe</p> <p>:   Describe a deployment.</p> <p>genius deployment show</p> <p>:   List all deployments.</p> <p>genius deployment delete</p> <p>:   Delete a deployment.</p> <p>genius deployment status</p> <p>:   Get the status of a deployment.</p>"},{"location":"guides/cli/#command-genius-deployment-create","title":"Command: genius deployment create","text":"<p>usage: genius deployment create [-h] [--replicas REPLICAS] [--env_vars ENV_VARS] [--cpu CPU] [--memory MEMORY] [--storage STORAGE] [--gpu GPU] [--kube_config_path KUBE_CONFIG_PATH] [--cluster_name CLUSTER_NAME] [--context_name CONTEXT_NAME] [--namespace NAMESPACE] [--labels LABELS] [--annotations ANNOTATIONS] [--api_key API_KEY] [--api_host API_HOST] [--verify_ssl VERIFY_SSL] [--ssl_ca_cert SSL_CA_CERT] name image command</p> <p>name</p> <p>:   Name of the deployment.</p> <p>image</p> <p>:   Docker image for the deployment.</p> <p>command</p> <p>:   Command to run in the container.</p> <p>Options genius deployment create</p> <p>--replicas REPLICAS:   Number of replicas. --env_vars ENV_VARS:   Environment variables as a JSON string. --cpu CPU:   CPU requirements. --memory MEMORY:   Memory requirements. --storage STORAGE:   Storage requirements. --gpu GPU:   GPU requirements. --kube_config_path KUBE_CONFIG_PATH:   Path to the kubeconfig file. --cluster_name CLUSTER_NAME:   Name of the Kubernetes cluster. --context_name CONTEXT_NAME:   Name of the kubeconfig context. --namespace NAMESPACE:   Kubernetes namespace. --labels LABELS:   Labels for Kubernetes resources, as a JSON string. --annotations ANNOTATIONS:   Annotations for Kubernetes resources, as a JSON string. --api_key API_KEY:   API key for Kubernetes cluster. --api_host API_HOST:   API host for Kubernetes cluster. --verify_ssl VERIFY_SSL:   Whether to verify SSL certificates. --ssl_ca_cert SSL_CA_CERT:   Path to the SSL CA certificate.</p>"},{"location":"guides/cli/#command-genius-deployment-scale","title":"Command: genius deployment scale","text":"<p>usage: genius deployment scale [-h] [--kube_config_path KUBE_CONFIG_PATH] [--cluster_name CLUSTER_NAME] [--context_name CONTEXT_NAME] [--namespace NAMESPACE] [--labels LABELS] [--annotations ANNOTATIONS] [--api_key API_KEY] [--api_host API_HOST] [--verify_ssl VERIFY_SSL] [--ssl_ca_cert SSL_CA_CERT] name replicas</p> <p>name</p> <p>:   Name of the deployment.</p> <p>replicas</p> <p>:   Number of replicas.</p> <p>Options genius deployment scale</p> <p>--kube_config_path KUBE_CONFIG_PATH:   Path to the kubeconfig file. --cluster_name CLUSTER_NAME:   Name of the Kubernetes cluster. --context_name CONTEXT_NAME:   Name of the kubeconfig context. --namespace NAMESPACE:   Kubernetes namespace. --labels LABELS:   Labels for Kubernetes resources, as a JSON string. --annotations ANNOTATIONS:   Annotations for Kubernetes resources, as a JSON string. --api_key API_KEY:   API key for Kubernetes cluster. --api_host API_HOST:   API host for Kubernetes cluster. --verify_ssl VERIFY_SSL:   Whether to verify SSL certificates. --ssl_ca_cert SSL_CA_CERT:   Path to the SSL CA certificate.</p>"},{"location":"guides/cli/#command-genius-deployment-describe","title":"Command: genius deployment describe","text":"<p>usage: genius deployment describe [-h] [--kube_config_path KUBE_CONFIG_PATH] [--cluster_name CLUSTER_NAME] [--context_name CONTEXT_NAME] [--namespace NAMESPACE] [--labels LABELS] [--annotations ANNOTATIONS] [--api_key API_KEY] [--api_host API_HOST] [--verify_ssl VERIFY_SSL] [--ssl_ca_cert SSL_CA_CERT] name</p> <p>name</p> <p>:   Name of the deployment.</p> <p>Options genius deployment describe</p> <p>--kube_config_path KUBE_CONFIG_PATH:   Path to the kubeconfig file. --cluster_name CLUSTER_NAME:   Name of the Kubernetes cluster. --context_name CONTEXT_NAME:   Name of the kubeconfig context. --namespace NAMESPACE:   Kubernetes namespace. --labels LABELS:   Labels for Kubernetes resources, as a JSON string. --annotations ANNOTATIONS:   Annotations for Kubernetes resources, as a JSON string. --api_key API_KEY:   API key for Kubernetes cluster. --api_host API_HOST:   API host for Kubernetes cluster. --verify_ssl VERIFY_SSL:   Whether to verify SSL certificates. --ssl_ca_cert SSL_CA_CERT:   Path to the SSL CA certificate.</p>"},{"location":"guides/cli/#command-genius-deployment-show","title":"Command: genius deployment show","text":"<p>usage: genius deployment show [-h] [--kube_config_path KUBE_CONFIG_PATH] [--cluster_name CLUSTER_NAME] [--context_name CONTEXT_NAME] [--namespace NAMESPACE] [--labels LABELS] [--annotations ANNOTATIONS] [--api_key API_KEY] [--api_host API_HOST] [--verify_ssl VERIFY_SSL] [--ssl_ca_cert SSL_CA_CERT]</p> <p>Options genius deployment show</p> <p>--kube_config_path KUBE_CONFIG_PATH:   Path to the kubeconfig file. --cluster_name CLUSTER_NAME:   Name of the Kubernetes cluster. --context_name CONTEXT_NAME:   Name of the kubeconfig context. --namespace NAMESPACE:   Kubernetes namespace. --labels LABELS:   Labels for Kubernetes resources, as a JSON string. --annotations ANNOTATIONS:   Annotations for Kubernetes resources, as a JSON string. --api_key API_KEY:   API key for Kubernetes cluster. --api_host API_HOST:   API host for Kubernetes cluster. --verify_ssl VERIFY_SSL:   Whether to verify SSL certificates. --ssl_ca_cert SSL_CA_CERT:   Path to the SSL CA certificate.</p>"},{"location":"guides/cli/#command-genius-deployment-delete","title":"Command: genius deployment delete","text":"<p>usage: genius deployment delete [-h] [--kube_config_path KUBE_CONFIG_PATH] [--cluster_name CLUSTER_NAME] [--context_name CONTEXT_NAME] [--namespace NAMESPACE] [--labels LABELS] [--annotations ANNOTATIONS] [--api_key API_KEY] [--api_host API_HOST] [--verify_ssl VERIFY_SSL] [--ssl_ca_cert SSL_CA_CERT] name</p> <p>name</p> <p>:   Name of the deployment.</p> <p>Options genius deployment delete</p> <p>--kube_config_path KUBE_CONFIG_PATH:   Path to the kubeconfig file. --cluster_name CLUSTER_NAME:   Name of the Kubernetes cluster. --context_name CONTEXT_NAME:   Name of the kubeconfig context. --namespace NAMESPACE:   Kubernetes namespace. --labels LABELS:   Labels for Kubernetes resources, as a JSON string. --annotations ANNOTATIONS:   Annotations for Kubernetes resources, as a JSON string. --api_key API_KEY:   API key for Kubernetes cluster. --api_host API_HOST:   API host for Kubernetes cluster. --verify_ssl VERIFY_SSL:   Whether to verify SSL certificates. --ssl_ca_cert SSL_CA_CERT:   Path to the SSL CA certificate.</p>"},{"location":"guides/cli/#command-genius-deployment-status","title":"Command: genius deployment status","text":"<p>usage: genius deployment status [-h] [--kube_config_path KUBE_CONFIG_PATH] [--cluster_name CLUSTER_NAME] [--context_name CONTEXT_NAME] [--namespace NAMESPACE] [--labels LABELS] [--annotations ANNOTATIONS] [--api_key API_KEY] [--api_host API_HOST] [--verify_ssl VERIFY_SSL] [--ssl_ca_cert SSL_CA_CERT] name</p> <p>name</p> <p>:   Name of the deployment.</p> <p>Options genius deployment status</p> <p>--kube_config_path KUBE_CONFIG_PATH:   Path to the kubeconfig file. --cluster_name CLUSTER_NAME:   Name of the Kubernetes cluster. --context_name CONTEXT_NAME:   Name of the kubeconfig context. --namespace NAMESPACE:   Kubernetes namespace. --labels LABELS:   Labels for Kubernetes resources, as a JSON string. --annotations ANNOTATIONS:   Annotations for Kubernetes resources, as a JSON string. --api_key API_KEY:   API key for Kubernetes cluster. --api_host API_HOST:   API host for Kubernetes cluster. --verify_ssl VERIFY_SSL:   Whether to verify SSL certificates. --ssl_ca_cert SSL_CA_CERT:   Path to the SSL CA certificate.</p>"},{"location":"guides/cli/#kubernetes-services","title":"Kubernetes Services","text":""},{"location":"guides/cli/#command-genius-service","title":"Command: genius service","text":"<p>usage: genius service [-h] {create,delete,describe,show} ...</p> <p>POSITIONAL ARGUMENTS genius service</p> <p>genius service create</p> <p>:   Create a new service.</p> <p>genius service delete</p> <p>:   Delete a service.</p> <p>genius service describe</p> <p>:   Describe a service.</p> <p>genius service show</p> <p>:   List all services.</p>"},{"location":"guides/cli/#command-genius-service-create","title":"Command: genius service create","text":"<p>usage: genius service create [-h] [--replicas REPLICAS] [--port PORT] [--target_port TARGET_PORT] [--env_vars ENV_VARS] [--cpu CPU] [--memory MEMORY] [--storage STORAGE] [--gpu GPU] [--kube_config_path KUBE_CONFIG_PATH] [--cluster_name CLUSTER_NAME] [--context_name CONTEXT_NAME] [--namespace NAMESPACE] [--labels LABELS] [--annotations ANNOTATIONS] [--api_key API_KEY] [--api_host API_HOST] [--verify_ssl VERIFY_SSL] [--ssl_ca_cert SSL_CA_CERT] name image command</p> <p>name</p> <p>:   Name of the service.</p> <p>image</p> <p>:   Docker image for the service.</p> <p>command</p> <p>:   Command to run in the container.</p> <p>Options genius service create</p> <p>--replicas REPLICAS:   Number of replicas. --port PORT:   Service port. --target_port TARGET_PORT:   Container target port. --env_vars ENV_VARS:   Environment variables as a JSON string. --cpu CPU:   CPU requirements. --memory MEMORY:   Memory requirements. --storage STORAGE:   Storage requirements. --gpu GPU:   GPU requirements. --kube_config_path KUBE_CONFIG_PATH:   Path to the kubeconfig file. --cluster_name CLUSTER_NAME:   Name of the Kubernetes cluster. --context_name CONTEXT_NAME:   Name of the kubeconfig context. --namespace NAMESPACE:   Kubernetes namespace. --labels LABELS:   Labels for Kubernetes resources, as a JSON string. --annotations ANNOTATIONS:   Annotations for Kubernetes resources, as a JSON string. --api_key API_KEY:   API key for Kubernetes cluster. --api_host API_HOST:   API host for Kubernetes cluster. --verify_ssl VERIFY_SSL:   Whether to verify SSL certificates. --ssl_ca_cert SSL_CA_CERT:   Path to the SSL CA certificate.</p>"},{"location":"guides/cli/#command-genius-service-delete","title":"Command: genius service delete","text":"<p>usage: genius service delete [-h] [--kube_config_path KUBE_CONFIG_PATH] [--cluster_name CLUSTER_NAME] [--context_name CONTEXT_NAME] [--namespace NAMESPACE] [--labels LABELS] [--annotations ANNOTATIONS] [--api_key API_KEY] [--api_host API_HOST] [--verify_ssl VERIFY_SSL] [--ssl_ca_cert SSL_CA_CERT] name</p> <p>name</p> <p>:   Name of the service.</p> <p>Options genius service delete</p> <p>--kube_config_path KUBE_CONFIG_PATH:   Path to the kubeconfig file. --cluster_name CLUSTER_NAME:   Name of the Kubernetes cluster. --context_name CONTEXT_NAME:   Name of the kubeconfig context. --namespace NAMESPACE:   Kubernetes namespace. --labels LABELS:   Labels for Kubernetes resources, as a JSON string. --annotations ANNOTATIONS:   Annotations for Kubernetes resources, as a JSON string. --api_key API_KEY:   API key for Kubernetes cluster. --api_host API_HOST:   API host for Kubernetes cluster. --verify_ssl VERIFY_SSL:   Whether to verify SSL certificates. --ssl_ca_cert SSL_CA_CERT:   Path to the SSL CA certificate.</p>"},{"location":"guides/cli/#command-genius-service-describe","title":"Command: genius service describe","text":"<p>usage: genius service describe [-h] [--kube_config_path KUBE_CONFIG_PATH] [--cluster_name CLUSTER_NAME] [--context_name CONTEXT_NAME] [--namespace NAMESPACE] [--labels LABELS] [--annotations ANNOTATIONS] [--api_key API_KEY] [--api_host API_HOST] [--verify_ssl VERIFY_SSL] [--ssl_ca_cert SSL_CA_CERT] name</p> <p>name</p> <p>:   Name of the service.</p> <p>Options genius service describe</p> <p>--kube_config_path KUBE_CONFIG_PATH:   Path to the kubeconfig file. --cluster_name CLUSTER_NAME:   Name of the Kubernetes cluster. --context_name CONTEXT_NAME:   Name of the kubeconfig context. --namespace NAMESPACE:   Kubernetes namespace. --labels LABELS:   Labels for Kubernetes resources, as a JSON string. --annotations ANNOTATIONS:   Annotations for Kubernetes resources, as a JSON string. --api_key API_KEY:   API key for Kubernetes cluster. --api_host API_HOST:   API host for Kubernetes cluster. --verify_ssl VERIFY_SSL:   Whether to verify SSL certificates. --ssl_ca_cert SSL_CA_CERT:   Path to the SSL CA certificate.</p>"},{"location":"guides/cli/#command-genius-service-show","title":"Command: genius service show","text":"<p>usage: genius service show [-h] [--kube_config_path KUBE_CONFIG_PATH] [--cluster_name CLUSTER_NAME] [--context_name CONTEXT_NAME] [--namespace NAMESPACE] [--labels LABELS] [--annotations ANNOTATIONS] [--api_key API_KEY] [--api_host API_HOST] [--verify_ssl VERIFY_SSL] [--ssl_ca_cert SSL_CA_CERT]</p> <p>Options genius service show</p> <p>--kube_config_path KUBE_CONFIG_PATH:   Path to the kubeconfig file. --cluster_name CLUSTER_NAME:   Name of the Kubernetes cluster. --context_name CONTEXT_NAME:   Name of the kubeconfig context. --namespace NAMESPACE:   Kubernetes namespace. --labels LABELS:   Labels for Kubernetes resources, as a JSON string. --annotations ANNOTATIONS:   Annotations for Kubernetes resources, as a JSON string. --api_key API_KEY:   API key for Kubernetes cluster. --api_host API_HOST:   API host for Kubernetes cluster. --verify_ssl VERIFY_SSL:   Whether to verify SSL certificates. --ssl_ca_cert SSL_CA_CERT:   Path to the SSL CA certificate.</p>"},{"location":"guides/cli/#kubernetes-jobs","title":"Kubernetes Jobs","text":""},{"location":"guides/cli/#command-genius-job","title":"Command: genius job","text":"<p>usage: genius job [-h] {create,delete,status} ...</p> <p>POSITIONAL ARGUMENTS genius job</p> <p>genius job create</p> <p>:   Create a new job.</p> <p>genius job delete</p> <p>:   Delete a job.</p> <p>genius job status</p> <p>:   Get the status of a job.</p>"},{"location":"guides/cli/#command-genius-job-create","title":"Command: genius job create","text":"<p>usage: genius job create [-h] [--env_vars ENV_VARS] [--cpu CPU] [--memory MEMORY] [--storage STORAGE] [--gpu GPU] [--kube_config_path KUBE_CONFIG_PATH] [--cluster_name CLUSTER_NAME] [--context_name CONTEXT_NAME] [--namespace NAMESPACE] [--labels LABELS] [--annotations ANNOTATIONS] [--api_key API_KEY] [--api_host API_HOST] [--verify_ssl VERIFY_SSL] [--ssl_ca_cert SSL_CA_CERT] name image command</p> <p>name</p> <p>:   Name of the job.</p> <p>image</p> <p>:   Docker image for the job.</p> <p>command</p> <p>:   Command to run in the container.</p> <p>Options genius job create</p> <p>--env_vars ENV_VARS:   Environment variables as a JSON string. --cpu CPU:   CPU requirements. --memory MEMORY:   Memory requirements. --storage STORAGE:   Storage requirements. --gpu GPU:   GPU requirements. --kube_config_path KUBE_CONFIG_PATH:   Path to the kubeconfig file. --cluster_name CLUSTER_NAME:   Name of the Kubernetes cluster. --context_name CONTEXT_NAME:   Name of the kubeconfig context. --namespace NAMESPACE:   Kubernetes namespace. --labels LABELS:   Labels for Kubernetes resources, as a JSON string. --annotations ANNOTATIONS:   Annotations for Kubernetes resources, as a JSON string. --api_key API_KEY:   API key for Kubernetes cluster. --api_host API_HOST:   API host for Kubernetes cluster. --verify_ssl VERIFY_SSL:   Whether to verify SSL certificates. --ssl_ca_cert SSL_CA_CERT:   Path to the SSL CA certificate.</p>"},{"location":"guides/cli/#command-genius-job-delete","title":"Command: genius job delete","text":"<p>usage: genius job delete [-h] [--kube_config_path KUBE_CONFIG_PATH] [--cluster_name CLUSTER_NAME] [--context_name CONTEXT_NAME] [--namespace NAMESPACE] [--labels LABELS] [--annotations ANNOTATIONS] [--api_key API_KEY] [--api_host API_HOST] [--verify_ssl VERIFY_SSL] [--ssl_ca_cert SSL_CA_CERT] name</p> <p>name</p> <p>:   Name of the job.</p> <p>Options genius job delete</p> <p>--kube_config_path KUBE_CONFIG_PATH:   Path to the kubeconfig file. --cluster_name CLUSTER_NAME:   Name of the Kubernetes cluster. --context_name CONTEXT_NAME:   Name of the kubeconfig context. --namespace NAMESPACE:   Kubernetes namespace. --labels LABELS:   Labels for Kubernetes resources, as a JSON string. --annotations ANNOTATIONS:   Annotations for Kubernetes resources, as a JSON string. --api_key API_KEY:   API key for Kubernetes cluster. --api_host API_HOST:   API host for Kubernetes cluster. --verify_ssl VERIFY_SSL:   Whether to verify SSL certificates. --ssl_ca_cert SSL_CA_CERT:   Path to the SSL CA certificate.</p>"},{"location":"guides/cli/#command-genius-job-status","title":"Command: genius job status","text":"<p>usage: genius job status [-h] [--kube_config_path KUBE_CONFIG_PATH] [--cluster_name CLUSTER_NAME] [--context_name CONTEXT_NAME] [--namespace NAMESPACE] [--labels LABELS] [--annotations ANNOTATIONS] [--api_key API_KEY] [--api_host API_HOST] [--verify_ssl VERIFY_SSL] [--ssl_ca_cert SSL_CA_CERT] name</p> <p>name</p> <p>:   Name of the job.</p> <p>Options genius job status</p> <p>--kube_config_path KUBE_CONFIG_PATH:   Path to the kubeconfig file. --cluster_name CLUSTER_NAME:   Name of the Kubernetes cluster. --context_name CONTEXT_NAME:   Name of the kubeconfig context. --namespace NAMESPACE:   Kubernetes namespace. --labels LABELS:   Labels for Kubernetes resources, as a JSON string. --annotations ANNOTATIONS:   Annotations for Kubernetes resources, as a JSON string. --api_key API_KEY:   API key for Kubernetes cluster. --api_host API_HOST:   API host for Kubernetes cluster. --verify_ssl VERIFY_SSL:   Whether to verify SSL certificates. --ssl_ca_cert SSL_CA_CERT:   Path to the SSL CA certificate.</p>"},{"location":"guides/cli/#kubernetes-cron-jobs","title":"Kubernetes Cron Jobs","text":""},{"location":"guides/cli/#command-genius-cron_job","title":"Command: genius cron_job","text":"<p>usage: genius cron_job [-h] {create_cronjob,delete_cronjob,get_cronjob_status} ...</p> <p>POSITIONAL ARGUMENTS genius cron_job</p> <p>genius cron_job create_cronjob</p> <p>:   Create a new cronjob.</p> <p>genius cron_job delete_cronjob</p> <p>:   Delete a cronjob.</p> <p>genius cron_job get_cronjob_status</p> <p>:   Get the status of a cronjob.</p>"},{"location":"guides/cli/#command-genius-cron_job-create_cronjob","title":"Command: genius cron_job create_cronjob","text":"<p>usage: genius cron_job create_cronjob [-h] [--env_vars ENV_VARS] [--cpu CPU] [--memory MEMORY] [--storage STORAGE] [--gpu GPU] [--kube_config_path KUBE_CONFIG_PATH] [--cluster_name CLUSTER_NAME] [--context_name CONTEXT_NAME] [--namespace NAMESPACE] [--labels LABELS] [--annotations ANNOTATIONS] [--api_key API_KEY] [--api_host API_HOST] [--verify_ssl VERIFY_SSL] [--ssl_ca_cert SSL_CA_CERT] name image command schedule</p> <p>name</p> <p>:   Name of the cronjob.</p> <p>image</p> <p>:   Docker image for the cronjob.</p> <p>command</p> <p>:   Command to run in the container.</p> <p>schedule</p> <p>:   Cron schedule.</p> <p>Options genius cron_job create_cronjob</p> <p>--env_vars ENV_VARS:   Environment variables as a JSON string. --cpu CPU:   CPU requirements. --memory MEMORY:   Memory requirements. --storage STORAGE:   Storage requirements. --gpu GPU:   GPU requirements. --kube_config_path KUBE_CONFIG_PATH:   Path to the kubeconfig file. --cluster_name CLUSTER_NAME:   Name of the Kubernetes cluster. --context_name CONTEXT_NAME:   Name of the kubeconfig context. --namespace NAMESPACE:   Kubernetes namespace. --labels LABELS:   Labels for Kubernetes resources, as a JSON string. --annotations ANNOTATIONS:   Annotations for Kubernetes resources, as a JSON string. --api_key API_KEY:   API key for Kubernetes cluster. --api_host API_HOST:   API host for Kubernetes cluster. --verify_ssl VERIFY_SSL:   Whether to verify SSL certificates. --ssl_ca_cert SSL_CA_CERT:   Path to the SSL CA certificate.</p>"},{"location":"guides/cli/#command-genius-cron_job-delete_cronjob","title":"Command: genius cron_job delete_cronjob","text":"<p>usage: genius cron_job delete_cronjob [-h] [--kube_config_path KUBE_CONFIG_PATH] [--cluster_name CLUSTER_NAME] [--context_name CONTEXT_NAME] [--namespace NAMESPACE] [--labels LABELS] [--annotations ANNOTATIONS] [--api_key API_KEY] [--api_host API_HOST] [--verify_ssl VERIFY_SSL] [--ssl_ca_cert SSL_CA_CERT] name</p> <p>name</p> <p>:   Name of the cronjob.</p> <p>Options genius cron_job delete_cronjob</p> <p>--kube_config_path KUBE_CONFIG_PATH:   Path to the kubeconfig file. --cluster_name CLUSTER_NAME:   Name of the Kubernetes cluster. --context_name CONTEXT_NAME:   Name of the kubeconfig context. --namespace NAMESPACE:   Kubernetes namespace. --labels LABELS:   Labels for Kubernetes resources, as a JSON string. --annotations ANNOTATIONS:   Annotations for Kubernetes resources, as a JSON string. --api_key API_KEY:   API key for Kubernetes cluster. --api_host API_HOST:   API host for Kubernetes cluster. --verify_ssl VERIFY_SSL:   Whether to verify SSL certificates. --ssl_ca_cert SSL_CA_CERT:   Path to the SSL CA certificate.</p>"},{"location":"guides/cli/#command-genius-cron_job-get_cronjob_status","title":"Command: genius cron_job get_cronjob_status","text":"<p>usage: genius cron_job get_cronjob_status [-h] [--kube_config_path KUBE_CONFIG_PATH] [--cluster_name CLUSTER_NAME] [--context_name CONTEXT_NAME] [--namespace NAMESPACE] [--labels LABELS] [--annotations ANNOTATIONS] [--api_key API_KEY] [--api_host API_HOST] [--verify_ssl VERIFY_SSL] [--ssl_ca_cert SSL_CA_CERT] name</p> <p>name</p> <p>:   Name of the cronjob.</p> <p>Options genius cron_job get_cronjob_status</p> <p>--kube_config_path KUBE_CONFIG_PATH:   Path to the kubeconfig file. --cluster_name CLUSTER_NAME:   Name of the Kubernetes cluster. --context_name CONTEXT_NAME:   Name of the kubeconfig context. --namespace NAMESPACE:   Kubernetes namespace. --labels LABELS:   Labels for Kubernetes resources, as a JSON string. --annotations ANNOTATIONS:   Annotations for Kubernetes resources, as a JSON string. --api_key API_KEY:   API key for Kubernetes cluster. --api_host API_HOST:   API host for Kubernetes cluster. --verify_ssl VERIFY_SSL:   Whether to verify SSL certificates. --ssl_ca_cert SSL_CA_CERT:   Path to the SSL CA certificate.</p>"},{"location":"guides/cli/#packaging","title":"Packaging","text":""},{"location":"guides/cli/#command-genius-docker-package","title":"Command: genius docker package","text":"<p>Usage: genius docker package [-h] [--auth AUTH] [--base_image BASE_IMAGE] [--workdir WORKDIR] [--local_dir LOCAL_DIR] [--packages [PACKAGES ...]] [--os_packages [OS_PACKAGES ...]] [--env_vars ENV_VARS] image_name repository</p> <p>image_name</p> <p>:   Name of the Docker image.</p> <p>repository</p> <p>:   Container repository to upload to.</p> <p>Options genius docker package</p> <p>--auth AUTH:   Authentication credentials as a JSON string. --base_image BASE_IMAGE:   The base image to use for the Docker container. --workdir WORKDIR:   The working directory in the Docker container. --local_dir LOCAL_DIR:   The local directory to copy into the Docker container. --packages [PACKAGES ...]:   List of Python packages to install in the Docker container. --os_packages [OS_PACKAGES ...]:   List of OS packages to install in the Docker container. --env_vars ENV_VARS:   Environment variables to set in the Docker container.</p>"},{"location":"guides/cli/#miscellaneous","title":"Miscellaneous","text":""},{"location":"guides/cli/#command-genius-plugins","title":"Command: genius plugins","text":"<p>Usage: genius plugins [-h] [spout_or_bolt]</p> <p>spout_or_bolt</p> <p>:   The spout or bolt to print help for.</p>"},{"location":"guides/cli/#command-genius-list","title":"Command: genius list","text":"<p>Usage: genius list [-h] [--verbose]</p> <p>Options genius list</p> <p>--verbose:   Print verbose output.</p>"},{"location":"guides/concepts/","title":"Concepts","text":"<p>The Geniusrise framework is built around loosely-coupled modules acting as a cohesive adhesive between distinct, modular components, much like how one would piece together Lego blocks. This design approach not only promotes flexibility but also ensures that each module or \"Lego block\" remains sufficiently independent. Such independence is crucial for diverse teams, each with its own unique infrastructure and requirements, to seamlessly build and manage their respective components.</p> <p>Geniusrise comes with a sizable set of plugins which implement various features and integrations. The independence and modularity of the design enable sharing of these building blocks in the community.</p>"},{"location":"guides/concepts/#concepts_1","title":"Concepts","text":"<ol> <li> <p>Task: At its core, a task represents a discrete unit of work within the Geniusrise framework. Think of it as a singular action or operation that the system needs to execute. A task further manifests itself into a Bolt or a Spout as stated below.</p> </li> <li> <p>Components of a Task: Each task is equipped with four components:</p> <ol> <li>State Manager: This component is responsible for continuously monitoring and managing the task's state, ensuring that it progresses smoothly from initiation to completion and to report errors and ship logs into a central location.</li> <li>Data Manager: As the name suggests, the Data Manager oversees the input and output data associated with a task, ensuring data integrity and efficient data flow. It also ensures data sanity follows partition semantics and isolation.</li> <li>Runner: These are wrappers for executing a task on various platforms. Depending on the platform, the runner ensures that the task is executed seamlessly.</li> </ol> </li> <li> <p>Task Classification: Tasks within the Geniusrise framework can be broadly classified into two categories:</p> <ul> <li>Spout: If a task's primary function is to ingest or bring in data, it's termed as a 'spout'.</li> <li>Bolt: For tasks that don't primarily ingest data but perform other operations, they are termed 'bolts'.</li> </ul> </li> </ol> <p>The beauty of the Geniusrise framework lies in its adaptability. Developers can script their workflow components once and have the freedom to deploy them across various platforms. To facilitate this, Geniusrise offers:</p> <ol> <li>Runners for Task Execution: Geniusrise is equipped with a diverse set of runners, each tailored for different platforms, ensuring that tasks can be executed almost anywhere:<ol> <li>On your local machine for quick testing and development.</li> <li>Within Docker containers for isolated, reproducible environments.</li> <li>On Kubernetes clusters for scalable, cloud-native deployments.</li> <li>Using Apache Airflow for complex workflow orchestration. (Coming Soon).</li> <li>On AWS ECS for containerized application management. (Coming Soon).</li> <li>With AWS Batch for efficient batch computing workloads. (Coming Soon).</li> <li>With Docker Swarm clusters as an alternative orchestrator to kubernetes. (Coming Soon).</li> </ol> </li> </ol> <p>This document delves into the core components and concepts that make up the Geniusrise framework.</p>"},{"location":"guides/concepts/#tradeoffs","title":"Tradeoffs","text":"<p>Because of the very loose coupling of the components, though the framework can be used to build very complex networks with independently running nodes, it provides limited orchestration capability, like synchronous pipelines. An external orchestrator like airflow can be used in such cases to orchestrate geniusrise components.</p>"},{"location":"guides/deployment/","title":"Deployment","text":""},{"location":"guides/deployment/#introduction","title":"Introduction","text":"<p>This guide provides comprehensive instructions on how to deploy and manage resources in a Kubernetes cluster using the Geniusrise platform. The guide covers the following functionalities:</p> <ul> <li>Connecting to a Kubernetes cluster</li> <li>Managing Pods</li> <li>Managing Deployments</li> <li>Managing Services</li> <li>Managing Jobs</li> <li>Managing Cron jobs</li> </ul>"},{"location":"guides/deployment/#prerequisites","title":"Prerequisites","text":"<ul> <li>A working Kubernetes cluster</li> <li>Kubeconfig file for cluster access</li> <li>Python 3.x installed</li> <li>Geniusrise CLI installed</li> </ul>"},{"location":"guides/deployment/#connecting-to-a-kubernetes-cluster","title":"Connecting to a Kubernetes Cluster","text":"<p>Before performing any operations, you need to connect to your Kubernetes cluster. You can do this in two ways:</p> <ol> <li>Using a kubeconfig file and context name</li> <li>Using an API key and API host</li> </ol>"},{"location":"guides/deployment/#using-kubeconfig-and-context-name","title":"Using Kubeconfig and Context Name","text":"<pre><code>genius k8s &lt;command&gt; --kube_config_path /path/to/kubeconfig.yaml --context_name my-context\n</code></pre>"},{"location":"guides/deployment/#using-api-key-and-api-host","title":"Using API Key and API Host","text":"<pre><code>genius k8s &lt;command&gt; --api_key my-api-key --api_host https://api.k8s.my-cluster.com --namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre>"},{"location":"guides/deployment/#managing-pods","title":"Managing Pods","text":""},{"location":"guides/deployment/#checking-pod-status","title":"Checking Pod Status","text":"<p>To get the status of a specific pod:</p> <pre><code>genius k8s status my-pod-name --namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre>"},{"location":"guides/deployment/#listing-all-pods","title":"Listing All Pods","text":"<p>To list all the pods in the current namespace:</p> <pre><code>genius k8s show --namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre>"},{"location":"guides/deployment/#describing-a-pod","title":"Describing a Pod","text":"<p>To get detailed information about a specific pod:</p> <pre><code>genius k8s describe my-pod-name --namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre>"},{"location":"guides/deployment/#fetching-pod-logs","title":"Fetching Pod Logs","text":"<p>To get the logs of a specific pod:</p> <pre><code>genius k8s logs my-pod-name --namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre>"},{"location":"guides/deployment/#managing-deployments","title":"Managing Deployments","text":""},{"location":"guides/deployment/#creating-a-new-deployment","title":"Creating a New Deployment","text":"<p>To create a new deployment:</p> <pre><code>genius deployment create --name my-deployment --image my-image --command \"echo hello\" --namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre>"},{"location":"guides/deployment/#scaling-a-deployment","title":"Scaling a Deployment","text":"<p>To scale a deployment:</p> <pre><code>genius deployment scale --name my-deployment --replicas 3 --namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre>"},{"location":"guides/deployment/#listing-all-deployments","title":"Listing All Deployments","text":"<p>To list all deployments:</p> <pre><code>genius deployment show\n</code></pre>"},{"location":"guides/deployment/#describing-a-deployment","title":"Describing a Deployment","text":"<p>To describe a specific deployment:</p> <pre><code>genius deployment describe my-deployment --namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre>"},{"location":"guides/deployment/#deleting-a-deployment","title":"Deleting a Deployment","text":"<p>To delete a deployment:</p> <pre><code>genius deployment delete my-deployment --namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre>"},{"location":"guides/deployment/#checking-deployment-status","title":"Checking Deployment Status","text":"<p>To check the status of a deployment:</p> <pre><code>genius deployment status my-deployment --namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre>"},{"location":"guides/deployment/#advanced-features","title":"Advanced Features","text":""},{"location":"guides/deployment/#environment-variables","title":"Environment Variables","text":"<p>You can pass environment variables to your pods and deployments like so:</p> <pre><code>genius deployment create --name my-deployment --image my-image --command \"echo hello\" --env_vars '{\"MY_VAR\": \"value\"}' --namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre>"},{"location":"guides/deployment/#resource-requirements","title":"Resource Requirements","text":"<p>You can specify resource requirements for your pods and deployments:</p> <pre><code>genius deployment create --name my-deployment --image my-image --command \"echo hello\" --cpu \"100m\" --memory \"256Mi\" --namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre>"},{"location":"guides/deployment/#gpu-support","title":"GPU Support","text":"<p>To allocate GPUs to your pods:</p> <pre><code>genius deployment create --name my-deployment --image my-image --command \"echo hello\" --gpu \"1\" --namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre>"},{"location":"guides/deployment/#managing-services","title":"Managing Services","text":""},{"location":"guides/deployment/#creating-a-new-service","title":"Creating a New Service","text":"<p>To create a new service:</p> <pre><code>genius service create --name example-service --image example-image --command \"echo hello\" --port 8080 --target_port 8080 --namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre>"},{"location":"guides/deployment/#deleting-a-service","title":"Deleting a Service","text":"<p>To delete a service:</p> <pre><code>genius service delete --name example-service --namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre>"},{"location":"guides/deployment/#describing-a-service","title":"Describing a Service","text":"<p>To describe a specific service:</p> <pre><code>genius service describe --name example-service --namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre>"},{"location":"guides/deployment/#listing-all-services","title":"Listing All Services","text":"<p>To list all services:</p> <pre><code>genius service show --namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre>"},{"location":"guides/deployment/#managing-jobs","title":"Managing Jobs","text":""},{"location":"guides/deployment/#creating-a-new-job","title":"Creating a New Job","text":"<p>To create a new job:</p> <pre><code>genius job create --name example-job --image example-image --command \"echo hello\" --cpu \"100m\" --memory \"256Mi\" --namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre>"},{"location":"guides/deployment/#deleting-a-job","title":"Deleting a Job","text":"<p>To delete a job:</p> <pre><code>genius job delete --name example-job --namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre>"},{"location":"guides/deployment/#checking-job-status","title":"Checking Job Status","text":"<p>To check the status of a job:</p> <pre><code>genius job status --name example-job --namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre>"},{"location":"guides/deployment/#managing-cron-jobs","title":"Managing Cron Jobs","text":""},{"location":"guides/deployment/#creating-a-new-cron-job","title":"Creating a New Cron Job","text":"<p>To create a new cron job, you can use the <code>create_cronjob</code> sub-command. You'll need to specify the name, Docker image, command to run, and the cron schedule.</p> <pre><code>genius cronjob create_cronjob --name example-cronjob --image example-image --command \"echo hello\" --schedule \"*/5 * * * *\" --namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre>"},{"location":"guides/deployment/#additional-options","title":"Additional Options","text":"<ul> <li><code>--env_vars</code>: To set environment variables, pass them as a JSON string.</li> <li><code>--cpu</code>, <code>--memory</code>, <code>--storage</code>, <code>--gpu</code>: To set resource requirements.</li> </ul>"},{"location":"guides/deployment/#deleting-a-cron-job","title":"Deleting a Cron Job","text":"<p>To delete a cron job, use the <code>delete_cronjob</code> sub-command and specify the name of the cron job.</p> <pre><code>genius cronjob delete_cronjob --name example-cronjob --namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre>"},{"location":"guides/deployment/#checking-cron-job-status","title":"Checking Cron Job Status","text":"<p>To check the status of a cron job, use the <code>get_cronjob_status</code> sub-command and specify the name of the cron job.</p> <pre><code>genius cronjob get_cronjob_status --name example-cronjob --namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre>"},{"location":"guides/deployment/#advanced-features-for-cron-jobs","title":"Advanced Features for Cron Jobs","text":""},{"location":"guides/deployment/#environment-variables_1","title":"Environment Variables","text":"<p>You can pass environment variables to your cron jobs like so:</p> <pre><code>genius cronjob create_cronjob --name example-cronjob --image example-image --command \"echo hello\" --schedule \"*/5 * * * *\" --env_vars '{\"MY_VAR\": \"value\"}' --namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre>"},{"location":"guides/deployment/#resource-requirements_1","title":"Resource Requirements","text":"<p>You can specify resource requirements for your cron jobs:</p> <pre><code>genius cronjob create_cronjob --name example-cronjob --image example-image --command \"echo hello\" --schedule \"*/5 * * * *\" --cpu \"100m\" --memory \"256Mi\" --namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre>"},{"location":"guides/deployment/#gpu-support_1","title":"GPU Support","text":"<p>To allocate GPUs to your cron jobs:</p> <pre><code>genius cronjob create_cronjob --name example-cronjob --image example-image --command \"echo hello\" --schedule \"*/5 * * * *\" --gpu \"1\" --namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre>"},{"location":"guides/dev_cycle/","title":"Dev Cycle","text":"<p>This document describes one full local development cycle.</p> <p>Lets say we want to build a pipeline which tags medical terms in EHR documents.</p>"},{"location":"guides/dev_cycle/#strategies","title":"Strategies","text":"<p>Pre-requisites:</p> <ol> <li>SNOMED-CT: is a knowledge graph of standard medical terminology</li> <li>IHTSDO: a standards body for medical terminologies in a number of countries.</li> <li>UMLS: unified medical language system is a set of files and software that brings together many health and biomedical vocabularies and standards together.</li> </ol>"},{"location":"guides/dev_cycle/#strategy-1-named-entity-recognition","title":"Strategy 1: Named entity recognition","text":""},{"location":"guides/dev_cycle/#1-create-a-labelled-dataset","title":"1. Create a labelled dataset","text":"<p>We need a corpus of documents with medical terms labeled. For example, we could use wikipedia + wikidata to build such a dataset, given entities in wikipedia are linked and indexed in the wikidata knowledge graph. Reference: Building a Massive Corpus for Named Entity Recognition using Free Open Data Sources. We could also annotate medical datasets like MIMIC-III annotated with SNOMED-CT based MedCAT which is a medical annotation tool developed on the knowledge graph of medical terminology (SNOMED-CT), as it would be more pertinent to our usecase, reference: DNER Clinical (named entity recognition) from free clinical text to Snomed-CT concept</p>"},{"location":"guides/dev_cycle/#2-train-a-model-on-the-ner-dataset","title":"2. Train a model on the NER dataset","text":"<p>We could choose a large language model and train the model on the NER fine-tuning task. The model would then be able to recognize and tag medical terms in any given text data.</p>"},{"location":"guides/dev_cycle/#strategy-2-vector-knowledge-graph-search","title":"Strategy 2: Vector knowledge graph search","text":""},{"location":"guides/dev_cycle/#1-create-a-vectorized-knowledge-graph","title":"1. Create a vectorized knowledge graph","text":"<p>We use an LLM to create a vectorized layer over SNOMED-CT. This layer can be used to semantically search for \"seed\" nodes in the graph. We can then use these seed nodes to traverse nodes a few hops adjacent to the seed nodes.</p>"},{"location":"guides/dev_cycle/#2-retrieval-augmented-ner","title":"2. Retrieval Augmented NER","text":"<p>We use the knowledge graph search results to not only annotate each node seen in the EHR document, but also add additional information about those nodes derived from its adjacent nodes. But first, we also need to make sure that we query the right information instead of simply vectorized chunks and throwing it at semantic search. We would need a \"traditional\" pipeline for this - lemmatization followed by POS tagging. We use both proper nouns and out of vocabulary words as search query terms.</p>"},{"location":"guides/dev_cycle/#boilerplate","title":"Boilerplate","text":"<p>To setup a local geniusrise project, simply use the geniusrise project creator script:</p> <pre><code>curl -L https://geniusrise.new | bash # TODO: host this or create a template github repo\n</code></pre>"},{"location":"guides/dev_cycle/#existing-project","title":"Existing project","text":"<p>If you wish to add geniusrise to an existing project:</p> <pre><code>pip install geniusrise\npip freeze &gt; requirements.txt\n</code></pre>"},{"location":"guides/dev_cycle/#from-scratch","title":"From scratch","text":"<p>Here is how to set up from scratch:</p> <pre><code>#!/bin/bash\n# Prompt for project details\nread -p \"Enter your project name: \" project_name\nread -p \"Enter your name: \" author_name\nread -p \"Enter your email: \" author_email\nread -p \"Enter your GitHub username: \" github_username\nread -p \"Enter a brief description of your project: \" project_description\n# Create project structure\nmkdir $project_name\ncd $project_name\nmkdir $project_name tests\n# Create basic files\ntouch README.md\ntouch requirements.txt\ntouch setup.py\ntouch Makefile\ntouch $project_name/__init__.py\ntouch tests/__init__.py\n# Populate README.md\necho \"# $project_name\" &gt; README.md\necho \"\\n$project_description\" &gt;&gt; README.md\n# Populate setup.py\ncat &lt;&lt;EOL &gt; setup.py\nfrom setuptools import setup, find_packages\nwith open(\"README.md\", \"r\", encoding=\"utf-8\") as fh:\nlong_description = fh.read()\nsetup(\nname='$project_name',\nversion='0.1.0',\npackages=find_packages(exclude=[\"tests\", \"tests.*\"]),\ninstall_requires=[],\npython_requires='&gt;=3.10',\nauthor='$author_name',\nauthor_email='$author_email',\ndescription='$project_description',\nlong_description=long_description,\nlong_description_content_type='text/markdown',\nurl='https://github.com/$github_username/$project_name',\nclassifiers=[\n'Programming Language :: Python :: 3',\n'License :: OSI Approved :: MIT License',\n'Operating System :: OS Independent',\n],\n)\nEOL\n# Populate Makefile\ncat &lt;&lt;EOL &gt; Makefile\nsetup:\n@pip install -r ./requirements.txt\ntest:\n@coverage run -m pytest -v ./tests\npublish:\n@python setup.py sdist bdist_wheel\n@twine upload dist/$project_name-\\$${VERSION}-* --verbose\nEOL\n# Set up the virtual environment and install necessary packages\nvirtualenv venv -p `which python3.10`\nsource venv/bin/activate\npip install twine setuptools pytest coverage geniusrise\npip freeze &gt; requirements.txt\n# Fetch .pre-commit-config.yaml and .gitignore from geniusrise/geniusrise\ncurl -O https://raw.githubusercontent.com/geniusrise/geniusrise/master/.pre-commit-config.yaml\ncurl -O https://raw.githubusercontent.com/geniusrise/geniusrise/master/.gitignore\necho \"Project $project_name initialized!\"\n</code></pre> <p>Create a install script out of this and execute it:</p> <pre><code>touch install.sh\nchmod +x ./install.sh\n./install.sh\n</code></pre>"},{"location":"guides/dev_cycle/#preparing-the-knowledge-graph","title":"Preparing the knowledge graph","text":"<p>Lets prepare the knowledge graph by vectorizing each node's knowledge into a vectorized flat memory. This is a periodic activity that one needs to do whenever a new version of SNOMED-CT is released (typically bi-annually).</p> <p>We use the international version of SNOMED-CT from https://www.nlm.nih.gov/healthit/snomedct/international.html.</p> <pre><code>mkdir data\ncd data\n</code></pre> <p>Go to UMLS or IHTSDO website, register, agree to the agreements and after approval, download the knowledge graph.</p> <p></p> <p>Unzip the file</p> <pre><code>unzip SnomedCT_InternationalRF2_PRODUCTION_20230901T120000Z.zip\n</code></pre>"},{"location":"guides/dev_cycle/#todo","title":"TODO \ud83d\ude22","text":"<p>Need to document https://github.com/geniusrise/geniusrise-healthcare</p>"},{"location":"guides/installation/","title":"Installation","text":"<p>Geniusrise is composed of the core framework and various plugins that implement specific tasks. The core has to be installed first, and after that selected plugins can be installed as and when required.</p>"},{"location":"guides/installation/#installing-geniusrise","title":"Installing Geniusrise","text":""},{"location":"guides/installation/#using-pip","title":"Using pip","text":"<p>To install the core framework using pip in local env, simply run:</p> <pre><code>pip install geniusrise\n</code></pre> <p>Or if you wish to install at user level:</p> <pre><code>pip install generiusrise --user\n</code></pre> <p>Or on a global level (might conflict with your OS package manager):</p> <pre><code>sudo pip install geniusrise\n</code></pre> <p>To verify the installation, you can check whether the geniusrise binary exists in PATH:</p> <p><pre><code>which genius\n\ngenius --help\n</code></pre> &lt;!--</p>"},{"location":"guides/installation/#docker","title":"Docker","text":"<p>Geniusrise containers are available on Docker hub.</p> <pre><code>docker run -it --rm geniusrise/geniusrise:latest\n``` --&gt;\n\n## Installing Plugins\n---\n\nGeniusrise offers a variety of plugins that act as composable lego blocks. To install a specific plugin, use the following format:\n\n```bash\npip install geniusrise-&lt;plugin-name&gt;\n</code></pre> <p>Replace <code>&lt;plugin-name&gt;</code> with the name of the desired plugin.</p> <p>Available plugins are:</p> <ol> <li>geniusrise-text: bolts for text models</li> <li>geniusrise-vision: bolts for vision models</li> <li>geniusrise-audio: bolts for audio models</li> <li>geniusrise-openai: bolts for openai</li> <li>geniusrise-listeners: spouts for streaming event listeners</li> <li>geniusrise-databases: spouts for databases</li> </ol> <p>Please visit https://github.com/geniusrise for a complete list of available plugins.</p>"},{"location":"guides/installation/#using-conda","title":"Using Conda","text":"<ol> <li>Activate the environment:</li> </ol> <pre><code>conda activate your-env\n</code></pre> <ol> <li>Install Geniusrise:</li> </ol> <pre><code>pip install geniusrise\n</code></pre>"},{"location":"guides/installation/#using-poetry","title":"Using Poetry","text":"<ol> <li>Add Geniusrise as a dependency:</li> </ol> <pre><code>poetry add geniusrise\n</code></pre> <p>For plugins:</p> <pre><code>poetry add geniusrise-&lt;plugin-name&gt;\n</code></pre>"},{"location":"guides/installation/#development","title":"Development","text":"<p>For development, you may want to install from the repo:</p> <pre><code>git clone git@github.com:geniusrise/geniusrise.git\ncd geniusrise\nvirtualenv venv -p `which python3.10`\nsource venv/bin/activate\npip install -r ./requirements.txt\n\nmake install # installs in your local venv directory\n</code></pre> <p>That's it! You've successfully installed Geniusrise and its plugins. \ud83c\udf89</p>"},{"location":"guides/installation/#alternative-methods-todo","title":"Alternative Methods: TODO \ud83d\ude2d","text":""},{"location":"guides/installation/#using-package-managers","title":"Using package managers","text":"<p>Geniusrise is also available as native packages for some Linux distributions.</p>"},{"location":"guides/installation/#aur","title":"AUR","text":"<p>Geniusrise is available on the AUR for arch and derived distros.</p> <pre><code>yay -S geniusrise\n</code></pre> <p>or directly from git master:</p> <pre><code>yay -S geniusrise-git\n</code></pre>"},{"location":"guides/installation/#ppa","title":"PPA","text":"<p>Geniusrise is also available on the PPA for debian-based distros.</p> <p>Coming soon \ud83d\ude22</p>"},{"location":"guides/installation/#brew-cask","title":"Brew (cask)","text":"<p>Coming soon \ud83d\ude22</p>"},{"location":"guides/installation/#nix","title":"Nix","text":"<p>Coming soon \ud83d\ude22</p>"},{"location":"guides/kubernetes/","title":"Kubernetes Runner","text":""},{"location":"guides/kubernetes/#overview","title":"Overview","text":"<p>This runner module enables running spouts or bolts on Kubernetes. It provides the ability to:</p> <ol> <li>create</li> <li>delete</li> <li>scale</li> <li>describe</li> </ol> <p>various Kubernetes resources like</p> <ol> <li>Pods</li> <li>Deployments</li> <li>Services</li> </ol>"},{"location":"guides/kubernetes/#command-line-interface","title":"Command-Line Interface","text":"<p>The following commands are available:</p> <ol> <li><code>create</code>: Create a Kubernetes resource.</li> <li><code>delete</code>: Delete a Kubernetes resource.</li> <li><code>status</code>: Get the status of a Kubernetes resource.</li> <li><code>logs</code>: Get logs of a Kubernetes resource.</li> <li><code>pod</code>: Describe a Kubernetes pod.</li> <li><code>pods</code>: List all pods.</li> <li><code>service</code>: Describe a Kubernetes service.</li> <li><code>services</code>: List all services.</li> <li><code>deployment</code>: Describe a Kubernetes deployment.</li> <li><code>deployments</code>: List all deployments.</li> <li><code>scale</code>: Scale a Kubernetes deployment.</li> </ol>"},{"location":"guides/kubernetes/#common-arguments","title":"Common Arguments","text":"<p>These arguments are common to all commands:</p> <ul> <li><code>--kube_config_path</code>: Path to the kubeconfig file.</li> <li><code>--cluster_name</code>: Name of the Kubernetes cluster.</li> <li><code>--context_name</code>: Name of the kubeconfig context.</li> <li><code>--namespace</code>: Kubernetes namespace (default is \"default\").</li> <li><code>--labels</code>: Labels for Kubernetes resources, as a JSON string.</li> <li><code>--annotations</code>: Annotations for Kubernetes resources, as a JSON string.</li> <li><code>--api_key</code>: API key for Kubernetes cluster.</li> <li><code>--api_host</code>: API host for Kubernetes cluster.</li> <li><code>--verify_ssl</code>: Whether to verify SSL certificates (default is True).</li> <li><code>--ssl_ca_cert</code>: Path to the SSL CA certificate.</li> </ul>"},{"location":"guides/kubernetes/#create_resource","title":"<code>create_resource</code>","text":"<p>Create a Kubernetes resource.</p> <ul> <li><code>name</code>: Name of the resource.</li> <li><code>image</code>: Docker image for the resource.</li> <li><code>command</code>: Command to run in the container.</li> <li><code>--registry_creds</code>: Credentials for Docker registry, as a JSON string.</li> <li><code>--is_service</code>: Whether this is a service (default is False).</li> <li><code>--replicas</code>: Number of replicas (default is 1).</li> <li><code>--port</code>: Service port (default is 80).</li> <li><code>--target_port</code>: Container target port (default is 8080).</li> <li><code>--env_vars</code>: Environment variables, as a JSON string.</li> </ul> <p>Example:</p> <pre><code>python script.py create_resource my_resource nginx \"nginx -g 'daemon off;'\" --replicas=3\n</code></pre>"},{"location":"guides/kubernetes/#delete_resource","title":"<code>delete_resource</code>","text":"<p>Delete a Kubernetes resource.</p> <ul> <li><code>name</code>: Name of the resource.</li> <li><code>--is_service</code>: Whether this is a service (default is False).</li> </ul> <p>Example:</p> <pre><code>python script.py delete_resource my_resource\n</code></pre>"},{"location":"guides/kubernetes/#get_status","title":"<code>get_status</code>","text":"<p>Get the status of a Kubernetes resource.</p> <ul> <li><code>name</code>: Name of the resource.</li> </ul> <p>Example:</p> <pre><code>python script.py get_status my_resource\n</code></pre>"},{"location":"guides/kubernetes/#get_logs","title":"<code>get_logs</code>","text":"<p>Get logs of a Kubernetes resource.</p> <ul> <li><code>name</code>: Name of the resource.</li> <li><code>--tail_lines</code>: Number of lines to tail (default is 10).</li> </ul> <p>Example:</p> <pre><code>python script.py get_logs my_resource --tail_lines=20\n</code></pre>"},{"location":"guides/kubernetes/#scale","title":"<code>scale</code>","text":"<p>Scale a Kubernetes deployment.</p> <ul> <li><code>name</code>: Name of the deployment.</li> <li><code>replicas</code>: Number of replicas.</li> </ul> <p>Example:</p> <pre><code>python script.py scale my_resource 5\n</code></pre>"},{"location":"guides/kubernetes/#list_pods-list_services-list_deployments","title":"<code>list_pods</code>, <code>list_services</code>, <code>list_deployments</code>","text":"<p>List all pods, services, or deployments.</p> <p>Example:</p> <pre><code>python script.py list_pods\n</code></pre>"},{"location":"guides/kubernetes/#describe_pod-describe_service-describe_deployment","title":"<code>describe_pod</code>, <code>describe_service</code>, <code>describe_deployment</code>","text":"<p>Describe a pod, service, or deployment.</p> <ul> <li><code>name</code>: Name of the resource.</li> </ul> <p>Example:</p> <pre><code>python script.py describe_pod my_pod\n</code></pre>"},{"location":"guides/kubernetes/#yaml-configuration","title":"YAML Configuration","text":"<p>You can also use a YAML configuration file to specify the common arguments. The command-specific arguments will still come from the command line.</p> <p>Example YAML:</p> <pre><code>deploy:\ntype: \"k8s\"\nargs:\nkube_config_path: \"\"\ncluster_name: \"geniusrise\"\ncontext_name: \"eks\"\nnamespace: \"geniusrise_k8s_test\"\nlabels: { \"tag1\": \"lol\", \"tag2\": \"lel\" }\nannotations: {}\napi_key:\napi_host: localhost\nverify_ssl: true\nssl_ca_cert:\n</code></pre> <p>To use the YAML configuration, you can read it in your Python script and pass the arguments to the <code>K8sResourceManager</code> methods.</p> <p>Example:</p> <pre><code>python script.py --config=my_config.yaml create_resource my_resource nginx \"nginx -g 'daemon off;'\" --replicas=3\n</code></pre> <p>In this example, the <code>--config=my_config.yaml</code> would be used to read the common arguments from the YAML file, and the rest of the arguments would be taken from the command line.</p>"},{"location":"guides/local/","title":"Local setup","text":"<p>Lets create a workspace for local experimentation. We will not build anything here, just try to use whatever components are available. This is what a low-code workflow could look like.</p> <p>Lets create a workflow in which:</p> <ol> <li>A web server listens for all kinds of HTTP events.<ol> <li>Clients send the following information to the server:<ol> <li>HTTP request</li> <li>Response and response status code</li> </ol> </li> <li>The server buffers events in batches of 1000 and uploads them on to s3.</li> </ol> </li> <li>Train a small LLM model on the data to be used to predict whether the request was valid.</li> </ol> <p>A representation of the process using a sequence diagram:</p> <pre>58fea0e0-5c21-4ed3-87ff-0e12c327d0ee</pre> <p>This model could be used to predict if a request will fail before serving it. It could also be used to classify requests as malicious etc.</p>"},{"location":"guides/local/#install","title":"Install","text":"<p>Let's start by installing geniusrise and itc components in a local virtual environment.</p> <ol> <li>Create a directory:</li> </ol> <pre><code>mkdir test\ncd test\n</code></pre> <ol> <li>Create a virtualenv:</li> </ol> <pre><code>virtualenv venv\nsource venv/bin/activate\n</code></pre> <ol> <li>Install geniursise</li> </ol> <pre><code>pip install geniusrise\npip install geniusrise-listeners\npip install geniusrise-huggingface\n</code></pre> <ol> <li>Save the installed package versions</li> </ol> <pre><code>pip freeze &gt; requirements.txt\n</code></pre> <ol> <li>Verify if everything is installed:</li> </ol> <pre><code>$ genius list\n\n+--------------------------------------------+-------+\n| Name                                       | Type  |\n+--------------------------------------------+-------+\n| TestSpoutCtlSpout                          | Spout |\n| Kafka                                      | Spout |\n| MQTT                                       | Spout |\n| Quic                                       | Spout |\n| RESTAPIPoll                                | Spout |\n| RabbitMQ                                   | Spout |\n| RedisPubSub                                | Spout |\n| RedisStream                                | Spout |\n| SNS                                        | Spout |\n| SQS                                        | Spout |\n| Udp                                        | Spout |\n| Webhook                                    | Spout |\n| Websocket                                  | Spout |\n| TestBoltCtlBolt                            | Bolt  |\n| HuggingFaceClassificationFineTuner         | Bolt  |\n| HuggingFaceCommonsenseReasoningFineTuner   | Bolt  |\n| HuggingFaceFineTuner                       | Bolt  |\n| HuggingFaceInstructionTuningFineTuner      | Bolt  |\n| HuggingFaceLanguageModelingFineTuner       | Bolt  |\n| HuggingFaceNamedEntityRecognitionFineTuner | Bolt  |\n| HuggingFaceQuestionAnsweringFineTuner      | Bolt  |\n| HuggingFaceSentimentAnalysisFineTuner      | Bolt  |\n| HuggingFaceSummarizationFineTuner          | Bolt  |\n| HuggingFaceTranslationFineTuner            | Bolt  |\n| NamedEntityRecognitionFineTuner            | Bolt  |\n| OpenAIClassificationFineTuner              | Bolt  |\n| OpenAICommonsenseReasoningFineTuner        | Bolt  |\n| OpenAIFineTuner                            | Bolt  |\n| OpenAIInstructionFineTuner                 | Bolt  |\n| OpenAILanguageModelFineTuner               | Bolt  |\n| OpenAIQuestionAnsweringFineTuner           | Bolt  |\n| OpenAISentimentAnalysisFineTuner           | Bolt  |\n| OpenAISummarizationFineTuner               | Bolt  |\n| OpenAITranslationFineTuner                 | Bolt  |\n+--------------------------------------------+-------+\n</code></pre>"},{"location":"guides/local/#input-data","title":"Input Data","text":"<p>Lets start with the server which has to listen for HTTP events. We can use the <code>Webhook</code> listener for this purpose.</p> <p>Next, we have to ask ourselves 2 things:</p> <ol> <li>Where do we want the output?</li> <li>A: in s3 in batches (output = stream_to_batch)</li> <li>Do we want monitoring?</li> <li>A: no (state = none)</li> </ol> <p>Let's run the listener:</p> <pre><code>genius Webhook rise \\\nstream_to_batch \\\n--output_s3_bucket geniusrise-test \\\n--output_s3_folder train \\\nnone \\\nlisten \\\n--args port=8080\n</code></pre> <p>The server should be up with:</p> <pre><code>    \ud83d\ude80 Initialized Task with ID: Webhookaca9cb67-5c41-420c-9445-cf0015d9d866\n    [17/Sep/2023:14:00:18] ENGINE Bus STARTING\nCherryPy Checker:\nThe Application mounted at '' has an empty config.\n\n[17/Sep/2023:14:00:18] ENGINE Started monitor thread 'Autoreloader'.\n    [17/Sep/2023:14:00:18] ENGINE Serving on http://0.0.0.0:8080\n    [17/Sep/2023:14:00:18] ENGINE Bus STARTED\n</code></pre>"},{"location":"guides/local/#data","title":"Data","text":"<p>Lets create some data for testing:</p> <pre><code>while true; do\n# Generate a random customer ID\ncustomer_id=$(( RANDOM % 10000001 ))\n# Determine the status code based on the customer ID\nif [ $customer_id -gt 10000000 ]; then\nstatus_code=\"1\"\nelif [ $customer_id -le 10000 ]; then\nstatus_code=\"1\"\nelse\nstatus_code=\"0\"\nfi\n# Make the API call\ncurl --header \"Content-Type: application/json\" \\\n--request POST \\\n--data \"{\\\"text\\\":\\\"GET /api/v1/customer/$customer_id\\\",\\\"label\\\":\\\"$status_code\\\"}\" \\\nhttp://localhost:8080/application-1-tag-a-tag-b-whatever\ndone\n</code></pre> <p>Verify that the data is being dumped in the right place with the correct format:</p> <pre><code>$ aws s3 ls s3://geniusrise-test/train/\n\n2023-08-11 14:02:47      28700 DGtx4KjVZw5C2gfWmTVCmD.json\n2023-08-11 14:02:50      28700 UYXAvn8JC2yk6pMuAjKMPq.json\n</code></pre> <p>The Webhook spout generates data like this:</p> <pre><code>{'data': {'text': 'GET /api/v1/customer/28546', 'label': '401'},\n'endpoint': 'http://localhost:8080/application-1-tag-a-tag-b-whatever',\n'headers': {'Remote-Addr': '127.0.0.1',\n'Host': 'localhost:8080',\n'User-Agent': 'curl/8.1.2',\n'Accept': '*/*',\n'Content-Type': 'application/json',\n'Content-Length': '51'}}\n</code></pre> <p>We need to extract the <code>data</code> field from this data before training. This can be done by passing a lambda <code>lambda x: x['data']</code> to the fine tuning bolt.</p> <p>More info on other arguments can be found with:</p> <pre><code>genius Webhook rise --help\n</code></pre>"},{"location":"guides/local/#fine-tuning","title":"Fine-tuning","text":"<p>Now lets test the second leg of this, the model. Since we want to use the model for predicting the status code given the data, we will use classification as our task for fine-tuning the model.</p> <p>Lets use the <code>bert-base-uncased</code> model for now, as it is small enough to run on a CPU on a laptop. We also create a model on huggingface hub to store the model once it is trained: <code>ixaxaar/geniusrise-api-status-code-prediction</code>.</p> <pre><code>genius HuggingFaceClassificationFineTuner rise \\\nbatch \\\n--input_s3_bucket geniusrise-test \\\n--input_s3_folder train \\\nbatch \\\n--output_s3_bucket geniusrise-test \\\n--output_s3_folder api-prediction \\\nnone \\\nfine_tune \\\n--args \\\nmodel_name=\"bert-base-uncased\" \\\ntokenizer_name=\"bert-base-uncased\" \\\nnum_train_epochs=2 \\\nper_device_train_batch_size=64 \\\nmodel_class=BertForSequenceClassification \\\ntokenizer_class=BertTokenizer \\\ndata_masked=True \\\ndata_extractor_lambda=\"lambda x: x['data']\" \\\nhf_repo_id=ixaxaar/geniusrise-api-status-code-prediction \\\nhf_commit_message=\"initial local testing\" \\\nhf_create_pr=True \\\nhf_token=hf_lalala\n</code></pre> <pre><code>    \ud83d\ude80 Initialized Task with ID: HuggingFaceClassificationFineTuner772627a0-43a5-4f9d-9b0f-4362d69ba08c\n    Found credentials in shared credentials file: ~/.aws/credentials\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n    Loading dataset from /tmp/tmp3h3wav4h/train\n    New labels detected, ignore if fine-tuning\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 300/300 [00:00&lt;00:00, 4875.76 examples/s]\n{'train_runtime': 13.3748, 'train_samples_per_second': 44.861, 'train_steps_per_second': 22.43, 'train_loss': 0.6400579833984374, 'epoch': 2.0}\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 300/300 [00:13&lt;00:00, 22.43it/s]\npytorch_model.bin: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 438M/438M [01:29&lt;00:00, 4.88MB/s]\n    Successfully executed the bolt method: fine_tune \ud83d\udc4d\n</code></pre> <p>You'll see a progress bar at the bottom, on completion, a pull request will appear on huggingface hub. Here is the model we trained: https://huggingface.co/ixaxaar/geniusrise-api-status-code-prediction.</p>"},{"location":"guides/local/#packaging","title":"Packaging","text":"<p>Finally, lets package this workflow so that we can run it again and again.</p> <p>Create a <code>genius.yml</code> file, similar to the cli commands:</p> <pre><code>version: 1\nspouts:\nhttp_listener:\nname: Webhook\nmethod: listen\nargs:\nport: 8080\nstate:\ntype: none\noutput:\ntype: stream_to_batch\nargs:\nbucket: geniusrise-test\nfolder: train\nhttp_classifier:\nname: HuggingFaceClassificationFineTuner\nmethod: fine_tune\nargs:\nmodel_name: \"bert-base-uncased\"\ntokenizer_name: \"bert-base-uncased\"\nnum_train_epochs: 2\nper_device_train_batch_size: 2\nmodel_class: BertForSequenceClassification\ntokenizer_class: BertTokenizer\ndata_masked: True\ndata_extractor_lambda: \"lambda x: x['data']\"\nhf_repo_id: ixaxaar/geniusrise-api-status-code-prediction\nhf_commit_message: \"initial local testing\"\nhf_create_pr: True\nhf_token: hf_lalala\ninput:\ntype: spout\nargs:\nname: http_listener\noutput:\ntype: batch\nargs:\nbucket: geniusrise-test\nfolder: model\n</code></pre> <p>Finally run them:</p> <pre><code>genius rise\n</code></pre> <p>Or run them individually:</p> <pre><code>genius rise --spout all\ngenius rise --bolt all\n</code></pre> <p>Package this entire workspace into a docker container and upload to ECR:</p> <pre><code>genius docker package geniusrise ecr \\\n--auth '{\"aws_region\": \"ap-south-1\"}' \\\n--packages geniusrise-listeners geniusrise-huggingface\n</code></pre>"},{"location":"guides/local/#deployment","title":"Deployment","text":"<p>Delpoy the spout and bolt to kubernetes. We could use the command line to deploy:</p> <pre><code>genius Webhook deploy \\\nstream_to_batch \\\n--output_s3_bucket geniusrise-test \\\n--output_s3_folder train \\\nnone \\\nk8s \\\n--k8s_kind service \\\n--k8s_namespace geniusrise \\\n--k8s_cluster_name geniusrise-dev \\\n--k8s_context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev \\\n--k8s_name webhook \\\n--k8s_image \"genius-dev.dkr.ecr.ap-south-1.amazonaws.com/geniusrise\" \\\n--k8s_env_vars '{\"AWS_DEFAULT_REGION\": \"ap-south-1\", \"AWS_SECRET_ACCESS_KEY\": \"your-key\", \"AWS_ACCESS_KEY_ID\": \"your-secret\"}' \\\n--k8s_port 8080 \\\n--k8s_target_port 8080 \\\nlisten \\\n--args port=8080\n</code></pre> <p>Or we could simply use the yaml we created in the previous step:</p> <pre><code>genius rise up\n</code></pre> <p>See the status of the deployment:</p> <pre><code># Find the pod id\ngenius pod show \\\n--namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev 2&gt;&amp;1 | grep Running\n\ngenius pod describe \\\nwebhook-75c4bff67d-hbhts \\\n--namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n\ngenius deployment describe \\\nwebhook \\\n--namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n\ngenius service describe \\\nwebhook \\\n--namespace geniusrise \\\n--context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise-dev\n</code></pre>"},{"location":"guides/packaging/","title":"Packaging","text":""},{"location":"guides/packaging/#overview","title":"Overview","text":"<p>Geniusrise uses docker for packaging and delivering modules.</p>"},{"location":"guides/packaging/#capabilities","title":"Capabilities","text":"<ul> <li>Docker Image Creation: Create Docker images with custom base images, working directories, and local directories.</li> <li>Package Installation: Install both OS-level and Python packages during the Docker image creation.</li> <li>Environment Variables: Set environment variables in the Docker container.</li> <li>Multi-Repository Support: Upload Docker images to multiple types of container repositories.</li> <li>Authentication: Supports various authentication methods for different container repositories.</li> </ul>"},{"location":"guides/packaging/#command-line-interface","title":"Command-Line Interface","text":""},{"location":"guides/packaging/#syntax","title":"Syntax","text":"<pre><code>genius docker package &lt;image_name&gt; &lt;repository&gt; [options]\n</code></pre>"},{"location":"guides/packaging/#parameters","title":"Parameters","text":"<ul> <li><code>&lt;image_name&gt;</code>: The name of the Docker image to build and upload.</li> <li><code>&lt;repository&gt;</code>: The container repository to upload to (e.g., \"ECR\", \"DockerHub\", \"Quay\", \"ACR\", \"GCR\").</li> </ul>"},{"location":"guides/packaging/#options","title":"Options","text":"<ul> <li><code>--auth</code>: Authentication credentials as a JSON string. Default is an empty JSON object.</li> <li><code>--base_image</code>: The base image to use for the Docker container. Default is \"nvidia/cuda:12.2.0-runtime-ubuntu20.04\".</li> <li><code>--workdir</code>: The working directory in the Docker container. Default is \"/app\".</li> <li><code>--local_dir</code>: The local directory to copy into the Docker container. Default is \".\".</li> <li><code>--packages</code>: List of Python packages to install in the Docker container. Default is an empty list.</li> <li><code>--os_packages</code>: List of OS packages to install in the Docker container. Default is an empty list.</li> <li><code>--env_vars</code>: Environment variables to set in the Docker container. Default is an empty dictionary.</li> </ul>"},{"location":"guides/packaging/#authentication-details","title":"Authentication Details","text":"<ul> <li>ECR: <code>{\"aws_region\": \"ap-south-1\", \"aws_secret_access_key\": \"aws_key\", \"aws_access_key_id\": \"aws_secret\"}</code></li> <li>DockerHub: <code>{\"dockerhub_username\": \"username\", \"dockerhub_password\": \"password\"}</code></li> <li>ACR: <code>{\"acr_username\": \"username\", \"acr_password\": \"password\", \"acr_login_server\": \"login_server\"}</code></li> <li>GCR: <code>{\"gcr_key_file_path\": \"/path/to/keyfile.json\", \"gcr_repository\": \"repository\"}</code></li> <li>Quay: <code>{\"quay_username\": \"username\", \"quay_password\": \"password\"}</code></li> </ul>"},{"location":"guides/packaging/#examples","title":"Examples","text":""},{"location":"guides/packaging/#uploading-to-ecr-amazon-elastic-container-registry","title":"Uploading to ECR (Amazon Elastic Container Registry)","text":"<pre><code>genius docker package geniusrise ecr --auth '{\"aws_region\": \"ap-south-1\"}'\n</code></pre>"},{"location":"guides/packaging/#uploading-to-dockerhub","title":"Uploading to DockerHub","text":"<pre><code>genius docker package geniusrise dockerhub --auth '{\"dockerhub_username\": \"username\", \"dockerhub_password\": \"password\"}'\n</code></pre>"},{"location":"guides/packaging/#uploading-to-acr-azure-container-registry","title":"Uploading to ACR (Azure Container Registry)","text":"<pre><code>genius docker package geniusrise acr --auth '{\"acr_username\": \"username\", \"acr_password\": \"password\", \"acr_login_server\": \"login_server\"}'\n</code></pre>"},{"location":"guides/packaging/#uploading-to-gcr-google-container-registry","title":"Uploading to GCR (Google Container Registry)","text":"<pre><code>genius docker package geniusrise gcr --auth '{\"gcr_key_file_path\": \"/path/to/keyfile.json\", \"gcr_repository\": \"repository\"}'\n</code></pre>"},{"location":"guides/packaging/#uploading-to-quay","title":"Uploading to Quay","text":"<pre><code>genius docker package geniusrise quay --auth '{\"quay_username\": \"username\", \"quay_password\": \"password\"}'\n</code></pre>"},{"location":"guides/packaging/#uploading-with-custom-packages-and-os-packages","title":"Uploading with Custom Packages and OS Packages","text":"<pre><code>genius docker package geniusrise dockerhub \\\n--packages geniusrise-listeners geniusrise-databases geniusrise-huggingface geniusrise-openai \\\n--os_packages libmysqlclient-dev libldap2-dev libsasl2-dev libssl-dev\n</code></pre>"},{"location":"guides/packaging/#uploading-with-environment-variables","title":"Uploading with Environment Variables","text":"<pre><code>genius docker package geniusrise dockerhub --env_vars '{\"API_KEY\": \"123456\", \"ENV\": \"production\"}'\n</code></pre>"},{"location":"guides/packaging/#complex-examples","title":"Complex Examples","text":""},{"location":"guides/packaging/#1-uploading-to-ecr-with-custom-base-image-and-packages","title":"1. Uploading to ECR with Custom Base Image and Packages","text":"<p>This example demonstrates how to upload a Docker image to ECR with a custom base image and additional Python packages.</p> <pre><code>genius docker package my_custom_image ecr \\\n--auth '{\"aws_region\": \"us-west-2\", \"aws_secret_access_key\": \"aws_key\", \"aws_access_key_id\": \"aws_secret\"}' \\\n--base_image \"python:3.9-slim\" \\\n--packages \"numpy pandas scikit-learn\" \\\n--os_packages \"gcc g++\"\n</code></pre>"},{"location":"guides/packaging/#2-uploading-to-dockerhub-with-environment-variables-and-working-directory","title":"2. Uploading to DockerHub with Environment Variables and Working Directory","text":"<p>This example shows how to upload a Docker image to DockerHub with custom environment variables and a specific working directory.</p> <pre><code>genius docker package my_app dockerhub \\\n--auth '{\"dockerhub_username\": \"username\", \"dockerhub_password\": \"password\"}' \\\n--env_vars '{\"DEBUG\": \"True\", \"SECRET_KEY\": \"mysecret\"}' \\\n--workdir \"/my_app\"\n</code></pre>"},{"location":"guides/packaging/#3-uploading-to-acr-with-multiple-local-directories","title":"3. Uploading to ACR with Multiple Local Directories","text":"<p>In this example, we upload a Docker image to Azure Container Registry (ACR) and specify multiple local directories to be copied into the Docker container.</p> <pre><code># First, create a Dockerfile that copies multiple directories\n# Then use the following command\ngenius docker package multi_dir_app acr \\\n--auth '{\"acr_username\": \"username\", \"acr_password\": \"password\", \"acr_login_server\": \"login_server\"}' \\\n--local_dir \"./app ./config\"\n</code></pre>"},{"location":"guides/packaging/#4-uploading-to-gcr-with-custom-base-image-packages-and-os-packages","title":"4. Uploading to GCR with Custom Base Image, Packages, and OS Packages","text":"<p>This example demonstrates how to upload a Docker image to Google Container Registry (GCR) with a custom base image, Python packages, and OS packages.</p> <pre><code>genius docker package my_ml_model gcr \\\n--auth '{\"gcr_key_file_path\": \"/path/to/keyfile.json\", \"gcr_repository\": \"repository\"}' \\\n--base_image \"tensorflow/tensorflow:latest-gpu\" \\\n--packages \"scipy keras\" \\\n--os_packages \"libsm6 libxext6 libxrender-dev\"\n</code></pre>"},{"location":"guides/packaging/#5-uploading-to-quay-with-all-customizations","title":"5. Uploading to Quay with All Customizations","text":"<p>This example shows how to upload a Docker image to Quay with all available customizations like base image, working directory, local directory, Python packages, OS packages, and environment variables.</p> <pre><code>genius docker package full_custom quay \\\n--auth '{\"quay_username\": \"username\", \"quay_password\": \"password\"}' \\\n--base_image \"alpine:latest\" \\\n--workdir \"/custom_app\" \\\n--local_dir \"./src\" \\\n--packages \"flask gunicorn\" \\\n--os_packages \"bash curl\" \\\n--env_vars '{\"FLASK_ENV\": \"production\", \"PORT\": \"8000\"}'\n</code></pre>"},{"location":"guides/pin/","title":"Bulding an AI pin","text":"<p>Lets do an end to end project where we build an AI-pin to talk to a multi-modal language model.</p> <p>The system consists of two parts:</p> <ol> <li>Device: A low-power network device with camera, speaker and microphone</li> <li>Desktop: A central machine hosting the LLM, possibly a desktop computer running geniusrise</li> </ol> <pre>cb468722-53e0-4d6d-9a08-309874f2ff2c</pre> <p>We start with a ESP32 based platform as there are many these days. Lets look at two of them:</p>"},{"location":"guides/pin/#ttgo","title":"TTGO","text":"<p>The TTGO T-Camera Plus is a unique ESP32 module featuring a built-in camera and display. It's designed for applications that require direct image capture and display capabilities without the need for external screens or cameras.</p> <ul> <li>CPU: Dual-core Tensilica LX6 microprocessor up to 240 MHz</li> <li>Memory: 520 KB SRAM, 4 MB PSRAM</li> <li>Connectivity: Wi-Fi (802.11 b/g/n), Bluetooth (Classic and BLE)</li> <li>Camera: OV2640 camera module, 2 Megapixels</li> <li>Display: 1.3-inch OLED display</li> <li>Extras: Fish-eye lens, optional MPU6050 module for motion sensing</li> </ul> <p></p>"},{"location":"guides/pin/#seeed-studio-xiao","title":"Seeed Studio XIAO","text":"<p>Seeed Studio XIAO ESP32C3 is a mini but powerful module. It's part of the Seeed Studio XIAO series, known for its compact design and reliability in various IoT projects.</p> <ul> <li>CPU: RISC-V single-core processor, up to 160 MHz</li> <li>Memory: 400 KB SRAM, 4 MB Flash</li> <li>Connectivity: Wi-Fi (802.11 b/g/n), Bluetooth 5 (LE)</li> <li>I/O Pins: Rich set of peripherals including GPIOs, UART, SPI, I2C, and more.</li> <li>Size: Ultra-small form factor suitable for wearable devices and compact projects</li> </ul> <p></p>"},{"location":"guides/pin/#peripherals","title":"Peripherals","text":"<p>We used a bunch of these peripherals wherever the boards did not have them. We usually chose a platform with at least a screen and a camera included and added these peripherals to them.</p>"},{"location":"guides/pin/#microphone","title":"Microphone","text":"<ul> <li>Model: INMP441 I2S</li> <li>Features: High precision, omnidirectional, MEMS microphone module, Digital I2S interface</li> <li>Usage: Ideal for high-quality audio input and voice command projects</li> </ul> <p>product-page.</p> <p></p>"},{"location":"guides/pin/#speaker","title":"Speaker","text":"<ul> <li>Model: SeeedStudio Grove Speaker</li> <li>Features: Programmable, with built-in amplifier, capable of playing various tones and sounds</li> <li>Usage: Suitable for projects requiring audio output like alarms, voice notifications, and music playback</li> </ul> <p>product-page</p> <p></p>"},{"location":"guides/pin/#touchscreen","title":"Touchscreen","text":"<ul> <li>Model: SeeedStudio Round Display for XIAO</li> <li>Features: Touchscreen capability, round display, perfect for user interface projects</li> <li>Usage: Excellent for compact and wearable devices requiring user interaction</li> </ul> <p>product-page</p> <p></p>"},{"location":"guides/pin/#connections","title":"Connections","text":"<p>Now lets get connected. The following lists all connections, some soldering of headers may be required.</p>"},{"location":"guides/pin/#seeed-studio-xiao-connections","title":"Seeed Studio XIAO Connections","text":"<p>For the Seeed Studio XIAO, we'll connect a touchscreen display, an INMP441 I2S microphone, and a SeeedStudio Grove Speaker.</p>"},{"location":"guides/pin/#touchscreen-display","title":"Touchscreen Display","text":"<ul> <li>Display Model: Seeed Studio Round Display for XIAO</li> <li>Connection Type: SPI</li> <li>Required Pins:</li> <li>SCL (Serial Clock) to XIAO's SCL (GPIO18 for SPI clock)</li> <li>SDA (Serial Data) to XIAO's SDA (GPIO19 for SPI MOSI)</li> <li>RES (Reset) to any available GPIO pin (e.g., GPIO21) for display reset</li> <li>DC (Data/Command) to any available GPIO pin (e.g., GPIO22) for data/command selection</li> <li>CS (Chip Select) to any available GPIO pin (e.g., GPIO5) for SPI chip select</li> </ul> <p>Very easy to connect, xiao sits on the display.</p>"},{"location":"guides/pin/#microphone-inmp441-i2s","title":"Microphone (INMP441 I2S)","text":"<ul> <li>Connection Type: I2S</li> <li>Required Pins:</li> <li>WS (Word Select/LRCLK) to GPIO23</li> <li>SCK (Serial Clock) to GPIO18</li> <li>SD (Serial Data) to GPIO19</li> </ul>"},{"location":"guides/pin/#speaker-seeedstudio-grove","title":"Speaker (SeeedStudio Grove)","text":"<ul> <li>Connection Type: Digital I/O</li> <li>Required Pins:</li> <li>SIG to any PWM-capable GPIO pin (e.g., GPIO25) for audio signal</li> <li>GND to GND</li> </ul>"},{"location":"guides/pin/#ttgo-t-camera-plus-connections","title":"TTGO T-Camera Plus Connections","text":"<p>For the TTGO T-Camera Plus, we're connecting an INMP441 I2S microphone and a SeeedStudio Grove Speaker since it already includes a camera and display.</p>"},{"location":"guides/pin/#microphone-inmp441-i2s_1","title":"Microphone (INMP441 I2S)","text":"<ul> <li>Connection Type: I2S</li> <li>Required Pins:</li> <li>WS (Word Select/LRCLK) to GPIO32</li> <li>SCK (Serial Clock) to GPIO14</li> <li>SD (Serial Data) to GPIO27</li> </ul>"},{"location":"guides/pin/#speaker-seeedstudio-grove_1","title":"Speaker (SeeedStudio Grove)","text":"<ul> <li>Connection Type: Digital I/O</li> <li>Required Pins:</li> <li>SIG to any PWM-capable GPIO pin (e.g., GPIO33) for audio signal</li> <li>GND to GND</li> </ul>"},{"location":"guides/pin/#general-tips","title":"General Tips","text":"<ul> <li>Power Supply: Ensure that all devices are powered appropriately. The XIAO and TTGO can be powered via USB or an external 3.3V power supply.</li> <li>Common Ground: Make sure all components share a common ground connection.</li> <li>Programming: Use the Arduino IDE or ESP-IDF for programming the ESP32 devices. Libraries specific to the peripherals (e.g., display, I2S microphone, and speaker) will be required.</li> <li>I2S Library: For the INMP441 microphone, an I2S library suitable for ESP32 should be used to handle audio input.</li> <li>Display Library: For the touchscreen display, a library compatible with the specific model will be needed for interfacing and graphics rendering.</li> </ul>"},{"location":"guides/usage/","title":"Usage","text":"<p>The easiest way to use geniusrise is to host an API over a desired model. Use one of the examples from text, vision or audio.</p>"},{"location":"guides/usage/#run-on-local","title":"Run on Local","text":"<p>Say, we are interested in running an API over a vision / multi-modal model such as bakLlava from huggingface:</p>"},{"location":"guides/usage/#1-install-geniusrise-and-vision","title":"1. Install geniusrise and vision","text":"<pre><code>pip install torch\npip install geniusrise\npip install geniusrise-vision # vision multi-modal models\n# pip install geniusrise-text # text models, LLMs\n# pip install geniusrise-audio # audio models\n</code></pre>"},{"location":"guides/usage/#2-use-the-genius-cli-to-run-bakllava","title":"2. Use the genius cli to run bakLlava","text":"<pre><code>genius VisualQAAPI rise \\\nbatch \\\n--input_folder ./input \\\nbatch \\\n--output_folder ./output \\\nnone \\\nlisten \\\n--args \\\nmodel_name=\"llava-hf/bakLlava-v1-hf\" \\\nmodel_class=\"LlavaForConditionalGeneration\" \\\nprocessor_class=\"AutoProcessor\" \\\ndevice_map=\"cuda:0\" \\\nuse_cuda=True \\\nprecision=\"bfloat16\" \\\nquantization=0 \\\nmax_memory=None \\\ntorchscript=False \\\ncompile=False \\\nflash_attention=False \\\nbetter_transformers=False \\\nendpoint=\"*\" \\\nport=3000 \\\ncors_domain=\"http://localhost:3000\" \\\nusername=\"user\" \\\npassword=\"password\"\n</code></pre>"},{"location":"guides/usage/#3-test-the-api","title":"3. Test the API","text":"<pre><code>MY_IMAGE=/path/to/test/image\n\n(base64 -w 0 $MY_IMAGE | awk '{print \"{\\\"image_base64\\\": \\\"\"$0\"\\\", \\\"question\\\": \\\"&lt;image&gt;\\nUSER: Whats the content of the image?\\nASSISTANT:\\\", \\\"do_sample\\\": false, \\\"max_new_tokens\\\": 128}\"}' &gt; /tmp/image_payload.json)\ncurl -X POST http://localhost:3000/api/v1/answer_question \\\n-H \"Content-Type: application/json\" \\\n-u user:password \\\n-d @/tmp/image_payload.json | jq\n</code></pre>"},{"location":"guides/usage/#4-save-your-work","title":"4. Save your work","text":"<p>Save what you did to be replicated later as <code>genius.yml</code> file:</p> <pre><code>version: '1'\nbolts:\nmy_bolt:\nname: VisualQAAPI\nstate:\ntype: none\ninput:\ntype: batch\nargs:\ninput_folder: ./input\noutput:\ntype: batch\nargs:\noutput_folder: ./output\nmethod: listen\nargs:\nmodel_name: 'llava-hf/bakLlava-v1-hf'\nmodel_class: 'LlavaForConditionalGeneration'\nprocessor_class: 'AutoProcessor'\ndevice_map: 'cuda:0'\nuse_cuda: True\nprecision: 'bfloat16'\nquantization: 0\nmax_memory: None\ntorchscript: False\ncompile: False\nflash_attention: False\nbetter_transformers: False\nendpoint: '*'\nport: 3000\ncors_domain: 'http://localhost:3000'\nusername: 'user'\npassword: 'password'\n</code></pre> <p>To later re-run the same, simply navigate to the directory of this file and do:</p> <pre><code>genius rise\n</code></pre>"},{"location":"guides/usage/#advanced-usage","title":"Advanced Usage","text":"<p>For having a set of APIs, say for voice -&gt; text -&gt; text -&gt; voice pipeline, create a <code>genius.yml</code> file like this:</p> <pre><code>version: \"1\"\nbolts:\nspeech_to_text_bolt:\nname: SpeechToTextAPI\nstate:\ntype: none\ninput:\ntype: batch\nargs:\ninput_folder: ./input\noutput:\ntype: batch\nargs:\noutput_folder: ./output\nmethod: listen\nargs:\nmodel_name: openai/whisper-large-v3\nmodel_class: WhisperForConditionalGeneration\nprocessor_class: AutoProcessor\nuse_cuda: true\nprecision: float\nquantization: 0\ndevice_map: cuda:0\nmax_memory: null\ntorchscript: false\ncompile: false\nflash_attention: False\nbetter_transformers: False\nendpoint: \"0.0.0.0\"\nport: 3001\ncors_domain: http://localhost:3001\nusername: user\npassword: password\nchat_bolt:\nname: InstructionAPI\nstate:\ntype: none\ninput:\ntype: batch\nargs:\ninput_folder: ./input\noutput:\ntype: batch\nargs:\noutput_folder: ./output\nmethod: listen\nargs:\nmodel_name: TheBloke/Mistral-7B-Instruct-v0.1-GPTQ:gptq-4bit-32g-actorder_True\nmodel_class: AutoModelForCausalLM\ntokenizer_class: AutoTokenizer\nuse_cuda: true\nprecision: float16\nquantization: 0\ndevice_map: auto\nmax_memory: null\ntorchscript: false\ncompile: false\nflash_attention: False\nbetter_transformers: False\nawq_enabled: False\nendpoint: \"0.0.0.0\"\nport: 3002\ncors_domain: http://localhost:3002\nusername: user\npassword: password\ntext_to_speech_bolt:\nname: TextToSpeechAPI\nstate:\ntype: none\ninput:\ntype: batch\nargs:\ninput_folder: ./input\noutput:\ntype: batch\nargs:\noutput_folder: ./output\nmethod: listen\nargs:\nmodel_name: suno/bark\nmodel_class: BarkModel\nprocessor_class: BarkProcessor\nuse_cuda: true\nprecision: float32\nquantization: 0\ndevice_map: cuda:0\nmax_memory: null\ntorchscript: false\ncompile: false\nflash_attention: False\nbetter_transformers: False\nendpoint: \"0.0.0.0\"\nport: 3003\ncors_domain: http://localhost:3003\nusername: user\npassword: password\n</code></pre> <p>and run:</p> <pre><code>genius rise\n</code></pre> <p>(like docker-compose etc).</p> <p>then try it out:</p> <pre><code># Step 1: Transcribe audio file\nTRANSCRIPTION=$(echo $(base64 -w 0 sample.mp3) | awk '{print \"{\\\"audio_file\\\": \\\"\"$0\"\\\", \\\"model_sampling_rate\\\": 16000}\"}' | \\\ncurl -s -X POST http://localhost:3001/api/v1/transcribe \\\n-H \"Content-Type: application/json\" \\\n-u user:password \\\n-d @- | jq -r '.transcriptions.transcription')\necho \"Transcription: $TRANSCRIPTION\"\n# Step 2: Send a prompt to the text completion API\nPROMPT_JSON=$(jq -n --arg prompt \"$TRANSCRIPTION\" '{\"prompt\": $prompt, \"decoding_strategy\": \"generate\", \"max_new_tokens\": 100, \"do_sample\": true, \"pad_token_id\": 0}')\nCOMPLETION=$(echo $PROMPT_JSON | curl -s -X POST \"http://localhost:3002/api/v1/complete\" \\\n-H \"Content-Type: application/json\" \\\n-u \"user:password\" \\\n-d @- | jq -r '.completion')\necho \"Completion: $COMPLETION\"\n# Step 3: Synthesize speech from text and play the output\nSYNTH_JSON=$(jq -n --arg text \"$COMPLETION\" '{\"text\": $text, \"output_type\": \"mp3\", \"voice_preset\": \"v2/en_speaker_6\"}')\ncurl -s -X POST \"http://localhost:3003/api/v1/synthesize\" \\\n-H \"Content-Type: application/json\" \\\n-u \"user:password\" \\\n-d \"$SYNTH_JSON\" | jq -r '.audio_file' | base64 -d &gt; output.mp3\n\nvlc output.mp3 &amp;&gt;/dev/null\n</code></pre>"},{"location":"guides/usage/#run-on-remote","title":"Run on Remote","text":"<p>If we are running on a remote machine instead, perhaps we want to use our own model stored in S3?</p> <pre><code>genius VisualQAAPI rise \\\nbatch \\\n--input_s3_bucket my-s3-bucket \\\n--input_s3_folder model \\\nbatch \\\n--output_s3_bucket my-s3-bucket \\\n--output_s3_folder output-&lt;partition/keys&gt; \\\nnone \\\nlisten \\\n--args \\\nmodel_name=\"local\" \\\nmodel_class=\"LlavaForConditionalGeneration\" \\\nprocessor_class=\"AutoProcessor\" \\\ndevice_map=\"cuda:0\" \\\nuse_cuda=True \\\nprecision=\"bfloat16\" \\\nquantization=0 \\\nmax_memory=None \\\ntorchscript=False \\\ncompile=False \\\nflash_attention=False \\\nbetter_transformers=False \\\nendpoint=\"*\" \\\nport=3000 \\\ncors_domain=\"http://localhost:3000\" \\\nusername=\"user\" \\\npassword=\"password\"\n</code></pre> <p>or in YAML:</p> <pre><code>version: '1'\nbolts:\nmy_bolt:\nname: VisualQAAPI\nstate:\ntype: none\ninput:\ntype: batch\nargs:\nbucket: my-s3-bucket\nfolder: model\noutput:\ntype: batch\nargs:\nbucket: my-s3-bucket\nfolder: output-&lt;partition/keys&gt;\nmethod: listen\nargs:\nmodel_name: 'llava-hf/bakLlava-v1-hf'\nmodel_class: 'LlavaForConditionalGeneration'\nprocessor_class: 'AutoProcessor'\ndevice_map: 'cuda:0'\nuse_cuda: True\nprecision: 'bfloat16'\nquantization: 0\nmax_memory: None\ntorchscript: False\ncompile: False\nflash_attention: False\nbetter_transformers: False\nendpoint: '*'\nport: 3000\ncors_domain: 'http://localhost:3000'\nusername: 'user'\npassword: 'password'\n</code></pre>"},{"location":"guides/usage/#docker-packaging","title":"Docker packaging","text":"<p>Perhaps we also want to now use docker to package?</p> <p>Refer Packaging</p>"},{"location":"guides/usage/#to-production","title":"To Production","text":"<p>And finally deploy as a replicaset on a kubernetes cluster for going to prod!</p> <p>Refer Deployment</p>"},{"location":"guides/usage/#observability","title":"Observability","text":"<p>We have prometheus integrated, just integrate with your prometheus cluster! Prometheus runs on <code>PROMETHEUS_PORT</code> ENV variable or <code>8282</code> by default.</p>"},{"location":"guides/yaml/","title":"YAML Structure and Operations","text":"<p>The YAML file for Geniusrise is called <code>Geniusfile.yaml</code> and it has the following structure:</p> <pre><code>version: 1\nspouts:\n&lt;spout_name&gt;:\nname: &lt;spout_name&gt;\nmethod: &lt;method_name&gt;\nargs:\n&lt;key&gt;: &lt;value&gt;\noutput:\ntype: &lt;output_type&gt;\nargs:\n&lt;key&gt;: &lt;value&gt;\nstate:\ntype: &lt;state_type&gt;\nargs:\n&lt;key&gt;: &lt;value&gt;\ndeploy:\ntype: &lt;deploy_type&gt;\nargs:\n&lt;key&gt;: &lt;value&gt;\nbolts:\n&lt;bolt_name&gt;:\nname: &lt;bolt_name&gt;\nmethod: &lt;method_name&gt;\nargs:\n&lt;key&gt;: &lt;value&gt;\ninput:\ntype: &lt;input_type&gt;\nargs:\n&lt;key&gt;: &lt;value&gt;\noutput:\ntype: &lt;output_type&gt;\nargs:\n&lt;key&gt;: &lt;value&gt;\nstate:\ntype: &lt;state_type&gt;\nargs:\n&lt;key&gt;: &lt;value&gt;\ndeploy:\ntype: &lt;deploy_type&gt;\nargs:\n&lt;key&gt;: &lt;value&gt;\n</code></pre>"},{"location":"guides/yaml/#example-yaml-files","title":"Example YAML Files","text":""},{"location":"guides/yaml/#example-1-basic-spout-and-bolt","title":"Example 1: Basic Spout and Bolt","text":"<pre><code>version: 1\nspouts:\nTestSpout:\nname: TestSpout\nmethod: listen\nargs:\nport: 8080\noutput:\ntype: batch\nargs:\nbucket: geniusrise-test\nfolder: train\nstate:\ntype: none\ndeploy:\ntype: k8s\nargs:\nkind: job\nname: coretest\nnamespace: geniusrise\nimage: \"geniusrise/geniusrise-core\"\nbolts:\nTestBolt:\nname: TestBolt\nmethod: process\nargs:\nfactor: 2\ninput:\ntype: batch\nargs:\nbucket: geniusrise-test\nfolder: train\noutput:\ntype: batch\nargs:\nbucket: geniusrise-test\nfolder: output\nstate:\ntype: none\ndeploy:\ntype: k8s\nargs:\nkind: job\nname: coretest\nnamespace: geniusrise\nimage: \"geniusrise/geniusrise-core\"\n</code></pre>"},{"location":"guides/yaml/#example-2-spout-with-redis-state","title":"Example 2: Spout with Redis State","text":"<pre><code>version: 1\nspouts:\nRedisSpout:\nname: RedisSpout\nmethod: listen\nargs:\nport: 8080\noutput:\ntype: streaming\nargs:\noutput_topic: geniusrise-stream\nkafka_servers: \"localhost:9092\"\nstate:\ntype: redis\nargs:\nredis_host: \"localhost\"\nredis_port: 6379\nredis_db: 0\ndeploy:\ntype: k8s\nargs:\nkind: service\nname: redisspout\nnamespace: geniusrise\nimage: \"geniusrise/geniusrise-core\"\n</code></pre>"},{"location":"guides/yaml/#example-3-bolt-with-postgres-state-and-ecs-deployment","title":"Example 3: Bolt with Postgres State and ECS Deployment","text":"<pre><code>version: 1\nbolts:\nPostgresBolt:\nname: PostgresBolt\nmethod: process\nargs:\nfactor: 2\ninput:\ntype: streaming\nargs:\ninput_topic: geniusrise-stream\nkafka_servers: \"localhost:9092\"\noutput:\ntype: batch\nargs:\nbucket: geniusrise-test\nfolder: output\nstate:\ntype: postgres\nargs:\npostgres_host: \"localhost\"\npostgres_port: 5432\npostgres_user: \"postgres\"\npostgres_password: \"password\"\npostgres_database: \"geniusrise\"\npostgres_table: \"state_table\"\ndeploy:\ntype: ecs\nargs:\nname: postgresbolt\naccount_id: \"123456789012\"\ncluster: \"geniusrise-cluster\"\nsubnet_ids: [\"subnet-abc123\", \"subnet-def456\"]\nsecurity_group_ids: [\"sg-abc123\"]\nlog_group: \"geniusrise-logs\"\nimage: \"geniusrise/geniusrise-core\"\n</code></pre>"},{"location":"guides/yaml/#example-4-spout-with-s3-state-and-lambda-deployment","title":"Example 4: Spout with S3 State and Lambda Deployment","text":"<pre><code>version: 1\nspouts:\nS3Spout:\nname: S3Spout\nmethod: listen\nargs:\ns3_bucket: geniusrise-data\ns3_prefix: input/\noutput:\ntype: streaming\nargs:\noutput_topic: geniusrise-s3-stream\nkafka_servers: \"localhost:9092\"\nstate:\ntype: s3\nargs:\nstate_bucket: geniusrise-state\nstate_prefix: s3spout/\ndeploy:\ntype: lambda\nargs:\nfunction_name: S3SpoutFunction\nrole_arn: arn:aws:iam::123456789012:role/execution_role\nruntime: python3.8\nhandler: s3spout.handler\n</code></pre>"},{"location":"guides/yaml/#example-5-bolt-with-dynamodb-state-and-fargate-deployment","title":"Example 5: Bolt with DynamoDB State and Fargate Deployment","text":"<pre><code>version: 1\nbolts:\nDynamoBolt:\nname: DynamoBolt\nmethod: process\nargs:\noperation: multiply\nfactor: 3\ninput:\ntype: streaming\nargs:\ninput_topic: geniusrise-s3-stream\nkafka_servers: \"localhost:9092\"\noutput:\ntype: batch\nargs:\nbucket: geniusrise-output\nfolder: dynamo/\nstate:\ntype: dynamodb\nargs:\ntable_name: DynamoStateTable\nregion: us-east-1\ndeploy:\ntype: fargate\nargs:\ncluster: geniusrise-fargate\ntask_definition: DynamoBoltTask\nlaunch_type: FARGATE\nsubnets: [\"subnet-xyz789\", \"subnet-uvw456\"]\n</code></pre>"},{"location":"guides/yaml/#example-6-spout-and-bolt-with-azure-blob-storage-and-azure-functions","title":"Example 6: Spout and Bolt with Azure Blob Storage and Azure Functions","text":"<pre><code>version: 1\nspouts:\nAzureBlobSpout:\nname: AzureBlobSpout\nmethod: listen\nargs:\ncontainer_name: geniusrise-input\nstorage_account: geniusriseaccount\nstorage_key: \"your_storage_key_here\"\noutput:\ntype: streaming\nargs:\noutput_topic: geniusrise-azure-stream\nkafka_servers: \"localhost:9092\"\nstate:\ntype: azure_blob\nargs:\ncontainer_name: geniusrise-state\nstorage_account: geniusriseaccount\nstorage_key: \"your_storage_key_here\"\ndeploy:\ntype: azure_function\nargs:\nfunction_name: AzureBlobSpoutFunction\nresource_group: geniusrise-rg\nstorage_account: geniusriseaccount\nplan: Consumption\nbolts:\nAzureBlobBolt:\nname: AzureBlobBolt\nmethod: process\nargs:\noperation: add\nvalue: 5\ninput:\ntype: streaming\nargs:\ninput_topic: geniusrise-azure-stream\nkafka_servers: \"localhost:9092\"\noutput:\ntype: azure_blob\nargs:\ncontainer_name: geniusrise-output\nstorage_account: geniusriseaccount\nstorage_key: \"your_storage_key_here\"\nstate:\ntype: azure_blob\nargs:\ncontainer_name: geniusrise-state\nstorage_account: geniusriseaccount\nstorage_key: \"your_storage_key_here\"\ndeploy:\ntype: azure_function\nargs:\nfunction_name: AzureBlobBoltFunction\nresource_group: geniusrise-rg\nstorage_account: geniusriseaccount\nplan: Consumption\n</code></pre>"},{"location":"guides/yaml/#running-and-deploying-yaml-files","title":"Running and Deploying YAML Files","text":"<p>To run the YAML file:</p> <pre><code>genius rise\n</code></pre> <p>To deploy the YAML file:</p> <pre><code>genius rise up\n</code></pre>"},{"location":"guides/yaml/#managing-kubernetes-deployments","title":"Managing Kubernetes Deployments","text":"<p>You can manage Kubernetes deployments using the <code>genius</code> CLI. Here are some example commands:</p> <pre><code># Show pods in a namespace\ngenius pod show --namespace geniusrise --context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise\n\n# Scale a deployment\ngenius pod scale --namespace geniusrise --context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise --name testspout --replicas 3\n# Delete a deployment\ngenius pod delete --namespace geniusrise --context_name arn:aws:eks:us-east-1:genius-dev:cluster/geniusrise --name testspout\n</code></pre>"},{"location":"guides/yaml/#managing-ecs-deployments","title":"Managing ECS Deployments","text":"<p>You can manage ECS deployments using the <code>genius</code> CLI. Here are some example commands:</p> <pre><code># Show tasks in a cluster\ngenius ecs show --cluster geniusrise-cluster --account_id 123456789012\n# Scale a service\ngenius ecs scale --cluster geniusrise-cluster --account_id 123456789012 --name postgresbolt --desired_count 3\n# Delete a service\ngenius ecs delete --cluster geniusrise-cluster --account_id 123456789012 --name postgresbolt\n</code></pre>"},{"location":"listeners/activemq/","title":"ActiveMQ","text":"<p>Spout for ActiveMQ</p> <p>             Bases: <code>Spout</code></p>"},{"location":"listeners/activemq/#activemq.ActiveMQ.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the ActiveMQ class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>StreamingOutput</code> <p>An instance of the StreamingOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"listeners/activemq/#activemq.ActiveMQ.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius ActiveMQ rise \\\nstreaming \\\n--output_kafka_topic activemq_test \\\n--output_kafka_cluster_connection_string localhost:9094 \\\nnone \\\nlisten \\\n--args host=localhost port=61613 destination=my_queue\n</code></pre>"},{"location":"listeners/activemq/#activemq.ActiveMQ.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_activemq_spout:\nname: \"ActiveMQ\"\nmethod: \"listen\"\nargs:\nhost: \"localhost\"\nport: 61613\ndestination: \"my_queue\"\noutput:\ntype: \"streaming\"\nargs:\noutput_topic: \"activemq_test\"\nkafka_servers: \"localhost:9094\"\n</code></pre>"},{"location":"listeners/activemq/#activemq.ActiveMQ.listen","title":"<code>listen(host, port, destination, username=None, password=None)</code>","text":"<p>\ud83d\udcd6 Start listening for data from the ActiveMQ server.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The ActiveMQ server host.</p> required <code>port</code> <code>int</code> <p>The ActiveMQ server port.</p> required <code>destination</code> <code>str</code> <p>The ActiveMQ destination (queue or topic).</p> required <code>username</code> <code>Optional[str]</code> <p>The username for authentication. Defaults to None.</p> <code>None</code> <code>password</code> <code>Optional[str]</code> <p>The password for authentication. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the ActiveMQ server.</p>"},{"location":"listeners/amqp/","title":"ActiveMQ","text":"<p>Spout for AMQP</p> <p>             Bases: <code>Spout</code></p>"},{"location":"listeners/amqp/#amqp.RabbitMQ.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the RabbitMQ class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>StreamingOutput</code> <p>An instance of the StreamingOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"listeners/amqp/#amqp.RabbitMQ.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius RabbitMQ rise \\\nstreaming \\\n--output_kafka_topic rabbitmq_test \\\n--output_kafka_cluster_connection_string localhost:9094 \\\nnone \\\nlisten \\\n--args queue_name=my_queue host=localhost\n</code></pre>"},{"location":"listeners/amqp/#amqp.RabbitMQ.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_rabbitmq_spout:\nname: \"RabbitMQ\"\nmethod: \"listen\"\nargs:\nqueue_name: \"my_queue\"\nhost: \"localhost\"\noutput:\ntype: \"streaming\"\nargs:\noutput_topic: \"rabbitmq_test\"\nkafka_servers: \"localhost:9094\"\n</code></pre>"},{"location":"listeners/amqp/#amqp.RabbitMQ.listen","title":"<code>listen(queue_name, host='localhost', username=None, password=None)</code>","text":"<p>\ud83d\udcd6 Start listening for data from the RabbitMQ server.</p> <p>Parameters:</p> Name Type Description Default <code>queue_name</code> <code>str</code> <p>The RabbitMQ queue name to listen to.</p> required <code>host</code> <code>str</code> <p>The RabbitMQ server host. Defaults to \"localhost\".</p> <code>'localhost'</code> <code>username</code> <code>Optional[str]</code> <p>The username for authentication. Defaults to None.</p> <code>None</code> <code>password</code> <code>Optional[str]</code> <p>The password for authentication. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the RabbitMQ server.</p>"},{"location":"listeners/grpc/","title":"GRPC","text":"<p>Spout for gRPC</p> <p>             Bases: <code>Spout</code></p>"},{"location":"listeners/grpc/#grpc.Grpc.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the Grpc class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>StreamingOutput</code> <p>An instance of the StreamingOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"listeners/grpc/#grpc.Grpc.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius Grpc rise \\\nstreaming \\\n--output_kafka_topic grpc_test \\\n--output_kafka_cluster_connection_string localhost:9094 \\\nnone \\\nlisten \\\n--args server_address=localhost:50051 request_data=my_request syntax=proto3\n</code></pre>"},{"location":"listeners/grpc/#grpc.Grpc.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_grpc_spout:\nname: \"Grpc\"\nmethod: \"listen\"\nargs:\nserver_address: \"localhost:50051\"\nrequest_data: \"my_request\"\nsyntax: \"proto3\"\noutput:\ntype: \"streaming\"\nargs:\noutput_topic: \"grpc_test\"\nkafka_servers: \"localhost:9094\"\n</code></pre>"},{"location":"listeners/grpc/#grpc.Grpc.listen","title":"<code>listen(server_address, request_data, syntax, certificate=None, client_key=None, client_cert=None)</code>","text":"<p>\ud83d\udcd6 Start listening for data from the gRPC server.</p> <p>Parameters:</p> Name Type Description Default <code>server_address</code> <code>str</code> <p>The address of the gRPC server.</p> required <code>request_data</code> <code>str</code> <p>Data to send in the request.</p> required <code>syntax</code> <code>str</code> <p>The syntax to be used (e.g., \"proto3\").</p> required <code>certificate</code> <code>Optional[str]</code> <p>Optional server certificate for SSL/TLS.</p> <code>None</code> <code>client_key</code> <code>Optional[str]</code> <p>Optional client key for SSL/TLS.</p> <code>None</code> <code>client_cert</code> <code>Optional[str]</code> <p>Optional client certificate for SSL/TLS.</p> <code>None</code> <p>Raises:</p> Type Description <code>grpc.RpcError</code> <p>If there is an error while processing gRPC messages.</p>"},{"location":"listeners/http_polling/","title":"HTTP polling","text":"<p>Spout for HTTP polling</p> <p>             Bases: <code>Spout</code></p>"},{"location":"listeners/http_polling/#http_polling.RESTAPIPoll.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the RESTAPIPoll class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>StreamingOutput</code> <p>An instance of the StreamingOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"listeners/http_polling/#http_polling.RESTAPIPoll.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius RESTAPIPoll rise \\\nstreaming \\\n--output_kafka_topic restapi_test \\\n--output_kafka_cluster_connection_string localhost:9094 \\\nnone \\\nlisten \\\n--args url=https://api.example.com method=GET interval=60\n</code></pre>"},{"location":"listeners/http_polling/#http_polling.RESTAPIPoll.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_restapi_poll:\nname: \"RESTAPIPoll\"\nmethod: \"listen\"\nargs:\nurl: \"https://api.example.com\"\nmethod: \"GET\"\ninterval: 60\noutput:\ntype: \"streaming\"\nargs:\noutput_topic: \"restapi_test\"\nkafka_servers: \"localhost:9094\"\n</code></pre>"},{"location":"listeners/http_polling/#http_polling.RESTAPIPoll.listen","title":"<code>listen(url, method, interval=60, body=None, headers=None, params=None)</code>","text":"<p>Start polling the REST API for data.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The API endpoint.</p> required <code>method</code> <code>str</code> <p>The HTTP method (GET, POST, etc.).</p> required <code>interval</code> <code>int</code> <p>The polling interval in seconds. Defaults to 60.</p> <code>60</code> <code>body</code> <code>Optional[Dict]</code> <p>The request body. Defaults to None.</p> <code>None</code> <code>headers</code> <code>Optional[Dict[str, str]]</code> <p>The request headers. Defaults to None.</p> <code>None</code> <code>params</code> <code>Optional[Dict[str, str]]</code> <p>The request query parameters. Defaults to None.</p> <code>None</code>"},{"location":"listeners/http_polling/#http_polling.RESTAPIPoll.poll_api","title":"<code>poll_api(url, method, body=None, headers=None, params=None)</code>","text":"<p>\ud83d\udcd6 Start polling the REST API for data.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The API endpoint.</p> required <code>method</code> <code>str</code> <p>The HTTP method (GET, POST, etc.).</p> required <code>interval</code> <code>int</code> <p>The polling interval in seconds.</p> required <code>body</code> <code>Optional[Dict]</code> <p>The request body. Defaults to None.</p> <code>None</code> <code>headers</code> <code>Optional[Dict[str, str]]</code> <p>The request headers. Defaults to None.</p> <code>None</code> <code>params</code> <code>Optional[Dict[str, str]]</code> <p>The request query parameters. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the REST API server.</p>"},{"location":"listeners/kafka/","title":"Kafka","text":"<p>Spout for Kafka</p> <p>             Bases: <code>Spout</code></p>"},{"location":"listeners/kafka/#kafka.Kafka.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the Kafka class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>StreamingOutput</code> <p>An instance of the StreamingOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"listeners/kafka/#kafka.Kafka.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius Kafka rise \\\nstreaming \\\n--output_kafka_topic kafka_test \\\n--output_kafka_cluster_connection_string localhost:9094 \\\nnone \\\nlisten \\\n--args topic=my_topic group_id=my_group\n</code></pre>"},{"location":"listeners/kafka/#kafka.Kafka.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_kafka_spout:\nname: \"Kafka\"\nmethod: \"listen\"\nargs:\ntopic: \"my_topic\"\ngroup_id: \"my_group\"\noutput:\ntype: \"streaming\"\nargs:\noutput_topic: \"kafka_test\"\nkafka_servers: \"localhost:9094\"\n</code></pre>"},{"location":"listeners/kafka/#kafka.Kafka.listen","title":"<code>listen(topic, group_id, bootstrap_servers='localhost:9092', username=None, password=None)</code>","text":"<p>\ud83d\udcd6 Start listening for data from the Kafka topic.</p> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>str</code> <p>The Kafka topic to listen to.</p> required <code>group_id</code> <code>str</code> <p>The Kafka consumer group ID.</p> required <code>bootstrap_servers</code> <code>str</code> <p>The Kafka bootstrap servers. Defaults to \"localhost:9092\".</p> <code>'localhost:9092'</code> <code>username</code> <code>Optional[str]</code> <p>The username for SASL/PLAIN authentication. Defaults to None.</p> <code>None</code> <code>password</code> <code>Optional[str]</code> <p>The password for SASL/PLAIN authentication. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the Kafka server.</p>"},{"location":"listeners/kinesis/","title":"Kinesis","text":"<p>Spout for Kinesis</p> <p>             Bases: <code>Spout</code></p>"},{"location":"listeners/kinesis/#kinesis.Kinesis.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the Kinesis class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>StreamingOutput</code> <p>An instance of the StreamingOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"listeners/kinesis/#kinesis.Kinesis.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius Kinesis rise \\\nstreaming \\\n--output_kafka_topic kinesis_test \\\n--output_kafka_cluster_connection_string localhost:9094 \\\nnone \\\nlisten \\\n--args stream_name=my_stream shard_id=shardId-000000000000\n</code></pre>"},{"location":"listeners/kinesis/#kinesis.Kinesis.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_kinesis_spout:\nname: \"Kinesis\"\nmethod: \"listen\"\nargs:\nstream_name: \"my_stream\"\nshard_id: \"shardId-000000000000\"\noutput:\ntype: \"streaming\"\nargs:\noutput_topic: \"kinesis_test\"\nkafka_servers: \"localhost:9094\"\n</code></pre>"},{"location":"listeners/kinesis/#kinesis.Kinesis.listen","title":"<code>listen(stream_name, shard_id='shardId-000000000000', region_name=None, aws_access_key_id=None, aws_secret_access_key=None)</code>","text":"<p>\ud83d\udcd6 Start listening for data from the Kinesis stream.</p> <p>Parameters:</p> Name Type Description Default <code>stream_name</code> <code>str</code> <p>The name of the Kinesis stream.</p> required <code>shard_id</code> <code>str</code> <p>The shard ID to read from. Defaults to \"shardId-000000000000\".</p> <code>'shardId-000000000000'</code> <code>region_name</code> <code>str</code> <p>The AWS region name.</p> <code>None</code> <code>aws_access_key_id</code> <code>str</code> <p>AWS access key ID for authentication.</p> <code>None</code> <code>aws_secret_access_key</code> <code>str</code> <p>AWS secret access key for authentication.</p> <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If there is an error while processing Kinesis records.</p>"},{"location":"listeners/mqtt/","title":"MQTT","text":"<p>Spout for MQTT</p> <p>             Bases: <code>Spout</code></p>"},{"location":"listeners/mqtt/#mqtt.MQTT.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the MQTT class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>StreamingOutput</code> <p>An instance of the StreamingOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"listeners/mqtt/#mqtt.MQTT.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius MQTT rise \\\nstreaming \\\n--output_kafka_topic mqtt_test \\\n--output_kafka_cluster_connection_string localhost:9094 \\\nnone \\\nlisten \\\n--args host=localhost port=1883 topic=my_topic\n</code></pre>"},{"location":"listeners/mqtt/#mqtt.MQTT.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_mqtt_spout:\nname: \"MQTT\"\nmethod: \"listen\"\nargs:\nhost: \"localhost\"\nport: 1883\ntopic: \"my_topic\"\noutput:\ntype: \"streaming\"\nargs:\noutput_topic: \"mqtt_test\"\nkafka_servers: \"localhost:9094\"\n</code></pre>"},{"location":"listeners/mqtt/#mqtt.MQTT.listen","title":"<code>listen(host='localhost', port=1883, topic='#', username=None, password=None)</code>","text":"<p>Start listening for data from the MQTT broker.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The MQTT broker host. Defaults to \"localhost\".</p> <code>'localhost'</code> <code>port</code> <code>int</code> <p>The MQTT broker port. Defaults to 1883.</p> <code>1883</code> <code>topic</code> <code>str</code> <p>The MQTT topic to subscribe to. Defaults to \"#\".</p> <code>'#'</code> <code>username</code> <code>Optional[str]</code> <p>The username for authentication. Defaults to None.</p> <code>None</code> <code>password</code> <code>Optional[str]</code> <p>The password for authentication. Defaults to None.</p> <code>None</code>"},{"location":"listeners/quic/","title":"Quic","text":"<p>Spout for Quic</p> <p>             Bases: <code>Spout</code></p>"},{"location":"listeners/quic/#quic.Quic.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the Quic class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>StreamingOutput</code> <p>An instance of the StreamingOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"listeners/quic/#quic.Quic.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius Quic rise \\\nstreaming \\\n--output_kafka_topic quic_test \\\n--output_kafka_cluster_connection_string localhost:9094 \\\nnone \\\nlisten \\\n--args cert_path=/path/to/cert.pem key_path=/path/to/key.pem host=localhost port=4433\n</code></pre>"},{"location":"listeners/quic/#quic.Quic.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_quic_spout:\nname: \"Quic\"\nmethod: \"listen\"\nargs:\ncert_path: \"/path/to/cert.pem\"\nkey_path: \"/path/to/key.pem\"\nhost: \"localhost\"\nport: 4433\noutput:\ntype: \"streaming\"\nargs:\noutput_topic: \"quic_test\"\nkafka_servers: \"localhost:9094\"\n</code></pre>"},{"location":"listeners/quic/#quic.Quic.handle_stream_data","title":"<code>handle_stream_data(data, stream_id)</code>  <code>async</code>","text":"<p>Handle incoming stream data.</p> <p>:param data: The incoming data. :param stream_id: The ID of the stream.</p>"},{"location":"listeners/quic/#quic.Quic.listen","title":"<code>listen(cert_path, key_path, host='localhost', port=4433)</code>","text":"<p>\ud83d\udcd6 Start listening for data from the QUIC server.</p> <p>Parameters:</p> Name Type Description Default <code>cert_path</code> <code>str</code> <p>Path to the certificate file.</p> required <code>key_path</code> <code>str</code> <p>Path to the private key file.</p> required <code>host</code> <code>str</code> <p>Hostname to listen on. Defaults to \"localhost\".</p> <code>'localhost'</code> <code>port</code> <code>int</code> <p>Port to listen on. Defaults to 4433.</p> <code>4433</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to start the QUIC server.</p>"},{"location":"listeners/redis_pubsub/","title":"Redis pubsub","text":"<p>Spout for Redis pubsub</p> <p>             Bases: <code>Spout</code></p>"},{"location":"listeners/redis_pubsub/#redis_pubsub.RedisPubSub.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the RedisPubSub class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>StreamingOutput</code> <p>An instance of the StreamingOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"listeners/redis_pubsub/#redis_pubsub.RedisPubSub.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius RedisPubSub rise \\\nstreaming \\\n--output_kafka_topic redis_test \\\n--output_kafka_cluster_connection_string localhost:9094 \\\nnone \\\nlisten \\\n--args channel=my_channel host=localhost port=6379 db=0\n</code></pre>"},{"location":"listeners/redis_pubsub/#redis_pubsub.RedisPubSub.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_redis_spout:\nname: \"RedisPubSub\"\nmethod: \"listen\"\nargs:\nchannel: \"my_channel\"\nhost: \"localhost\"\nport: 6379\ndb: 0\noutput:\ntype: \"streaming\"\nargs:\noutput_topic: \"redis_test\"\nkafka_servers: \"localhost:9094\"\n</code></pre>"},{"location":"listeners/redis_pubsub/#redis_pubsub.RedisPubSub.listen","title":"<code>listen(channel, host='localhost', port=6379, db=0, password=None)</code>","text":"<p>\ud83d\udcd6 Start listening for data from the Redis Pub/Sub channel.</p> <p>Parameters:</p> Name Type Description Default <code>channel</code> <code>str</code> <p>The Redis Pub/Sub channel to listen to.</p> required <code>host</code> <code>str</code> <p>The Redis server host. Defaults to \"localhost\".</p> <code>'localhost'</code> <code>port</code> <code>int</code> <p>The Redis server port. Defaults to 6379.</p> <code>6379</code> <code>db</code> <code>int</code> <p>The Redis database index. Defaults to 0.</p> <code>0</code> <code>password</code> <code>Optional[str]</code> <p>The password for authentication. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the Redis server.</p>"},{"location":"listeners/redis_streams/","title":"Redis streams","text":"<p>Spout for Redis streams</p> <p>             Bases: <code>Spout</code></p>"},{"location":"listeners/redis_streams/#redis_streams.RedisStream.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the RedisStream class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>StreamingOutput</code> <p>An instance of the StreamingOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"listeners/redis_streams/#redis_streams.RedisStream.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius RedisStream rise \\\nstreaming \\\n--output_kafka_topic redis_stream_test \\\n--output_kafka_cluster_connection_string localhost:9094 \\\nnone \\\nlisten \\\n--args stream_key=my_stream host=localhost port=6379 db=0\n</code></pre>"},{"location":"listeners/redis_streams/#redis_streams.RedisStream.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_redis_stream:\nname: \"RedisStream\"\nmethod: \"listen\"\nargs:\nstream_key: \"my_stream\"\nhost: \"localhost\"\nport: 6379\ndb: 0\noutput:\ntype: \"streaming\"\nargs:\noutput_topic: \"redis_stream_test\"\nkafka_servers: \"localhost:9094\"\n</code></pre>"},{"location":"listeners/redis_streams/#redis_streams.RedisStream.listen","title":"<code>listen(stream_key, host='localhost', port=6379, db=0, password=None)</code>","text":"<p>\ud83d\udcd6 Start the asyncio event loop to listen for data from the Redis stream.</p> <p>Parameters:</p> Name Type Description Default <code>stream_key</code> <code>str</code> <p>The Redis stream key to listen to.</p> required <code>host</code> <code>str</code> <p>The Redis server host. Defaults to \"localhost\".</p> <code>'localhost'</code> <code>port</code> <code>int</code> <p>The Redis server port. Defaults to 6379.</p> <code>6379</code> <code>db</code> <code>int</code> <p>The Redis database index. Defaults to 0.</p> <code>0</code> <code>password</code> <code>Optional[str]</code> <p>The password for authentication. Defaults to None.</p> <code>None</code>"},{"location":"listeners/sns/","title":"SNS","text":"<p>Spout for SNS</p> <p>             Bases: <code>Spout</code></p>"},{"location":"listeners/sns/#sns.SNS.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the SNS class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>StreamingOutput</code> <p>An instance of the StreamingOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"listeners/sns/#sns.SNS.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius SNS rise \\\nstreaming \\\n--output_kafka_topic sns_test \\\n--output_kafka_cluster_connection_string localhost:9094 \\\nnone \\\nlisten\n</code></pre>"},{"location":"listeners/sns/#sns.SNS.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_sns_spout:\nname: \"SNS\"\nmethod: \"listen\"\noutput:\ntype: \"streaming\"\nargs:\noutput_topic: \"sns_test\"\nkafka_servers: \"localhost:9094\"\n</code></pre>"},{"location":"listeners/sns/#sns.SNS.listen","title":"<code>listen()</code>","text":"<p>\ud83d\udcd6 Start the asyncio event loop to listen for data from AWS SNS.</p>"},{"location":"listeners/socket.io/","title":"Socket.io","text":"<p>Spout for socket.io</p> <p>             Bases: <code>Spout</code></p>"},{"location":"listeners/socket.io/#socketio.SocketIo.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the SocketIo class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>StreamingOutput</code> <p>An instance of the StreamingOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"listeners/socket.io/#socketio.SocketIo.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius SocketIo rise \\\nstreaming \\\n--output_kafka_topic socketio_test \\\n--output_kafka_cluster_connection_string localhost:9094 \\\nnone \\\nlisten \\\n--args url=http://localhost:3000 namespace=/chat\n</code></pre>"},{"location":"listeners/socket.io/#socketio.SocketIo.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_socketio_spout:\nname: \"SocketIo\"\nmethod: \"listen\"\nargs:\nurl: \"http://localhost:3000\"\nnamespace: \"/chat\"\noutput:\ntype: \"streaming\"\nargs:\noutput_topic: \"socketio_test\"\nkafka_servers: \"localhost:9094\"\n</code></pre>"},{"location":"listeners/socket.io/#socketio.SocketIo.listen","title":"<code>listen(url, namespace=None, event='message', auth=None)</code>","text":"<p>\ud83d\udcd6 Start listening for data from the Socket.io server.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The Socket.io server URL.</p> required <code>namespace</code> <code>Optional[str]</code> <p>The Socket.io namespace. Defaults to None.</p> <code>None</code> <code>event</code> <code>str</code> <p>The Socket.io event to listen to. Defaults to \"message\".</p> <code>'message'</code> <code>auth</code> <code>Optional[dict]</code> <p>Authentication dictionary. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the Socket.io server.</p>"},{"location":"listeners/sqs/","title":"SQS","text":"<p>Spout for SQS</p> <p>             Bases: <code>Spout</code></p>"},{"location":"listeners/sqs/#sqs.SQS.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the SQS class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>StreamingOutput</code> <p>An instance of the StreamingOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"listeners/sqs/#sqs.SQS.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius SQS rise \\\nstreaming \\\n--output_kafka_topic sqs_test \\\n--output_kafka_cluster_connection_string localhost:9094 \\\nnone \\\nlisten \\\n--args queue_url=https://sqs.us-east-1.amazonaws.com/123456789012/my-queue batch_size=10 batch_interval=10\n</code></pre>"},{"location":"listeners/sqs/#sqs.SQS.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_sqs_spout:\nname: \"SQS\"\nmethod: \"listen\"\nargs:\nqueue_url: \"https://sqs.us-east-1.amazonaws.com/123456789012/my-queue\"\nbatch_size: 10\nbatch_interval: 10\noutput:\ntype: \"streaming\"\nargs:\noutput_topic: \"sqs_test\"\nkafka_servers: \"localhost:9094\"\n</code></pre>"},{"location":"listeners/sqs/#sqs.SQS.listen","title":"<code>listen(queue_url, batch_size=10, batch_interval=10)</code>","text":"<p>\ud83d\udcd6 Start listening for new messages in the SQS queue.</p> <p>Parameters:</p> Name Type Description Default <code>queue_url</code> <code>str</code> <p>The URL of the SQS queue to listen to.</p> required <code>batch_size</code> <code>int</code> <p>The maximum number of messages to receive in each batch. Defaults to 10.</p> <code>10</code> <code>batch_interval</code> <code>int</code> <p>The time in seconds to wait for a new message if the queue is empty. Defaults to 10.</p> <code>10</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the SQS service.</p>"},{"location":"listeners/udp/","title":"UDP","text":"<p>Spout for UDP</p> <p>             Bases: <code>Spout</code></p>"},{"location":"listeners/udp/#udp.Udp.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the Udp class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>StreamingOutput</code> <p>An instance of the StreamingOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"listeners/udp/#udp.Udp.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius Udp rise \\\nstreaming \\\n--output_kafka_topic udp_test \\\n--output_kafka_cluster_connection_string localhost:9094 \\\nnone \\\nlisten \\\n--args host=localhost port=12345\n</code></pre>"},{"location":"listeners/udp/#udp.Udp.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_udp_spout:\nname: \"Udp\"\nmethod: \"listen\"\nargs:\nhost: \"localhost\"\nport: 12345\noutput:\ntype: \"streaming\"\nargs:\noutput_topic: \"udp_test\"\nkafka_servers: \"localhost:9094\"\n</code></pre>"},{"location":"listeners/udp/#udp.Udp.listen","title":"<code>listen(host='localhost', port=12345)</code>","text":"<p>\ud83d\udcd6 Start listening for data from the UDP server.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The UDP server host. Defaults to \"localhost\".</p> <code>'localhost'</code> <code>port</code> <code>int</code> <p>The UDP server port. Defaults to 12345.</p> <code>12345</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the UDP server.</p>"},{"location":"listeners/webhook/","title":"Webhook","text":"<p>Spout for Webhook</p> <p>             Bases: <code>Spout</code></p>"},{"location":"listeners/webhook/#webhook.Webhook.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the Webhook class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>StreamingOutput</code> <p>An instance of the StreamingOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"listeners/webhook/#webhook.Webhook.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius Webhook rise \\\nstreaming \\\n--output_kafka_topic webhook_test \\\n--output_kafka_cluster_connection_string localhost:9094 \\\nnone \\\nlisten \\\n--args endpoint=* port=3000\n</code></pre>"},{"location":"listeners/webhook/#webhook.Webhook.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_webhook_spout:\nname: \"Webhook\"\nmethod: \"listen\"\nargs:\nendpoint: \"*\"\nport: 3000\noutput:\ntype: \"streaming\"\nargs:\noutput_topic: \"webhook_test\"\nkafka_servers: \"localhost:9094\"\n</code></pre>"},{"location":"listeners/webhook/#webhook.Webhook.listen","title":"<code>listen(endpoint='*', port=3000, username=None, password=None)</code>","text":"<p>\ud83d\udcd6 Start listening for data from the webhook.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>The webhook endpoint to listen to. Defaults to \"*\".</p> <code>'*'</code> <code>port</code> <code>int</code> <p>The port to listen on. Defaults to 3000.</p> <code>3000</code> <code>username</code> <code>Optional[str]</code> <p>The username for basic authentication. Defaults to None.</p> <code>None</code> <code>password</code> <code>Optional[str]</code> <p>The password for basic authentication. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to start the CherryPy server.</p>"},{"location":"listeners/websocket/","title":"Websocket","text":"<p>Spout for Websocket</p> <p>             Bases: <code>Spout</code></p>"},{"location":"listeners/websocket/#websocket.Websocket.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the Websocket class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>StreamingOutput</code> <p>An instance of the StreamingOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"listeners/websocket/#websocket.Websocket.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius Websocket rise \\\nstreaming \\\n--output_kafka_topic websocket_test \\\n--output_kafka_cluster_connection_string localhost:9094 \\\nnone \\\nlisten \\\n--args host=localhost port=8765\n</code></pre>"},{"location":"listeners/websocket/#websocket.Websocket.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_websocket_spout:\nname: \"Websocket\"\nmethod: \"listen\"\nargs:\nhost: \"localhost\"\nport: 8765\noutput:\ntype: \"streaming\"\nargs:\noutput_topic: \"websocket_test\"\nkafka_servers: \"localhost:9094\"\n</code></pre>"},{"location":"listeners/websocket/#websocket.Websocket.__listen","title":"<code>__listen(host, port)</code>  <code>async</code>","text":"<p>Start listening for data from the WebSocket server.</p>"},{"location":"listeners/websocket/#websocket.Websocket.listen","title":"<code>listen(host='localhost', port=8765)</code>","text":"<p>\ud83d\udcd6 Start the WebSocket server.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The WebSocket server host. Defaults to \"localhost\".</p> <code>'localhost'</code> <code>port</code> <code>int</code> <p>The WebSocket server port. Defaults to 8765.</p> <code>8765</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to start the WebSocket server.</p>"},{"location":"listeners/websocket/#websocket.Websocket.receive_message","title":"<code>receive_message(websocket, path)</code>  <code>async</code>","text":"<p>Receive a message from a WebSocket client and save it along with metadata.</p> <p>Parameters:</p> Name Type Description Default <code>websocket</code> <p>WebSocket client connection.</p> required <code>path</code> <p>WebSocket path.</p> required"},{"location":"listeners/zeromq/","title":"ZeroMQ","text":"<p>Spout for ZeroMQ</p> <p>             Bases: <code>Spout</code></p>"},{"location":"listeners/zeromq/#zeromq.ZeroMQ.__init__","title":"<code>__init__(output, state, **kwargs)</code>","text":"<p>Initialize the ZeroMQ class.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>StreamingOutput</code> <p>An instance of the StreamingOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"listeners/zeromq/#zeromq.ZeroMQ.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius ZeroMQ rise \\\nstreaming \\\n--output_kafka_topic zmq_test \\\n--output_kafka_cluster_connection_string localhost:9094 \\\nnone \\\nlisten \\\n--args endpoint=tcp://localhost:5555 topic=my_topic syntax=json\n</code></pre>"},{"location":"listeners/zeromq/#zeromq.ZeroMQ.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nmy_zmq_spout:\nname: \"ZeroMQ\"\nmethod: \"listen\"\nargs:\nendpoint: \"tcp://localhost:5555\"\ntopic: \"my_topic\"\nsyntax: \"json\"\noutput:\ntype: \"streaming\"\nargs:\noutput_topic: \"zmq_test\"\nkafka_servers: \"localhost:9094\"\n</code></pre>"},{"location":"listeners/zeromq/#zeromq.ZeroMQ.listen","title":"<code>listen(endpoint, topic, syntax, socket_type='SUB')</code>","text":"<p>\ud83d\udcd6 Start listening for data from the ZeroMQ server.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>The endpoint to connect to (e.g., \"tcp://localhost:5555\").</p> required <code>topic</code> <code>str</code> <p>The topic to subscribe to.</p> required <code>syntax</code> <code>str</code> <p>The syntax to be used (e.g., \"json\").</p> required <code>socket_type</code> <code>Optional[str]</code> <p>The type of ZeroMQ socket (default is \"SUB\").</p> <code>'SUB'</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If unable to connect to the ZeroMQ server or process messages.</p>"},{"location":"ocr/ConvertImage/","title":"Convert Images","text":"<p>             Bases: <code>Bolt</code></p>"},{"location":"ocr/ConvertImage/#geniusrise_ocr.readers.image.ConvertImage.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>The <code>ConvertImage</code> class is designed to convert images from one format to another. It takes an input folder containing images and an output format as arguments. The class iterates through each image file in the specified folder and converts it to the desired format. Additional options like quality and subsampling can be specified for lossy formats like 'JPG'.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>An instance of the BatchInput class for reading the data.</p> required <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"ocr/ConvertImage/#geniusrise_ocr.readers.image.ConvertImage.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius ConvertImage rise \\\nbatch \\\n--bucket my_bucket \\\n--s3_folder s3/input \\\nbatch \\\n--bucket my_bucket \\\n--s3_folder s3/output \\\nnone \\\nprocess \\\n--args input_folder=/path/to/image/folder output_format=PNG quality=95 subsampling=0\n</code></pre>"},{"location":"ocr/ConvertImage/#geniusrise_ocr.readers.image.ConvertImage.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nconvert_images:\nname: \"ConvertImage\"\nmethod: \"process\"\nargs:\noutput_format: \"PNG\"\nquality: 95\nsubsampling: 0\ninput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/input\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/output\"\n</code></pre>"},{"location":"ocr/ConvertImage/#geniusrise_ocr.readers.image.ConvertImage.process","title":"<code>process(output_format, quality=None, subsampling=0)</code>","text":"<p>\ud83d\udcd6 Convert images in the given input folder to the specified output format.</p> <p>Parameters:</p> Name Type Description Default <code>output_format</code> <code>str</code> <p>The format to convert images to ('PNG' or 'JPG').</p> required <code>quality</code> <code>Optional[int]</code> <p>The quality of the output image for lossy formats like 'JPG'. Defaults to None.</p> <code>None</code> <code>subsampling</code> <code>Optional[int]</code> <p>The subsampling factor for JPEG compression. Defaults to 0.</p> <code>0</code> <p>This method iterates through each image file in the specified folder, reads the image, and converts it to the specified output format. Additional parameters like quality and subsampling can be set for lossy formats.</p>"},{"location":"ocr/FineTunePix2Struct/","title":"Fine-tune pix2struct","text":"<p>             Bases: <code>Bolt</code></p>"},{"location":"ocr/FineTunePix2Struct/#geniusrise_ocr.ocr.pix2struct.fine_tune.FineTunePix2Struct.__init__","title":"<code>__init__(input, output, state, model_name='google/pix2struct-large', **kwargs)</code>","text":"<p>The <code>FineTunePix2Struct</code> class is designed to fine-tune the Pix2Struct model on a custom OCR dataset. It supports three popular OCR dataset formats: COCO, ICDAR, and SynthText.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>An instance of the BatchInput class for reading the data.</p> required <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>model_name</code> <code>str</code> <p>The name of the Pix2Struct model to use. Default is \"google/pix2struct-large\".</p> <code>'google/pix2struct-large'</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Dataset Formats <ul> <li>COCO: Assumes a folder structure with an 'annotations.json' file containing image and text annotations.</li> <li>ICDAR: Assumes a folder structure with 'Images' and 'Annotations' folders containing image files and XML annotation files respectively.</li> <li>SynthText: Assumes a folder with image files and corresponding '.txt' files containing ground truth text.</li> </ul>"},{"location":"ocr/FineTunePix2Struct/#geniusrise_ocr.ocr.pix2struct.fine_tune.FineTunePix2Struct.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius FineTunePix2Struct rise \\\nbatch \\\n--bucket my_bucket \\\n--s3_folder s3/input \\\nbatch \\\n--bucket my_bucket \\\n--s3_folder s3/output \\\nnone \\\nprocess \\\n--args epochs=3 batch_size=32 learning_rate=0.001 dataset_format=coco use_cuda=true\n</code></pre>"},{"location":"ocr/FineTunePix2Struct/#geniusrise_ocr.ocr.pix2struct.fine_tune.FineTunePix2Struct.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nfine_tune_pix2struct:\nname: \"FineTunePix2Struct\"\nmethod: \"process\"\nargs:\nepochs: 3\nbatch_size: 32\nlearning_rate: 0.001\ndataset_format: coco\nuse_cuda: true\ninput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/input\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/output\"\n</code></pre>"},{"location":"ocr/FineTunePix2Struct/#geniusrise_ocr.ocr.pix2struct.fine_tune.FineTunePix2Struct.process","title":"<code>process(epochs, batch_size, learning_rate, dataset_format, use_cuda=False)</code>","text":"<p>\ud83d\udcd6 Fine-tune the Pix2Struct model on a custom OCR dataset.</p> <p>Parameters:</p> Name Type Description Default <code>epochs</code> <code>int</code> <p>Number of training epochs.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for training.</p> required <code>learning_rate</code> <code>float</code> <p>Learning rate for the optimizer.</p> required <code>dataset_format</code> <code>str</code> <p>Format of the OCR dataset. Supported formats are \"coco\", \"icdar\", and \"synthtext\".</p> required <code>use_cuda</code> <code>bool</code> <p>Whether to use CUDA for training. Default is False.</p> <code>False</code> <p>This method fine-tunes the Pix2Struct model using the images and annotations in the dataset specified by <code>dataset_format</code>. The fine-tuned model is saved to the specified output path.</p>"},{"location":"ocr/FineTuneTROCR/","title":"OCR API using trocr","text":"<p>             Bases: <code>Bolt</code></p>"},{"location":"ocr/FineTuneTROCR/#geniusrise_ocr.ocr.trocr.fine_tune.FineTuneTROCR.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>The <code>FineTuneTROCR</code> class is designed to fine-tune the TROCR model on a custom OCR dataset. It supports three popular OCR dataset formats: COCO, ICDAR, and SynthText.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>An instance of the BatchInput class for reading the data.</p> required <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Dataset Formats <ul> <li>COCO: Assumes a folder structure with an 'annotations.json' file containing image and text annotations.</li> <li>ICDAR: Assumes a folder structure with 'Images' and 'Annotations' folders containing image files and XML annotation files respectively.</li> <li>SynthText: Assumes a folder with image files and corresponding '.txt' files containing ground truth text.</li> </ul>"},{"location":"ocr/FineTuneTROCR/#geniusrise_ocr.ocr.trocr.fine_tune.FineTuneTROCR.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius FineTuneTROCR rise \\\nbatch \\\n--bucket my_bucket \\\n--s3_folder s3/input \\\nbatch \\\n--bucket my_bucket \\\n--s3_folder s3/output \\\nnone \\\nprocess \\\n--args epochs=3 batch_size=32 learning_rate=0.001 dataset_format=coco use_cuda=true\n</code></pre>"},{"location":"ocr/FineTuneTROCR/#geniusrise_ocr.ocr.trocr.fine_tune.FineTuneTROCR.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nfine_tune_trocr:\nname: \"FineTuneTROCR\"\nmethod: \"process\"\nargs:\nepochs: 3\nbatch_size: 32\nlearning_rate: 0.001\ndataset_format: coco\nuse_cuda: true\ninput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/input\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/output\"\n</code></pre>"},{"location":"ocr/FineTuneTROCR/#geniusrise_ocr.ocr.trocr.fine_tune.FineTuneTROCR.process","title":"<code>process(epochs, batch_size, learning_rate, dataset_format, use_cuda=False)</code>","text":"<p>\ud83d\udcd6 Fine-tune the TROCR model on a custom OCR dataset.</p> <p>Parameters:</p> Name Type Description Default <code>epochs</code> <code>int</code> <p>Number of training epochs.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for training.</p> required <code>learning_rate</code> <code>float</code> <p>Learning rate for the optimizer.</p> required <code>dataset_format</code> <code>str</code> <p>Format of the OCR dataset. Supported formats are \"coco\", \"icdar\", and \"synthtext\".</p> required <code>use_cuda</code> <code>bool</code> <p>Whether to use CUDA for training. Default is False.</p> <code>False</code> <p>This method fine-tunes the TROCR model using the images and annotations in the dataset specified by <code>dataset_format</code>. The fine-tuned model is saved to the specified output path.</p>"},{"location":"ocr/ImageClassPredictor/","title":"Predict image classes","text":"<p>             Bases: <code>Bolt</code></p>"},{"location":"ocr/ImageClassPredictor/#geniusrise_ocr.classification.predict.ImageClassPredictor.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>The <code>ImageClassPredictor</code> class classifies images using a pre-trained PyTorch model. It assumes that the <code>input.input_folder</code> contains sub-folders of images to be classified. The classified images are saved in <code>output.output_folder</code>, organized by their predicted labels.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>Instance of BatchInput for reading data.</p> required <code>output</code> <code>BatchOutput</code> <p>Instance of BatchOutput for saving data.</p> required <code>state</code> <code>State</code> <p>Instance of State for maintaining state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"ocr/ImageClassPredictor/#geniusrise_ocr.classification.predict.ImageClassPredictor.__init__--command-line-invocation-with-geniusrise","title":"Command Line Invocation with geniusrise","text":"<pre><code>genius ImageClassPredictor rise \\\nbatch \\\n--bucket my_bucket \\\n--s3_folder s3/input \\\nbatch \\\n--bucket my_bucket \\\n--s3_folder s3/output \\\nnone \\\npredict \\\n--args classes='{\"0\": \"cat\", \"1\": \"dog\"}' model_path=/path/to/model.pth\n</code></pre>"},{"location":"ocr/ImageClassPredictor/#geniusrise_ocr.classification.predict.ImageClassPredictor.__init__--yaml-configuration-with-geniusrise","title":"YAML Configuration with geniusrise","text":"<pre><code>version: \"1\"\nspouts:\nimage_classification:\nname: \"ImageClassPredictor\"\nmethod: \"predict\"\nargs:\nclasses: '{\"0\": \"cat\", \"1\": \"dog\"}'\nmodel_path: \"/path/to/model.pth\"\ninput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/input\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/output\"\n</code></pre>"},{"location":"ocr/ImageClassPredictor/#geniusrise_ocr.classification.predict.ImageClassPredictor.get_label","title":"<code>get_label(class_idx)</code>","text":"<p>\ud83d\udcd6 Get the label corresponding to the class index.</p> <p>Parameters:</p> Name Type Description Default <code>class_idx</code> <code>int</code> <p>The class index.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The label corresponding to the class index.</p> <p>This method returns the label that corresponds to a given class index based on the <code>classes</code> dictionary.</p>"},{"location":"ocr/ImageClassPredictor/#geniusrise_ocr.classification.predict.ImageClassPredictor.predict","title":"<code>predict(classes, model_path, use_cuda=False)</code>","text":"<p>\ud83d\udcd6 Classify images in the input sub-folders using a pre-trained PyTorch model.</p> <p>Parameters:</p> Name Type Description Default <code>classes</code> <code>str</code> <p>JSON string mapping class indices to labels.</p> required <code>model_path</code> <code>str</code> <p>Path to the pre-trained PyTorch model.</p> required <code>use_cuda</code> <code>bool</code> <p>Whether to use CUDA for model inference. Default is False.</p> <code>False</code> <p>This method iterates through each image file in the specified sub-folders, applies the model, and classifies the image. The classified images are then saved in an output folder, organized by their predicted labels.</p>"},{"location":"ocr/ParseCBZCBR/","title":"Parse CBZCBR files","text":"<p>             Bases: <code>Bolt</code></p>"},{"location":"ocr/ParseCBZCBR/#geniusrise_ocr.readers.cbz_cbr.ParseCBZCBR.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>The <code>ParseCBZCBR</code> class is designed to process CBZ and CBR files, which are commonly used for comic books. It takes an input folder containing CBZ/CBR files as an argument and iterates through each file. For each file, it extracts the images and saves them in a designated output folder.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>An instance of the BatchInput class for reading the data.</p> required <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"ocr/ParseCBZCBR/#geniusrise_ocr.readers.cbz_cbr.ParseCBZCBR.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius ParseCBZCBR rise \\\nbatch \\\n--bucket my_bucket \\\n--s3_folder s3/input \\\nbatch \\\n--bucket my_bucket \\\n--s3_folder s3/output \\\nnone \\\nprocess\n</code></pre>"},{"location":"ocr/ParseCBZCBR/#geniusrise_ocr.readers.cbz_cbr.ParseCBZCBR.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nparse_cbzcbr:\nname: \"ParseCBZCBR\"\nmethod: \"process\"\ninput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/input\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/output\"\n</code></pre>"},{"location":"ocr/ParseCBZCBR/#geniusrise_ocr.readers.cbz_cbr.ParseCBZCBR.process","title":"<code>process(input_folder=None)</code>","text":"<p>\ud83d\udcd6 Process CBZ and CBR files in the given input folder and extract images.</p> <p>Parameters:</p> Name Type Description Default <code>input_folder</code> <code>str</code> <p>The folder containing CBZ/CBR files to process.</p> <code>None</code> <p>This method iterates through each CBZ/CBR file in the specified folder and extracts the images.</p>"},{"location":"ocr/ParseDjvu/","title":"Parse Djvu files","text":"<p>             Bases: <code>Bolt</code></p>"},{"location":"ocr/ParseDjvu/#geniusrise_ocr.readers.djvu.ParseDjvu.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>The <code>ParseDjvu</code> class is designed to process DJVU files and classify them as either text-based or image-based. It takes an input folder containing DJVU files as an argument and iterates through each file. For each DJVU, it samples a few pages to determine the type of content it primarily contains. If the DJVU is text-based, the class extracts the text from each page and saves it as a JSON file. If the DJVU is image-based, it converts each page to a PNG image and saves them in a designated output folder.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>An instance of the BatchInput class for reading the data.</p> required <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"ocr/ParseDjvu/#geniusrise_ocr.readers.djvu.ParseDjvu.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius ParseDjvu rise \\\nbatch \\\n--bucket my_bucket \\\n--s3_folder s3/input \\\nbatch \\\n--bucket my_bucket \\\n--s3_folder s3/output \\\nnone \\\nprocess\n</code></pre>"},{"location":"ocr/ParseDjvu/#geniusrise_ocr.readers.djvu.ParseDjvu.process","title":"<code>process(input_folder=None)</code>","text":"<p>\ud83d\udcd6 Process DJVU files in the given input folder and classify them as text-based or image-based.</p> <p>Parameters:</p> Name Type Description Default <code>input_folder</code> <code>str</code> <p>The folder containing DJVU files to process.</p> <code>None</code> <p>This method iterates through each DJVU file in the specified folder, reads a sample of pages, and determines whether the DJVU is text-based or image-based. It then delegates further processing to <code>_process_text_djvu</code> or <code>_process_image_djvu</code> based on this determination.</p>"},{"location":"ocr/ParseEpub/","title":"Parse Epub files","text":"<p>             Bases: <code>Bolt</code></p>"},{"location":"ocr/ParseEpub/#geniusrise_ocr.readers.epub.ParseEpub.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>The <code>ParseEpub</code> class is designed to process EPUB files and classify them as either text-based or image-based. It takes an input folder containing EPUB files as an argument and iterates through each file. For each EPUB, it samples a few items to determine the type of content it primarily contains. If the EPUB is text-based, the class extracts the text from each item and saves it as a JSON file. If the EPUB is image-based, it saves the images in a designated output folder.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>An instance of the BatchInput class for reading the data.</p> required <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"ocr/ParseEpub/#geniusrise_ocr.readers.epub.ParseEpub.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius ParseEpub rise \\\nbatch \\\n--bucket my_bucket \\\n--s3_folder s3/input \\\nbatch \\\n--bucket my_bucket \\\n--s3_folder s3/output \\\nnone \\\nprocess\n</code></pre>"},{"location":"ocr/ParseEpub/#geniusrise_ocr.readers.epub.ParseEpub.process","title":"<code>process(input_folder=None)</code>","text":"<p>\ud83d\udcd6 Process EPUB files in the given input folder and classify them as text-based or image-based.</p> <p>Parameters:</p> Name Type Description Default <code>input_folder</code> <code>str</code> <p>The folder containing EPUB files to process.</p> <code>None</code> <p>This method iterates through each EPUB file in the specified folder, reads a sample of items, and determines whether the EPUB is text-based or image-based. It then delegates further processing to <code>_process_text_epub</code> or <code>_process_image_epub</code> based on this determination.</p>"},{"location":"ocr/ParseMOBI/","title":"Parse MOBI files","text":"<p>             Bases: <code>Bolt</code></p>"},{"location":"ocr/ParseMOBI/#geniusrise_ocr.readers.mobi.ParseMOBI.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>The <code>ParseMOBI</code> class is designed to process MOBI files. It takes an input folder containing MOBI files as an argument and iterates through each file. For each file, it extracts the images and saves them in a designated output folder.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>An instance of the BatchInput class for reading the data.</p> required <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"ocr/ParseMOBI/#geniusrise_ocr.readers.mobi.ParseMOBI.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius ParseMOBI rise \\\nbatch \\\n--bucket my_bucket \\\n--s3_folder s3/input \\\nbatch \\\n--bucket my_bucket \\\n--s3_folder s3/output \\\nnone \\\nprocess\n</code></pre>"},{"location":"ocr/ParseMOBI/#geniusrise_ocr.readers.mobi.ParseMOBI.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nparse_mobi:\nname: \"ParseMOBI\"\nmethod: \"process\"\ninput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/input\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/output\"\n</code></pre>"},{"location":"ocr/ParseMOBI/#geniusrise_ocr.readers.mobi.ParseMOBI.process","title":"<code>process(input_folder=None)</code>","text":"<p>\ud83d\udcd6 Process MOBI files in the given input folder and extract images.</p> <p>Parameters:</p> Name Type Description Default <code>input_folder</code> <code>str</code> <p>The folder containing MOBI files to process.</p> <code>None</code> <p>This method iterates through each MOBI file in the specified folder and extracts the images.</p>"},{"location":"ocr/ParsePdf/","title":"Parse PDF files","text":"<p>             Bases: <code>Bolt</code></p>"},{"location":"ocr/ParsePdf/#geniusrise_ocr.readers.pdf.ParsePdf.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>The <code>ParsePdf</code> class is designed to process PDF files and classify them as either text-based or image-based. It takes an input folder containing PDF files as an argument and iterates through each file. For each PDF, it samples a few pages to determine the type of content it primarily contains. If the PDF is text-based, the class extracts the text from each page and saves it as a JSON file. If the PDF is image-based, it converts each page to a PNG image and saves them in a designated output folder.</p> <pre><code>Args:\n    input (BatchInput): An instance of the BatchInput class for reading the data.\n    output (BatchOutput): An instance of the BatchOutput class for saving the data.\n    state (State): An instance of the State class for maintaining the state.\n    **kwargs: Additional keyword arguments.\n</code></pre>"},{"location":"ocr/ParsePdf/#geniusrise_ocr.readers.pdf.ParsePdf.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius ParsePdf rise \\\nbatch \\\n--bucket my_bucket \\\n--s3_folder s3/input \\\nbatch \\\n--bucket my_bucket \\\n--s3_folder s3/output \\\nnone \\\nprocess\n</code></pre>"},{"location":"ocr/ParsePdf/#geniusrise_ocr.readers.pdf.ParsePdf.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nparse_pdfs:\nname: \"ParsePdf\"\nmethod: \"process\"\ninput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/input\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/outupt\"\n</code></pre>"},{"location":"ocr/ParsePdf/#geniusrise_ocr.readers.pdf.ParsePdf.process","title":"<code>process(input_folder=None)</code>","text":"<p>\ud83d\udcd6 Process PDF files in the given input folder and classify them as text-based or image-based.</p> <p>Parameters:</p> Name Type Description Default <code>input_folder</code> <code>str</code> <p>The folder containing PDF files to process.</p> <code>None</code> <p>This method iterates through each PDF file in the specified folder, reads a sample of pages, and determines whether the PDF is text-based or image-based. It then delegates further processing to <code>_process_text_pdf</code> or <code>_process_image_pdf</code> based on this determination.</p>"},{"location":"ocr/ParsePostScript/","title":"Parse PostScript files","text":"<p>             Bases: <code>Bolt</code></p>"},{"location":"ocr/ParsePostScript/#geniusrise_ocr.readers.postscript.ParsePostScript.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>The <code>ParsePostScript</code> class is designed to process PostScript files and classify them as either text-based or image-based. It takes an input folder containing PostScript files as an argument and iterates through each file. For each PostScript file, it converts it to PDF and samples a few pages to determine the type of content it primarily contains. If the PostScript is text-based, the class extracts the text from each page and saves it as a JSON file. If the PostScript is image-based, it converts each page to a PNG image and saves them in a designated output folder.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>An instance of the BatchInput class for reading the data.</p> required <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"ocr/ParsePostScript/#geniusrise_ocr.readers.postscript.ParsePostScript.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius ParsePostScript rise \\\nbatch \\\n--bucket my_bucket \\\n--s3_folder s3/input \\\nbatch \\\n--bucket my_bucket \\\n--s3_folder s3/output \\\nnone \\\nprocess\n</code></pre>"},{"location":"ocr/ParsePostScript/#geniusrise_ocr.readers.postscript.ParsePostScript.process","title":"<code>process(input_folder=None)</code>","text":"<p>\ud83d\udcd6 Process PostScript files in the given input folder and classify them as text-based or image-based.</p> <p>Parameters:</p> Name Type Description Default <code>input_folder</code> <code>str</code> <p>The folder containing PostScript files to process.</p> <code>None</code> <p>This method iterates through each PostScript file in the specified folder, converts it to PDF, reads a sample of pages, and determines whether the PostScript is text-based or image-based. It then delegates further processing to <code>_process_text_ps</code> or <code>_process_image_ps</code> based on this determination.</p>"},{"location":"ocr/ParseXPS/","title":"Parse XPS files","text":"<p>             Bases: <code>Bolt</code></p>"},{"location":"ocr/ParseXPS/#geniusrise_ocr.readers.xps.ParseXPS.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>The <code>ParseXPS</code> class is designed to process XPS files. It takes an input folder containing XPS files as an argument and iterates through each file. For each file, it extracts the images and saves them in a designated output folder.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>An instance of the BatchInput class for reading the data.</p> required <code>output</code> <code>BatchOutput</code> <p>An instance of the BatchOutput class for saving the data.</p> required <code>state</code> <code>State</code> <p>An instance of the State class for maintaining the state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"ocr/ParseXPS/#geniusrise_ocr.readers.xps.ParseXPS.__init__--using-geniusrise-to-invoke-via-command-line","title":"Using geniusrise to invoke via command line","text":"<pre><code>genius ParseXPS rise \\\nbatch \\\n--bucket my_bucket \\\n--s3_folder s3/input \\\nbatch \\\n--bucket my_bucket \\\n--s3_folder s3/output \\\nnone \\\nprocess\n</code></pre>"},{"location":"ocr/ParseXPS/#geniusrise_ocr.readers.xps.ParseXPS.__init__--using-geniusrise-to-invoke-via-yaml-file","title":"Using geniusrise to invoke via YAML file","text":"<pre><code>version: \"1\"\nspouts:\nparse_xps:\nname: \"ParseXPS\"\nmethod: \"process\"\ninput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/input\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/output\"\n</code></pre>"},{"location":"ocr/ParseXPS/#geniusrise_ocr.readers.xps.ParseXPS.process","title":"<code>process(input_folder=None)</code>","text":"<p>\ud83d\udcd6 Process XPS files in the given input folder and extract images.</p> <p>Parameters:</p> Name Type Description Default <code>input_folder</code> <code>str</code> <p>The folder containing XPS files to process.</p> <code>None</code> <p>This method iterates through each XPS file in the specified folder and extracts the images.</p>"},{"location":"ocr/Pix2StructImageOCR/","title":"OCR using pix2struct","text":"<p>             Bases: <code>Bolt</code></p>"},{"location":"ocr/Pix2StructImageOCR/#geniusrise_ocr.ocr.pix2struct.bulk.Pix2StructImageOCR.__init__","title":"<code>__init__(input, output, state, model_name='google/pix2struct-large', **kwargs)</code>","text":"<p>The <code>Pix2StructImageOCR</code> class performs OCR on images using Google's Pix2Struct model. It expects the <code>input.input_folder</code> to contain the images for OCR and saves the OCR results as JSON files in <code>output.output_folder</code>.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>Instance of BatchInput for reading data.</p> required <code>output</code> <code>BatchOutput</code> <p>Instance of BatchOutput for saving data.</p> required <code>state</code> <code>State</code> <p>Instance of State for maintaining state.</p> required <code>model_name</code> <code>str</code> <p>The name of the Pix2Struct model to use. Default is \"google/pix2struct-large\".</p> <code>'google/pix2struct-large'</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"ocr/Pix2StructImageOCR/#geniusrise_ocr.ocr.pix2struct.bulk.Pix2StructImageOCR.__init__--command-line-invocation-with-geniusrise","title":"Command Line Invocation with geniusrise","text":"<pre><code>genius Pix2StructImageOCR rise \\\nbatch \\\n--bucket my_bucket \\\n--s3_folder s3/input \\\nbatch \\\n--bucket my_bucket \\\n--s3_folder s3/output \\\nnone \\\nprocess\n</code></pre>"},{"location":"ocr/Pix2StructImageOCR/#geniusrise_ocr.ocr.pix2struct.bulk.Pix2StructImageOCR.__init__--yaml-configuration-with-geniusrise","title":"YAML Configuration with geniusrise","text":"<pre><code>version: \"1\"\nspouts:\nocr_processing:\nname: \"Pix2StructImageOCR\"\nmethod: \"process\"\ninput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/input\"\nuse_cuda: true\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/output\"\nuse_cuda: true\n</code></pre>"},{"location":"ocr/Pix2StructImageOCR/#geniusrise_ocr.ocr.pix2struct.bulk.Pix2StructImageOCR.process","title":"<code>process(use_cuda=True)</code>","text":"<p>\ud83d\udcd6 Perform OCR on images in the input folder and save the OCR results as JSON files in the output folder.</p> <p>Parameters:</p> Name Type Description Default <code>use_cuda</code> <code>bool</code> <p>Whether to use CUDA for model inference. Default is True.</p> <code>True</code>"},{"location":"ocr/Pix2StructImageOCRAPI/","title":"OCR API using pix2struct","text":"<p>             Bases: <code>Bolt</code></p>"},{"location":"ocr/Pix2StructImageOCRAPI/#geniusrise_ocr.ocr.pix2struct.api.Pix2StructImageOCRAPI.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>The <code>Pix2StructImageOCRAPI</code> class performs OCR on images using Google's Pix2Struct model. The class exposes an API endpoint for OCR on single images. The endpoint is accessible at <code>/api/v1/ocr</code>. The API takes a POST request with a JSON payload containing a base64 encoded image under the key <code>image_base64</code>. It returns a JSON response containing the OCR result under the key <code>ocr_text</code>.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>Instance of BatchInput for reading data.</p> required <code>output</code> <code>BatchOutput</code> <p>Instance of BatchOutput for saving data.</p> required <code>state</code> <code>State</code> <p>Instance of State for maintaining state.</p> required <code>model_name</code> <code>str</code> <p>The name of the Pix2Struct model to use. Default is \"google/pix2struct-large\".</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"ocr/Pix2StructImageOCRAPI/#geniusrise_ocr.ocr.pix2struct.api.Pix2StructImageOCRAPI.__init__--command-line-invocation-with-geniusrise","title":"Command Line Invocation with geniusrise","text":"<pre><code>genius Pix2StructImageOCRAPI rise \\\nbatch \\\n--bucket my_bucket \\\n--s3_folder s3/input \\\nbatch \\\n--bucket my_bucket \\\n--s3_folder s3/output \\\nnone \\\nlisten \\\n--args endpoint=* port=3000 cors_domain=* use_cuda=True\n</code></pre>"},{"location":"ocr/Pix2StructImageOCRAPI/#geniusrise_ocr.ocr.pix2struct.api.Pix2StructImageOCRAPI.__init__--yaml-configuration-with-geniusrise","title":"YAML Configuration with geniusrise","text":"<pre><code>version: \"1\"\nspouts:\nocr_processing:\nname: \"Pix2StructImageOCRAPI\"\nmethod: \"listen\"\nargs:\nendpoint: *\nport: 3000\ncors_domain: *\nuse_cuda: true\ninput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/input\"\nuse_cuda: true\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/output\"\nuse_cuda: true\n</code></pre>"},{"location":"ocr/TROCRImageOCR/","title":"OCR using trocr","text":"<p>             Bases: <code>Bolt</code></p>"},{"location":"ocr/TROCRImageOCR/#geniusrise_ocr.ocr.trocr.bulk.TROCRImageOCR.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>The <code>TROCRImageOCR</code> class performs OCR (Optical Character Recognition) on images using Microsoft's TROCR model. It expects the <code>input.input_folder</code> to contain the images for OCR and saves the OCR results as JSON files in <code>output.output_folder</code>.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>Instance of BatchInput for reading data.</p> required <code>output</code> <code>BatchOutput</code> <p>Instance of BatchOutput for saving data.</p> required <code>state</code> <code>State</code> <p>Instance of State for maintaining state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"ocr/TROCRImageOCR/#geniusrise_ocr.ocr.trocr.bulk.TROCRImageOCR.__init__--command-line-invocation-with-geniusrise","title":"Command Line Invocation with geniusrise","text":"<pre><code>genius TROCRImageOCR rise \\\nbatch \\\n--bucket my_bucket \\\n--s3_folder s3/input \\\nbatch \\\n--bucket my_bucket \\\n--s3_folder s3/output \\\nnone \\\nprocess\n</code></pre>"},{"location":"ocr/TROCRImageOCR/#geniusrise_ocr.ocr.trocr.bulk.TROCRImageOCR.__init__--yaml-configuration-with-geniusrise","title":"YAML Configuration with geniusrise","text":"<pre><code>version: \"1\"\nspouts:\nocr_processing:\nname: \"TROCRImageOCR\"\nmethod: \"process\"\ninput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/input\"\nuse_cuda: true\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/output\"\nuse_cuda: true\n</code></pre>"},{"location":"ocr/TROCRImageOCR/#geniusrise_ocr.ocr.trocr.bulk.TROCRImageOCR.process","title":"<code>process(kind='printed', use_cuda=True)</code>","text":"<p>\ud83d\udcd6 Perform OCR on images in the input folder and save the OCR results as JSON files in the output folder.</p> <p>This method iterates through each image file in <code>input.input_folder</code>, performs OCR using the TROCR model, and saves the OCR results as JSON files in <code>output.output_folder</code>.</p> <p>Parameters:</p> Name Type Description Default <code>kind</code> <code>str</code> <p>The kind of TROCR model to use. Default is \"printed\". Options are \"printed\" or \"handwritten\".</p> <code>'printed'</code> <code>use_cuda</code> <code>bool</code> <p>Whether to use CUDA for model inference. Default is True.</p> <code>True</code>"},{"location":"ocr/TROCRImageOCRAPI/","title":"OCR API using trocr","text":"<p>             Bases: <code>Bolt</code></p>"},{"location":"ocr/TROCRImageOCRAPI/#geniusrise_ocr.ocr.trocr.api.TROCRImageOCRAPI.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>The <code>TROCRImageOCR</code> class performs OCR (Optical Character Recognition) on images using Microsoft's TROCR model. The class exposes an API endpoint for OCR on single images. The endpoint is accessible at <code>/api/v1/ocr</code>. The API takes a POST request with a JSON payload containing a base64 encoded image under the key <code>image_base64</code>. It returns a JSON response containing the OCR result under the key <code>ocr_text</code>.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>Instance of BatchInput for reading data.</p> required <code>output</code> <code>BatchOutput</code> <p>Instance of BatchOutput for saving data.</p> required <code>state</code> <code>State</code> <p>Instance of State for maintaining state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"ocr/TROCRImageOCRAPI/#geniusrise_ocr.ocr.trocr.api.TROCRImageOCRAPI.__init__--command-line-invocation-with-geniusrise","title":"Command Line Invocation with geniusrise","text":"<pre><code>genius TROCRImageOCR rise \\\nbatch \\\n--bucket my_bucket \\\n--s3_folder s3/input \\\nbatch \\\n--bucket my_bucket \\\n--s3_folder s3/output \\\nnone \\\nlisten \\\n--args endpoint=* port=3000 cors_domain=* kind=handwriting use_cuda=True\n</code></pre>"},{"location":"ocr/TROCRImageOCRAPI/#geniusrise_ocr.ocr.trocr.api.TROCRImageOCRAPI.__init__--yaml-configuration-with-geniusrise","title":"YAML Configuration with geniusrise","text":"<pre><code>version: \"1\"\nspouts:\nocr_processing:\nname: \"TROCRImageOCR\"\nmethod: \"listen\"\nargs:\nendpoint: *\nport: 3000\ncors_domain: *\nkind: handwriting\nuse_cuda: true\ninput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/input\"\nuse_cuda: true\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/output\"\nuse_cuda: true\n</code></pre>"},{"location":"ocr/TROCRImageOCRAPI/#geniusrise_ocr.ocr.trocr.api.TROCRImageOCRAPI.__init__--api-example","title":"API Example","text":"<pre><code>curl -X POST \"http://localhost:3000/api/v1/ocr\" -H \"Content-Type: application/json\" -d '{\"image_base64\": \"your_base64_encoded_image_here\"}'\n</code></pre>"},{"location":"ocr/TROCRImageOCRAPI/#geniusrise_ocr.ocr.trocr.api.TROCRImageOCRAPI.preprocess_and_detect_boxes","title":"<code>preprocess_and_detect_boxes(image)</code>","text":"<p>Preprocess the image and detect text bounding boxes using the EAST model.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image.Image</code> <p>PIL Image object.</p> required <p>Returns:</p> Type Description <code>List[Tuple[int, int, int, int]]</code> <p>List[Tuple[int, int, int, int]]: List of bounding boxes (x, y, w, h).</p>"},{"location":"ocr/TrainImageClassifier/","title":"Train image classifier","text":"<p>             Bases: <code>Bolt</code></p>"},{"location":"ocr/TrainImageClassifier/#geniusrise_ocr.classification.train.TrainImageClassifier.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>The <code>TrainImageClassifier</code> class trains an image classifier using a ResNet-152 model. It assumes that the <code>input.input_folder</code> contains sub-folders named 'train' and 'test'. Each of these sub-folders should contain class-specific folders with images. The trained model is saved as 'model.pth' in <code>output.output_folder</code>.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>Instance of BatchInput for reading data.</p> required <code>output</code> <code>BatchOutput</code> <p>Instance of BatchOutput for saving data.</p> required <code>state</code> <code>State</code> <p>Instance of State for maintaining state.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"ocr/TrainImageClassifier/#geniusrise_ocr.classification.train.TrainImageClassifier.__init__--command-line-invocation-with-geniusrise","title":"Command Line Invocation with geniusrise","text":"<pre><code>genius TrainImageClassifier rise \\\nbatch \\\n--bucket my_bucket \\\n--s3_folder s3/input \\\nbatch \\\n--bucket my_bucket \\\n--s3_folder s3/output \\\nnone \\\nprocess \\\n--args num_classes=4 epochs=10 batch_size=32 learning_rate=0.001\n</code></pre>"},{"location":"ocr/TrainImageClassifier/#geniusrise_ocr.classification.train.TrainImageClassifier.__init__--yaml-configuration-with-geniusrise","title":"YAML Configuration with geniusrise","text":"<pre><code>version: \"1\"\nspouts:\nimage_training:\nname: \"TrainImageClassifier\"\nmethod: \"process\"\nargs:\nnum_classes: 4\nepochs: 10\nbatch_size: 32\nlearning_rate: 0.001\ninput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/input\"\noutput:\ntype: \"batch\"\nargs:\nbucket: \"my_bucket\"\ns3_folder: \"s3/output\"\n</code></pre>"},{"location":"ocr/TrainImageClassifier/#geniusrise_ocr.classification.train.TrainImageClassifier.process","title":"<code>process(num_classes=4, epochs=10, batch_size=32, learning_rate=0.001, use_cuda=False)</code>","text":"<p>\ud83d\udcd6 Train an image classifier using a ResNet-152 model.</p> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <code>int</code> <p>Number of classes of the images.</p> <code>4</code> <code>epochs</code> <code>int</code> <p>Number of training epochs. Default is 10.</p> <code>10</code> <code>batch_size</code> <code>int</code> <p>Batch size for training. Default is 32.</p> <code>32</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for the optimizer. Default is 0.001.</p> <code>0.001</code> <code>use_cuda</code> <code>bool</code> <p>Whether to use CUDA for model training. Default is False.</p> <code>False</code> <p>This method trains a ResNet-152 model using the images in the 'train' and 'test' sub-folders of <code>input.input_folder</code>. Each of these sub-folders should contain class-specific folders with images. The trained model is saved as 'model.pth' in <code>output.output_folder</code>.</p>"},{"location":"text/api/base/","title":"Base Fine Tuner","text":"<p>             Bases: <code>TextBulk</code></p> <p>A class representing a Hugging Face API for generating text using a pre-trained language model.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>Any</code> <p>The pre-trained language model.</p> <code>tokenizer</code> <code>Any</code> <p>The tokenizer used to preprocess input text.</p> <code>model_name</code> <code>str</code> <p>The name of the pre-trained language model.</p> <code>model_revision</code> <code>Optional[str]</code> <p>The revision of the pre-trained language model.</p> <code>tokenizer_name</code> <code>str</code> <p>The name of the tokenizer used to preprocess input text.</p> <code>tokenizer_revision</code> <code>Optional[str]</code> <p>The revision of the tokenizer used to preprocess input text.</p> <code>model_class</code> <code>str</code> <p>The name of the class of the pre-trained language model.</p> <code>tokenizer_class</code> <code>str</code> <p>The name of the class of the tokenizer used to preprocess input text.</p> <code>use_cuda</code> <code>bool</code> <p>Whether to use a GPU for inference.</p> <code>quantization</code> <code>int</code> <p>The level of quantization to use for the pre-trained language model.</p> <code>precision</code> <code>str</code> <p>The precision to use for the pre-trained language model.</p> <code>device_map</code> <code>str | Dict | None</code> <p>The mapping of devices to use for inference.</p> <code>max_memory</code> <code>Dict[int, str]</code> <p>The maximum memory to use for inference.</p> <code>torchscript</code> <code>bool</code> <p>Whether to use a TorchScript-optimized version of the pre-trained language model.</p> <code>model_args</code> <code>Any</code> <p>Additional arguments to pass to the pre-trained language model.</p> Methods <p>text(**kwargs: Any) -&gt; Dict[str, Any]:     Generates text based on the given prompt and decoding strategy.</p> <p>listen(model_name: str, model_class: str = \"AutoModelForCausalLM\", tokenizer_class: str = \"AutoTokenizer\", use_cuda: bool = False, precision: str = \"float16\", quantization: int = 0, device_map: str | Dict | None = \"auto\", max_memory={0: \"24GB\"}, torchscript: bool = True, endpoint: str = \"\", port: int = 3000, cors_domain: str = \"http://localhost:3000\", username: Optional[str] = None, password: Optional[str] = None, *model_args: Any) -&gt; None:     Starts a CherryPy server to listen for requests to generate text.</p>"},{"location":"text/api/base/#geniusrise_text.base.api.TextAPI.__init__","title":"<code>__init__(input, output, state)</code>","text":"<p>Initializes a new instance of the TextAPI class.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>The input data to process.</p> required <code>output</code> <code>BatchOutput</code> <p>The output data to process.</p> required <code>state</code> <code>State</code> <p>The state of the API.</p> required"},{"location":"text/api/base/#geniusrise_text.base.api.TextAPI.listen","title":"<code>listen(model_name, model_class='AutoModelForCausalLM', tokenizer_class='AutoTokenizer', use_cuda=False, precision='float16', quantization=0, device_map='auto', max_memory={0: '24GB'}, torchscript=False, compile=False, awq_enabled=False, flash_attention=False, concurrent_queries=False, use_vllm=False, use_llama_cpp=False, vllm_tokenizer_mode='auto', vllm_download_dir=None, vllm_load_format='auto', vllm_seed=42, vllm_max_model_len=1024, vllm_enforce_eager=False, vllm_max_context_len_to_capture=8192, vllm_block_size=16, vllm_gpu_memory_utilization=0.9, vllm_swap_space=4, vllm_sliding_window=None, vllm_pipeline_parallel_size=1, vllm_tensor_parallel_size=1, vllm_worker_use_ray=False, vllm_max_parallel_loading_workers=None, vllm_disable_custom_all_reduce=False, vllm_max_num_batched_tokens=None, vllm_max_num_seqs=64, vllm_max_paddings=512, vllm_max_lora_rank=None, vllm_max_loras=None, vllm_max_cpu_loras=None, vllm_lora_extra_vocab_size=0, vllm_placement_group=None, vllm_log_stats=False, llama_cpp_filename=None, llama_cpp_n_gpu_layers=0, llama_cpp_split_mode=llama_cpp.LLAMA_SPLIT_LAYER, llama_cpp_tensor_split=None, llama_cpp_vocab_only=False, llama_cpp_use_mmap=True, llama_cpp_use_mlock=False, llama_cpp_kv_overrides=None, llama_cpp_seed=llama_cpp.LLAMA_DEFAULT_SEED, llama_cpp_n_ctx=2048, llama_cpp_n_batch=512, llama_cpp_n_threads=None, llama_cpp_n_threads_batch=None, llama_cpp_rope_scaling_type=llama_cpp.LLAMA_ROPE_SCALING_UNSPECIFIED, llama_cpp_rope_freq_base=0.0, llama_cpp_rope_freq_scale=0.0, llama_cpp_yarn_ext_factor=-1.0, llama_cpp_yarn_attn_factor=1.0, llama_cpp_yarn_beta_fast=32.0, llama_cpp_yarn_beta_slow=1.0, llama_cpp_yarn_orig_ctx=0, llama_cpp_mul_mat_q=True, llama_cpp_logits_all=False, llama_cpp_embedding=False, llama_cpp_offload_kqv=True, llama_cpp_last_n_tokens_size=64, llama_cpp_lora_base=None, llama_cpp_lora_scale=1.0, llama_cpp_lora_path=None, llama_cpp_numa=False, llama_cpp_chat_format=None, llama_cpp_draft_model=None, llama_cpp_verbose=True, endpoint='*', port=3000, cors_domain='http://localhost:3000', username=None, password=None, **model_args)</code>","text":"<p>Starts a CherryPy server to listen for requests to generate text.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name or identifier of the pre-trained model to be used.</p> required <code>model_class</code> <code>str</code> <p>Class name of the model to be used from the transformers library.</p> <code>'AutoModelForCausalLM'</code> <code>tokenizer_class</code> <code>str</code> <p>Class name of the tokenizer to be used from the transformers library.</p> <code>'AutoTokenizer'</code> <code>use_cuda</code> <code>bool</code> <p>Flag to enable CUDA for GPU acceleration.</p> <code>False</code> <code>precision</code> <code>str</code> <p>Specifies the precision configuration for PyTorch tensors, e.g., \"float16\".</p> <code>'float16'</code> <code>quantization</code> <code>int</code> <p>Level of model quantization to reduce model size and inference time.</p> <code>0</code> <code>device_map</code> <code>Union[str, Dict, None]</code> <p>Maps model layers to specific devices for distributed inference.</p> <code>'auto'</code> <code>max_memory</code> <code>Dict[int, str]</code> <p>Maximum memory allocation for the model on each device.</p> <code>{0: '24GB'}</code> <code>torchscript</code> <code>bool</code> <p>Enables the use of TorchScript for model optimization.</p> <code>False</code> <code>compile</code> <code>bool</code> <p>Enables model compilation for further optimization.</p> <code>False</code> <code>awq_enabled</code> <code>bool</code> <p>Enables Adaptive Weight Quantization (AWQ) for model optimization.</p> <code>False</code> <code>flash_attention</code> <code>bool</code> <p>Utilizes Flash Attention optimizations for faster processing.</p> <code>False</code> <code>concurrent_queries</code> <code>bool</code> <p>Allows the server to handle multiple requests concurrently if True.</p> <code>False</code> <code>use_vllm</code> <code>bool</code> <p>Flag to use Very Large Language Models (VLLM) integration.</p> <code>False</code> <code>use_llama_cpp</code> <code>bool</code> <p>Flag to use llama.cpp integration for language model inference.</p> <code>False</code> <code>llama_cpp_filename</code> <code>Optional[str]</code> <p>The filename of the model file for llama.cpp.</p> <code>None</code> <code>llama_cpp_n_gpu_layers</code> <code>int</code> <p>Number of layers to offload to GPU in llama.cpp configuration.</p> <code>0</code> <code>llama_cpp_split_mode</code> <code>int</code> <p>Defines how the model is split across multiple GPUs in llama.cpp.</p> <code>llama_cpp.LLAMA_SPLIT_LAYER</code> <code>llama_cpp_tensor_split</code> <code>Optional[List[float]]</code> <p>Custom tensor split configuration for llama.cpp.</p> <code>None</code> <code>llama_cpp_vocab_only</code> <code>bool</code> <p>Loads only the vocabulary part of the model in llama.cpp.</p> <code>False</code> <code>llama_cpp_use_mmap</code> <code>bool</code> <p>Enables memory-mapped files for model loading in llama.cpp.</p> <code>True</code> <code>llama_cpp_use_mlock</code> <code>bool</code> <p>Locks the model in RAM to prevent swapping in llama.cpp.</p> <code>False</code> <code>llama_cpp_kv_overrides</code> <code>Optional[Dict[str, Union[bool, int, float]]]</code> <p>Key-value pairs for overriding default llama.cpp model parameters.</p> <code>None</code> <code>llama_cpp_seed</code> <code>int</code> <p>Seed for random number generation in llama.cpp.</p> <code>llama_cpp.LLAMA_DEFAULT_SEED</code> <code>llama_cpp_n_ctx</code> <code>int</code> <p>The number of context tokens for the model in llama.cpp.</p> <code>2048</code> <code>llama_cpp_n_batch</code> <code>int</code> <p>Batch size for processing prompts in llama.cpp.</p> <code>512</code> <code>llama_cpp_n_threads</code> <code>Optional[int]</code> <p>Number of threads for generation in llama.cpp.</p> <code>None</code> <code>llama_cpp_n_threads_batch</code> <code>Optional[int]</code> <p>Number of threads for batch processing in llama.cpp.</p> <code>None</code> <code>llama_cpp_rope_scaling_type</code> <code>Optional[int]</code> <p>Specifies the RoPE (Rotary Positional Embeddings) scaling type in llama.cpp.</p> <code>llama_cpp.LLAMA_ROPE_SCALING_UNSPECIFIED</code> <code>llama_cpp_rope_freq_base</code> <code>float</code> <p>Base frequency for RoPE in llama.cpp.</p> <code>0.0</code> <code>llama_cpp_rope_freq_scale</code> <code>float</code> <p>Frequency scaling factor for RoPE in llama.cpp.</p> <code>0.0</code> <code>llama_cpp_yarn_ext_factor</code> <code>float</code> <p>Extrapolation mix factor for YaRN in llama.cpp.</p> <code>-1.0</code> <code>llama_cpp_yarn_attn_factor</code> <code>float</code> <p>Attention factor for YaRN in llama.cpp.</p> <code>1.0</code> <code>llama_cpp_yarn_beta_fast</code> <code>float</code> <p>Beta fast parameter for YaRN in llama.cpp.</p> <code>32.0</code> <code>llama_cpp_yarn_beta_slow</code> <code>float</code> <p>Beta slow parameter for YaRN in llama.cpp.</p> <code>1.0</code> <code>llama_cpp_yarn_orig_ctx</code> <code>int</code> <p>Original context size for YaRN in llama.cpp.</p> <code>0</code> <code>llama_cpp_mul_mat_q</code> <code>bool</code> <p>Flag to enable matrix multiplication for queries in llama.cpp.</p> <code>True</code> <code>llama_cpp_logits_all</code> <code>bool</code> <p>Returns logits for all tokens when set to True in llama.cpp.</p> <code>False</code> <code>llama_cpp_embedding</code> <code>bool</code> <p>Enables embedding mode only in llama.cpp.</p> <code>False</code> <code>llama_cpp_offload_kqv</code> <code>bool</code> <p>Offloads K, Q, V matrices to GPU in llama.cpp.</p> <code>True</code> <code>llama_cpp_last_n_tokens_size</code> <code>int</code> <p>Size for the last_n_tokens buffer in llama.cpp.</p> <code>64</code> <code>llama_cpp_lora_base</code> <code>Optional[str]</code> <p>Base model path for LoRA adjustments in llama.cpp.</p> <code>None</code> <code>llama_cpp_lora_scale</code> <code>float</code> <p>Scale factor for LoRA adjustments in llama.cpp.</p> <code>1.0</code> <code>llama_cpp_lora_path</code> <code>Optional[str]</code> <p>Path to LoRA adjustments file in llama.cpp.</p> <code>None</code> <code>llama_cpp_numa</code> <code>Union[bool, int]</code> <p>NUMA configuration for llama.cpp.</p> <code>False</code> <code>llama_cpp_chat_format</code> <code>Optional[str]</code> <p>Specifies the chat format for llama.cpp.</p> <code>None</code> <code>llama_cpp_draft_model</code> <code>Optional[llama_cpp.LlamaDraftModel]</code> <p>Draft model for speculative decoding in llama.cpp.</p> <code>None</code> <code>endpoint</code> <code>str</code> <p>Network interface to bind the server to.</p> <code>'*'</code> <code>port</code> <code>int</code> <p>Port number to listen on for incoming requests.</p> <code>3000</code> <code>cors_domain</code> <code>str</code> <p>Specifies the domain to allow for Cross-Origin Resource Sharing (CORS).</p> <code>'http://localhost:3000'</code> <code>username</code> <code>Optional[str]</code> <p>Username for basic authentication, if required.</p> <code>None</code> <code>password</code> <code>Optional[str]</code> <p>Password for basic authentication, if required.</p> <code>None</code> <code>**model_args</code> <code>Any</code> <p>Additional arguments to pass to the pre-trained language model or llama.cpp configuration.</p> <code>{}</code>"},{"location":"text/api/base/#geniusrise_text.base.api.TextAPI.text","title":"<code>text(**kwargs)</code>","text":"<p>Generates text based on the given prompt and decoding strategy.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to the pre-trained language model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the prompt, arguments, and generated text.</p>"},{"location":"text/api/base/#geniusrise_text.base.api.TextAPI.validate_password","title":"<code>validate_password(realm, username, password)</code>","text":"<p>Validate the username and password against expected values.</p> <p>Parameters:</p> Name Type Description Default <code>realm</code> <code>str</code> <p>The authentication realm.</p> required <code>username</code> <code>str</code> <p>The provided username.</p> required <code>password</code> <code>str</code> <p>The provided password.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if credentials are valid, False otherwise.</p>"},{"location":"text/api/classification/","title":"Classification","text":"<p>             Bases: <code>TextAPI</code></p> <p>TextClassificationAPI leveraging Hugging Face's transformers for text classification tasks. This API provides an interface to classify text into various categories like sentiment, topic, intent, etc.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>AutoModelForSequenceClassification</code> <p>A Hugging Face model for sequence classification.</p> <code>tokenizer</code> <code>AutoTokenizer</code> <p>A tokenizer for preprocessing text.</p> <code>hf_pipeline</code> <code>Pipeline</code> <p>A Hugging Face pipeline for text classification.</p> Methods <p>classify(self): Classifies text using the model and tokenizer. classification_pipeline(self): Classifies text using the Hugging Face pipeline. initialize_pipeline(self): Lazy initialization of the classification pipeline.</p> <p>Example CLI Usage: <pre><code>genius TextClassificationAPI rise \\\nbatch \\\n--input_folder ./input \\\nbatch \\\n--output_folder ./output \\\nnone \\\n--id cardiffnlp/twitter-roberta-base-hate-multiclass-latest-lol \\\nlisten \\\n--args \\\nmodel_name=\"cardiffnlp/twitter-roberta-base-hate-multiclass-latest\" \\\nmodel_class=\"AutoModelForSequenceClassification\" \\\ntokenizer_class=\"AutoTokenizer\" \\\nuse_cuda=True \\\nprecision=\"float\" \\\nquantization=0 \\\ndevice_map=\"cuda:0\" \\\nmax_memory=None \\\ntorchscript=False \\\nendpoint=\"*\" \\\nport=3000 \\\ncors_domain=\"http://localhost:3000\" \\\nusername=\"user\" \\\npassword=\"password\"\n</code></pre></p>"},{"location":"text/api/classification/#geniusrise_text.classification.api.TextClassificationAPI.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>Initializes the TextClassificationAPI with the necessary configurations for input, output, and state management.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>Configuration for the input data.</p> required <code>output</code> <code>BatchOutput</code> <p>Configuration for the output data.</p> required <code>state</code> <code>State</code> <p>State management for the API.</p> required <code>**kwargs</code> <p>Additional keyword arguments for extended functionality.</p> <code>{}</code>"},{"location":"text/api/classification/#geniusrise_text.classification.api.TextClassificationAPI.classification_pipeline","title":"<code>classification_pipeline()</code>","text":"<p>Accepts text input and returns classification results using the Hugging Face pipeline.</p> <p>This method uses the Hugging Face pipeline for efficient and robust text classification. It's suitable for various classification tasks such as sentiment analysis, topic classification, and intent recognition.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the original input text and the classification results.</p> <p>Example CURL Request for text classification: <pre><code>/usr/bin/curl -X POST localhost:3000/api/v1/classification_pipeline             -H \"Content-Type: application/json\"             -d '{\"text\": \"The movie was fantastic, with great acting and plot.\"}' | jq\n</code></pre></p>"},{"location":"text/api/classification/#geniusrise_text.classification.api.TextClassificationAPI.classify","title":"<code>classify()</code>","text":"<p>Accepts text input and returns classification results. The method uses the model and tokenizer to classify the text and provide the likelihood of each class label.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the original input text and the classification scores for each label.</p> <p>Example CURL Request for text classification: <pre><code>/usr/bin/curl -X POST localhost:3000/api/v1/classify             -H \"Content-Type: application/json\"             -d '{\n        \"text\": \"tata sons lost a major contract to its rival mahindra motors\"\n    }' | jq\n</code></pre></p>"},{"location":"text/api/classification/#geniusrise_text.classification.api.TextClassificationAPI.initialize_pipeline","title":"<code>initialize_pipeline()</code>","text":"<p>Lazy initialization of the Hugging Face pipeline for classification.</p>"},{"location":"text/api/instruction_tuning/","title":"Instruction Tuning","text":"<p>             Bases: <code>TextAPI</code></p> <p>InstructionAPI is designed for generating text based on prompts using instruction-tuned language models. It serves as an interface to Hugging Face's pre-trained instruction-tuned models, providing a flexible API for various text generation tasks. It can be used in scenarios ranging from generating creative content to providing instructions or answers based on the prompts.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>Any</code> <p>The loaded instruction-tuned language model.</p> <code>tokenizer</code> <code>Any</code> <p>The tokenizer for processing text suitable for the model.</p> Methods <p>complete(**kwargs: Any) -&gt; Dict[str, Any]:     Generates text based on the given prompt and decoding strategy.</p> <p>listen(**model_args: Any) -&gt; None:     Starts a server to listen for text generation requests.</p> <p>CLI Usage Example: <pre><code>genius InstructionAPI rise \\\nbatch \\\n--input_folder ./input \\\nbatch \\\n--output_folder ./output \\\nnone \\\nlisten \\\n--args \\\nmodel_name=\"TheBloke/Mistral-7B-OpenOrca-AWQ\" \\\nmodel_class=\"AutoModelForCausalLM\" \\\ntokenizer_class=\"AutoTokenizer\" \\\nuse_cuda=True \\\nprecision=\"float16\" \\\nquantization=0 \\\ndevice_map=\"auto\" \\\nmax_memory=None \\\ntorchscript=False \\\nawq_enabled=True \\\nflash_attention=True \\\nendpoint=\"*\" \\\nport=3001 \\\ncors_domain=\"http://localhost:3000\" \\\nusername=\"user\" \\\npassword=\"password\"\n</code></pre></p> <p>Or using VLLM: <pre><code>genius InstructionAPI rise \\\nbatch \\\n--input_folder ./input \\\nbatch \\\n--output_folder ./output \\\nnone \\\n--id mistralai/Mistral-7B-Instruct-v0.1 \\\nlisten \\\n--args \\\nmodel_name=\"mistralai/Mistral-7B-Instruct-v0.1\" \\\nmodel_class=\"AutoModelForCausalLM\" \\\ntokenizer_class=\"AutoTokenizer\" \\\nuse_cuda=True \\\nprecision=\"bfloat16\" \\\nquantization=0 \\\ndevice_map=\"auto\" \\\nmax_memory=None \\\ntorchscript=False \\\nuse_vllm=True \\\nvllm_enforce_eager=True \\\nvllm_max_model_len=1024 \\\nconcurrent_queries=False \\\nendpoint=\"*\" \\\nport=3000 \\\ncors_domain=\"http://localhost:3000\" \\\nusername=\"user\" \\\npassword=\"password\"\n</code></pre></p> <p>or using llama.cpp: <pre><code>genius InstructionAPI rise \\\nbatch \\\n--input_folder ./input \\\nbatch \\\n--output_folder ./output \\\nnone \\\nlisten \\\n--args \\\nmodel_name=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\" \\\nmodel_class=\"AutoModelForCausalLM\" \\\ntokenizer_class=\"AutoTokenizer\" \\\nuse_cuda=True \\\nuse_llama_cpp=True \\\nllama_cpp_filename=\"mistral-7b-instruct-v0.2.Q4_K_M.gguf\" \\\nllama_cpp_n_gpu_layers=35 \\\nllama_cpp_n_ctx=32768 \\\nconcurrent_queries=False \\\nendpoint=\"*\" \\\nport=3000 \\\ncors_domain=\"http://localhost:3000\" \\\nusername=\"user\" \\\npassword=\"password\"\n</code></pre></p>"},{"location":"text/api/instruction_tuning/#geniusrise_text.instruction.api.InstructionAPI.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>Initializes a new instance of the InstructionAPI class, setting up the necessary configurations for input, output, and state.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>Configuration for the input data.</p> required <code>output</code> <code>BatchOutput</code> <p>Configuration for the output data.</p> required <code>state</code> <code>State</code> <p>The state of the API.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for extended functionality.</p> <code>{}</code>"},{"location":"text/api/instruction_tuning/#geniusrise_text.instruction.api.InstructionAPI.chat","title":"<code>chat(**kwargs)</code>","text":"<p>Handles chat interaction using the Hugging Face pipeline. This method enables conversational text generation, simulating a chat-like interaction based on user and system prompts.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments containing 'user_prompt' and 'system_prompt'.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the user prompt, system prompt, and chat interaction results.</p> <p>Example CURL Request for chat interaction: <pre><code>/usr/bin/curl -X POST localhost:3001/api/v1/chat             -H \"Content-Type: application/json\"             -d '{\n        \"user_prompt\": \"What is the capital of France?\",\n        \"system_prompt\": \"The capital of France is\"\n    }' | jq\n</code></pre></p>"},{"location":"text/api/instruction_tuning/#geniusrise_text.instruction.api.InstructionAPI.chat_llama_cpp","title":"<code>chat_llama_cpp(**kwargs)</code>","text":"<p>Handles POST requests to generate chat completions using the llama.cpp engine. This method accepts various parameters for customizing the chat completion request, including messages, sampling settings, and more.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Dict[str, str]]</code> <p>The chat messages for generating a response.</p> required <code>functions</code> <code>Optional[List[Dict]]</code> <p>A list of functions to use for the chat completion (advanced usage).</p> required <code>function_call</code> <code>Optional[Dict]</code> <p>A function call to use for the chat completion (advanced usage).</p> required <code>tools</code> <code>Optional[List[Dict]]</code> <p>A list of tools to use for the chat completion (advanced usage).</p> required <code>tool_choice</code> <code>Optional[Dict]</code> <p>A tool choice option for the chat completion (advanced usage).</p> required <code>temperature</code> <code>float</code> <p>The temperature to use for sampling, controlling randomness.</p> required <code>top_p</code> <code>float</code> <p>The nucleus sampling's top-p parameter, controlling diversity.</p> required <code>top_k</code> <code>int</code> <p>The top-k sampling parameter, limiting the token selection pool.</p> required <code>min_p</code> <code>float</code> <p>The minimum probability threshold for sampling.</p> required <code>typical_p</code> <code>float</code> <p>The typical-p parameter for locally typical sampling.</p> required <code>stream</code> <code>bool</code> <p>Flag to stream the results.</p> required <code>stop</code> <code>Optional[Union[str, List[str]]]</code> <p>Tokens or sequences where generation should stop.</p> required <code>seed</code> <code>Optional[int]</code> <p>Seed for random number generation to ensure reproducibility.</p> required <code>response_format</code> <code>Optional[Dict]</code> <p>Specifies the format of the generated response.</p> required <code>max_tokens</code> <code>Optional[int]</code> <p>Maximum number of tokens to generate.</p> required <code>presence_penalty</code> <code>float</code> <p>Penalty for token presence to discourage repetition.</p> required <code>frequency_penalty</code> <code>float</code> <p>Penalty for token frequency to discourage common tokens.</p> required <code>repeat_penalty</code> <code>float</code> <p>Penalty applied to tokens that are repeated.</p> required <code>tfs_z</code> <code>float</code> <p>Tail-free sampling parameter to adjust the likelihood of tail tokens.</p> required <code>mirostat_mode</code> <code>int</code> <p>Mirostat sampling mode for dynamic adjustments.</p> required <code>mirostat_tau</code> <code>float</code> <p>Tau parameter for mirostat sampling, controlling deviation.</p> required <code>mirostat_eta</code> <code>float</code> <p>Eta parameter for mirostat sampling, controlling adjustment speed.</p> required <code>model</code> <code>Optional[str]</code> <p>Specifies the model to use for generation.</p> required <code>logits_processor</code> <code>Optional[List]</code> <p>List of logits processors for advanced generation control.</p> required <code>grammar</code> <code>Optional[Dict]</code> <p>Specifies grammar rules for the generated text.</p> required <code>logit_bias</code> <code>Optional[Dict[str, float]]</code> <p>Adjustments to the logits of specified tokens.</p> required <code>logprobs</code> <code>Optional[bool]</code> <p>Whether to include log probabilities in the output.</p> required <code>top_logprobs</code> <code>Optional[int]</code> <p>Number of top log probabilities to include.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the chat completion response or an error message.</p> <p>Example CURL Request: <pre><code>curl -X POST \"http://localhost:3000/api/v1/chat_llama_cpp\"             -H \"Content-Type: application/json\"             -d '{\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n            {\"role\": \"system\", \"content\": \"The capital of France is\"}\n        ],\n        \"temperature\": 0.2,\n        \"top_p\": 0.95,\n        \"top_k\": 40,\n        \"max_tokens\": 50,\n    }'\n</code></pre></p>"},{"location":"text/api/instruction_tuning/#geniusrise_text.instruction.api.InstructionAPI.chat_vllm","title":"<code>chat_vllm(**kwargs)</code>","text":"<p>Handles POST requests to generate chat completions using the VLLM (Versatile Language Learning Model) engine. This method accepts various parameters for customizing the chat completion request, including message content, generation settings, and more.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Dict[str, str]]</code> <p>The chat messages for generating a response. Each message should include a 'role' (either 'user' or 'system') and 'content'.</p> required <code>temperature</code> <code>float</code> <p>The sampling temperature. Defaults to 0.7. Higher values generate more random completions.</p> required <code>top_p</code> <code>float</code> <p>The nucleus sampling probability. Defaults to 1.0. A smaller value leads to higher diversity.</p> required <code>n</code> <code>int</code> <p>The number of completions to generate. Defaults to 1.</p> required <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens to generate. Controls the length of the generated response.</p> required <code>stop</code> <code>Union[str, List[str]]</code> <p>Sequence(s) where the generation should stop. Can be a single string or a list of strings.</p> required <code>stream</code> <code>bool</code> <p>Whether to stream the response. Streaming may be useful for long completions.</p> required <code>presence_penalty</code> <code>float</code> <p>Adjusts the likelihood of tokens based on their presence in the conversation so far. Defaults to 0.0.</p> required <code>frequency_penalty</code> <code>float</code> <p>Adjusts the likelihood of tokens based on their frequency in the conversation so far. Defaults to 0.0.</p> required <code>logit_bias</code> <code>Dict[str, float]</code> <p>Adjustments to the logits of specified tokens, identified by token IDs as keys and adjustment values as values.</p> required <code>user</code> <code>str</code> <p>An identifier for the user making the request. Can be used for logging or customization.</p> required <code>best_of</code> <code>int</code> <p>Generates 'n' completions server-side and returns the best one. Higher values incur more computation cost.</p> required <code>top_k</code> <code>int</code> <p>Filters the generated tokens to the top-k tokens with the highest probabilities. Defaults to -1, which disables top-k filtering.</p> required <code>ignore_eos</code> <code>bool</code> <p>Whether to ignore the end-of-sentence token in generation. Useful for more fluid continuations.</p> required <code>use_beam_search</code> <code>bool</code> <p>Whether to use beam search instead of sampling for generation. Beam search can produce more coherent results.</p> required <code>stop_token_ids</code> <code>List[int]</code> <p>List of token IDs that should cause generation to stop.</p> required <code>skip_special_tokens</code> <code>bool</code> <p>Whether to skip special tokens (like padding or end-of-sequence tokens) in the output.</p> required <code>spaces_between_special_tokens</code> <code>bool</code> <p>Whether to insert spaces between special tokens in the output.</p> required <code>add_generation_prompt</code> <code>bool</code> <p>Whether to prepend the generation prompt to the output.</p> required <code>echo</code> <code>bool</code> <p>Whether to include the input prompt in the output.</p> required <code>repetition_penalty</code> <code>float</code> <p>Penalty applied to tokens that have been generated previously. Defaults to 1.0, which applies no penalty.</p> required <code>min_p</code> <code>float</code> <p>Sets a minimum threshold for token probabilities. Tokens with probabilities below this threshold are filtered out.</p> required <code>include_stop_str_in_output</code> <code>bool</code> <p>Whether to include the stop string(s) in the output.</p> required <code>length_penalty</code> <code>float</code> <p>Exponential penalty to the length for beam search. Only relevant if use_beam_search is True.</p> required <p>Dict[str, Any]: A dictionary with the chat completion response or an error message.</p> <p>Example CURL Request: <pre><code>curl -X POST \"http://localhost:3000/api/v1/chat_vllm\"             -H \"Content-Type: application/json\"             -d '{\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Whats the weather like in London?\"}\n        ],\n        \"temperature\": 0.7,\n        \"top_p\": 1.0,\n        \"n\": 1,\n        \"max_tokens\": 50,\n        \"stream\": false,\n        \"presence_penalty\": 0.0,\n        \"frequency_penalty\": 0.0,\n        \"logit_bias\": {},\n        \"user\": \"example_user\"\n    }'\n</code></pre> This request asks the VLLM engine to generate a completion for the provided chat context, with specified generation settings.</p>"},{"location":"text/api/instruction_tuning/#geniusrise_text.instruction.api.InstructionAPI.complete","title":"<code>complete(**kwargs)</code>","text":"<pre><code>    Handles POST requests to generate text based on the given prompt and decoding strategy. It uses the pre-trained\n    model specified in the setup to generate a completion for the input prompt.\n\n    Args:\n        **kwargs (Any): Arbitrary keyword arguments containing the 'prompt' and other parameters for text generation.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing the original prompt and the generated completion.\n\n    Example CURL Requests:\n    ```bash\n    /usr/bin/curl -X POST localhost:3001/api/v1/complete             -H \"Content-Type: application/json\"             -d '{\n            \"prompt\": \"&lt;|system|&gt;\n</code></pre> <p>&lt;|end|&gt; &lt;|user|&gt; How do I sort a list in Python?&lt;|end|&gt; &lt;|assistant|&gt;\",                 \"decoding_strategy\": \"generate\",                 \"max_new_tokens\": 100,                 \"do_sample\": true,                 \"temperature\": 0.7,                 \"top_k\": 50,                 \"top_p\": 0.95             }' | jq         ```</p>"},{"location":"text/api/instruction_tuning/#geniusrise_text.instruction.api.InstructionAPI.initialize_pipeline","title":"<code>initialize_pipeline()</code>","text":"<p>Lazy initialization of the Hugging Face pipeline for chat interaction.</p>"},{"location":"text/api/language_model/","title":"Language Model","text":"<p>             Bases: <code>TextAPI</code></p> <p>LanguageModelAPI is a class for interacting with pre-trained language models to generate text. It allows for customizable text generation via a CherryPy web server, handling requests and generating responses using a specified language model. This class is part of the GeniusRise ecosystem for facilitating NLP tasks.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>Any</code> <p>The loaded language model used for text generation.</p> <code>tokenizer</code> <code>Any</code> <p>The tokenizer corresponding to the language model, used for processing input text.</p> Methods <p>complete(**kwargs: Any) -&gt; Dict[str, Any]: Generates text based on provided prompts and model parameters.</p> <p>CLI Usage Example: <pre><code>genius LanguageModelAPI rise \\\nbatch \\\n--input_folder ./input \\\nbatch \\\n--output_folder ./output \\\nnone \\\n--id mistralai/Mistral-7B-v0.1-lol \\\nlisten \\\n--args \\\nmodel_name=\"mistralai/Mistral-7B-v0.1\" \\\nmodel_class=\"AutoModelForCausalLM\" \\\ntokenizer_class=\"AutoTokenizer\" \\\nuse_cuda=True \\\nprecision=\"float16\" \\\nquantization=0 \\\ndevice_map=\"auto\" \\\nmax_memory=None \\\ntorchscript=False \\\nendpoint=\"*\" \\\nport=3000 \\\ncors_domain=\"http://localhost:3000\" \\\nusername=\"user\" \\\npassword=\"password\"\n</code></pre></p> <p>or using VLLM: <pre><code>genius LanguageModelAPI rise \\\nbatch \\\n--input_folder ./input \\\nbatch \\\n--output_folder ./output \\\nnone \\\n--id mistralai/Mistral-7B-v0.1 \\\nlisten \\\n--args \\\nmodel_name=\"mistralai/Mistral-7B-v0.1\" \\\nmodel_class=\"AutoModelForCausalLM\" \\\ntokenizer_class=\"AutoTokenizer\" \\\nuse_cuda=True \\\nprecision=\"bfloat16\" \\\nuse_vllm=True \\\nvllm_enforce_eager=True \\\nvllm_max_model_len=2048 \\\nconcurrent_queries=False \\\nendpoint=\"*\" \\\nport=3000 \\\ncors_domain=\"http://localhost:3000\" \\\nusername=\"user\" \\\npassword=\"password\"\n</code></pre></p> <p>or using llama.cpp: <pre><code>genius LanguageModelAPI rise \\\nbatch \\\n--input_folder ./input \\\nbatch \\\n--output_folder ./output \\\nnone \\\nlisten \\\n--args \\\nmodel_name=\"TheBloke/Mistral-7B-v0.1-GGUF\" \\\nmodel_class=\"AutoModelForCausalLM\" \\\ntokenizer_class=\"AutoTokenizer\" \\\nuse_cuda=True \\\nuse_llama_cpp=True \\\nllama_cpp_filename=\"mistral-7b-v0.1.Q4_K_M.gguf\" \\\nllama_cpp_n_gpu_layers=35 \\\nllama_cpp_n_ctx=32768 \\\nconcurrent_queries=False \\\nendpoint=\"*\" \\\nport=3000 \\\ncors_domain=\"http://localhost:3000\" \\\nusername=\"user\" \\\npassword=\"password\"\n</code></pre></p>"},{"location":"text/api/language_model/#geniusrise_text.language_model.api.LanguageModelAPI.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>Initializes the LanguageModelAPI with configurations for the input, output, and state management, along with any additional model-specific parameters.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>The configuration for input data handling.</p> required <code>output</code> <code>BatchOutput</code> <p>The configuration for output data handling.</p> required <code>state</code> <code>State</code> <p>The state management for the API.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for model configuration and API setup.</p> <code>{}</code>"},{"location":"text/api/language_model/#geniusrise_text.language_model.api.LanguageModelAPI.complete","title":"<code>complete(**kwargs)</code>","text":"<p>Handles POST requests to generate text based on a given prompt and model-specific parameters. This method is exposed as a web endpoint through CherryPy and returns a JSON response containing the original prompt, the generated text, and any additional returned information from the model.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments containing the prompt, and any additional parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary with the original prompt, generated text, and other model-specific information.</p> <p>Example CURL Request: <pre><code>/usr/bin/curl -X POST localhost:3000/api/v1/complete \\\n-H \"Content-Type: application/json\" \\\n-d '{\n        \"prompt\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWrite a PRD for Oauth auth using keycloak\\n\\n### Response:\",\n        \"decoding_strategy\": \"generate\",\n        \"max_new_tokens\": 1024,\n        \"do_sample\": true\n    }' | jq\n</code></pre></p>"},{"location":"text/api/language_model/#geniusrise_text.language_model.api.LanguageModelAPI.complete_llama_cpp","title":"<code>complete_llama_cpp(**kwargs)</code>","text":"<p>Handles POST requests to generate chat completions using the llama.cpp engine. This method accepts various parameters for customizing the chat completion request, including messages, sampling settings, and more.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <p>The prompt to generate text from.</p> required <code>suffix</code> <p>A suffix to append to the generated text. If None, no suffix is appended.</p> required <code>max_tokens</code> <p>The maximum number of tokens to generate. If max_tokens &lt;= 0 or None, the maximum number of tokens to generate is unlimited and depends on n_ctx.</p> required <code>temperature</code> <p>The temperature to use for sampling.</p> required <code>top_p</code> <p>The top-p value to use for nucleus sampling. Nucleus sampling described in academic paper \"The Curious Case of Neural Text Degeneration\" https://arxiv.org/abs/1904.09751</p> required <code>min_p</code> <p>The min-p value to use for minimum p sampling. Minimum P sampling as described in https://github.com/ggerganov/llama.cpp/pull/3841</p> required <code>typical_p</code> <p>The typical-p value to use for sampling. Locally Typical Sampling implementation described in the paper https://arxiv.org/abs/2202.00666.</p> required <code>logprobs</code> <p>The number of logprobs to return. If None, no logprobs are returned.</p> required <code>echo</code> <p>Whether to echo the prompt.</p> required <code>stop</code> <p>A list of strings to stop generation when encountered.</p> required <code>frequency_penalty</code> <p>The penalty to apply to tokens based on their frequency in the prompt.</p> required <code>presence_penalty</code> <p>The penalty to apply to tokens based on their presence in the prompt.</p> required <code>repeat_penalty</code> <p>The penalty to apply to repeated tokens.</p> required <code>top_k</code> <p>The top-k value to use for sampling. Top-K sampling described in academic paper \"The Curious Case of Neural Text Degeneration\" https://arxiv.org/abs/1904.09751</p> required <code>stream</code> <p>Whether to stream the results.</p> required <code>seed</code> <p>The seed to use for sampling.</p> required <code>tfs_z</code> <p>The tail-free sampling parameter. Tail Free Sampling described in https://www.trentonbricken.com/Tail-Free-Sampling/.</p> required <code>mirostat_mode</code> <p>The mirostat sampling mode.</p> required <code>mirostat_tau</code> <p>The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.</p> required <code>mirostat_eta</code> <p>The learning rate used to update <code>mu</code> based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause <code>mu</code> to be updated more quickly, while a smaller learning rate will result in slower updates.</p> required <code>model</code> <p>The name to use for the model in the completion object.</p> required <code>stopping_criteria</code> <p>A list of stopping criteria to use.</p> required <code>logits_processor</code> <p>A list of logits processors to use.</p> required <code>grammar</code> <p>A grammar to use for constrained sampling.</p> required <code>logit_bias</code> <p>A logit bias to use.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the chat completion response or an error message.</p> <p>Example CURL Request: <pre><code>curl -X POST \"http://localhost:3001/api/v1/complete_llama_cpp\"             -H \"Content-Type: application/json\"             -d '{\n        \"prompt\": \"Whats the weather like in London?\",\n        \"temperature\": 0.7,\n        \"top_p\": 0.95,\n        \"top_k\": 40,\n        \"max_tokens\": 50,\n        \"repeat_penalty\": 1.1\n    }'\n</code></pre></p>"},{"location":"text/api/language_model/#geniusrise_text.language_model.api.LanguageModelAPI.complete_vllm","title":"<code>complete_vllm(**kwargs)</code>","text":"<p>Handles POST requests to generate chat completions using the VLLM (Versatile Language Learning Model) engine. This method accepts various parameters for customizing the chat completion request, including message content, generation settings, and more.</p> <ul> <li>**kwargs (Any): Arbitrary keyword arguments. Expects data in JSON format containing any of the following keys:<ul> <li>messages (Union[str, List[Dict[str, str]]]): The messages for the chat context.</li> <li>temperature (float, optional): The sampling temperature. Defaults to 0.7.</li> <li>top_p (float, optional): The nucleus sampling probability. Defaults to 1.0.</li> <li>n (int, optional): The number of completions to generate. Defaults to 1.</li> <li>max_tokens (int, optional): The maximum number of tokens to generate.</li> <li>stop (Union[str, List[str]], optional): Stop sequence to end generation.</li> <li>stream (bool, optional): Whether to stream the response. Defaults to False.</li> <li>presence_penalty (float, optional): The presence penalty. Defaults to 0.0.</li> <li>frequency_penalty (float, optional): The frequency penalty. Defaults to 0.0.</li> <li>logit_bias (Dict[str, float], optional): Adjustments to the logits of specified tokens.</li> <li>user (str, optional): An identifier for the user making the request.</li> <li>(Additional model-specific parameters)</li> </ul> </li> </ul> <p>Dict[str, Any]: A dictionary with the chat completion response or an error message.</p> <p>Example CURL Request: <pre><code>curl -v -X POST \"http://localhost:3000/api/v1/complete_vllm\"             -H \"Content-Type: application/json\"             -u \"user:password\"             -d '{\n        \"messages\": [\"Whats the weather like in London?\"],\n        \"temperature\": 0.7,\n        \"top_p\": 1.0,\n        \"n\": 1,\n        \"max_tokens\": 50,\n        \"stream\": false,\n        \"presence_penalty\": 0.0,\n        \"frequency_penalty\": 0.0,\n        \"logit_bias\": {},\n        \"user\": \"example_user\"\n    }'\n</code></pre> This request asks the VLLM engine to generate a completion for the provided chat context, with specified generation settings.</p>"},{"location":"text/api/ner/","title":"Named Entity Recognition","text":"<p>             Bases: <code>TextAPI</code></p> <p>NamedEntityRecognitionAPI serves a Named Entity Recognition (NER) model using the Hugging Face transformers library. It is designed to recognize and classify named entities in text into predefined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>Any</code> <p>The loaded NER model, typically a Hugging Face transformer model specialized for token classification.</p> <code>tokenizer</code> <code>Any</code> <p>The tokenizer for preprocessing text compatible with the loaded model.</p> <p>Example CLI Usage: <pre><code>genius NamedEntityRecognitionAPI rise \\\nbatch \\\n--input_folder ./input \\\nbatch \\\n--output_folder ./output \\\nnone \\\n--id dslim/bert-large-NER-lol \\\nlisten \\\n--args \\\nmodel_name=\"dslim/bert-large-NER\" \\\nmodel_class=\"AutoModelForTokenClassification\" \\\ntokenizer_class=\"AutoTokenizer\" \\\nuse_cuda=True \\\nprecision=\"float\" \\\nquantization=0 \\\ndevice_map=\"cuda:0\" \\\nmax_memory=None \\\ntorchscript=False \\\nendpoint=\"0.0.0.0\" \\\nport=3000 \\\ncors_domain=\"http://localhost:3000\" \\\nusername=\"user\" \\\npassword=\"password\"\n</code></pre></p>"},{"location":"text/api/ner/#geniusrise_text.ner.api.NamedEntityRecognitionAPI.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>Initializes the NamedEntityRecognitionAPI class.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>The input data.</p> required <code>output</code> <code>BatchOutput</code> <p>The output data.</p> required <code>state</code> <code>State</code> <p>The state data.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"text/api/ner/#geniusrise_text.ner.api.NamedEntityRecognitionAPI.initialize_pipeline","title":"<code>initialize_pipeline()</code>","text":"<p>Lazy initialization of the NER Hugging Face pipeline.</p>"},{"location":"text/api/ner/#geniusrise_text.ner.api.NamedEntityRecognitionAPI.ner_pipeline","title":"<code>ner_pipeline(**kwargs)</code>","text":"<p>Recognizes named entities in the input text using the Hugging Face pipeline.</p> <p>This method leverages a pre-trained NER model to identify and classify entities in text into categories such as names, organizations, locations, etc. It's suitable for processing various types of text content.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments, typically containing 'text' for the input text.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the original input text and a list of recognized entities.</p> <p>Example CURL Request for NER: <pre><code>curl -X POST localhost:3000/api/v1/ner_pipeline             -H \"Content-Type: application/json\"             -d '{\"text\": \"John Doe works at OpenAI in San Francisco.\"}' | jq\n</code></pre></p>"},{"location":"text/api/ner/#geniusrise_text.ner.api.NamedEntityRecognitionAPI.recognize_entities","title":"<code>recognize_entities(**kwargs)</code>","text":"<p>Endpoint for recognizing named entities in the input text using the loaded NER model.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments, typically containing 'text' for the input text.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the original input text and a list of recognized entities             with their respective types.</p> <p>Example CURL Requests: <pre><code>curl -X POST localhost:3000/api/v1/recognize_entities \\\n-H \"Content-Type: application/json\" \\\n-d '{\"text\": \"John Doe works at OpenAI in San Francisco.\"}' | jq\n</code></pre></p> <pre><code>curl -X POST localhost:3000/api/v1/recognize_entities \\\n-H \"Content-Type: application/json\" \\\n-d '{\"text\": \"Alice is going to visit the Eiffel Tower in Paris next summer.\"}' | jq\n</code></pre>"},{"location":"text/api/nli/","title":"Natural Language Inference","text":"<p>             Bases: <code>TextAPI</code></p> <p>Represents a Natural Language Inference (NLI) API leveraging Hugging Face's transformer models. This class is capable of handling various NLI tasks such as entailment, classification, similarity checking, and more. Utilizes CherryPy for exposing API endpoints that can be interacted with via standard HTTP requests.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>AutoModelForSequenceClassification</code> <p>The loaded Hugging Face model for sequence classification tasks.</p> <code>tokenizer</code> <code>AutoTokenizer</code> <p>The tokenizer corresponding to the model, used for processing input text.</p> <p>CLI Usage Example: For interacting with the NLI API, you would typically start the server using a command similar to one listed in the provided examples. After the server is running, you can use CURL commands to interact with the different endpoints.</p> <p>Example:</p> <pre><code>genius NLIAPI rise \\\nbatch \\\n--input_folder ./input \\\nbatch \\\n--output_folder ./output \\\nnone \\\n--id \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7-lol\" \\\nlisten \\\n--args \\\nmodel_name=\"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\" \\\nmodel_class=\"AutoModelForSequenceClassification\" \\\ntokenizer_class=\"AutoTokenizer\" \\\nuse_cuda=True \\\nprecision=\"float\" \\\nquantization=0 \\\ndevice_map=\"cuda:0\" \\\nmax_memory=None \\\ntorchscript=False \\\nendpoint=\"*\" \\\nport=3000 \\\ncors_domain=\"http://localhost:3000\" \\\nusername=\"user\" \\\npassword=\"password\"\n</code></pre>"},{"location":"text/api/nli/#geniusrise_text.nli.api.NLIAPI.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>Initializes the NLIAPI with configurations for handling input, output, and state management.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>Configuration for the input data.</p> required <code>output</code> <code>BatchOutput</code> <p>Configuration for the output data.</p> required <code>state</code> <code>State</code> <p>State management for the API.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for extended functionality.</p> <code>{}</code>"},{"location":"text/api/nli/#geniusrise_text.nli.api.NLIAPI.classify","title":"<code>classify(**kwargs)</code>","text":"<p>Endpoint for classifying the input text into one of the provided candidate labels using zero-shot classification.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments, typically containing 'text' and 'candidate_labels'.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the input text, candidate labels, and classification scores.</p> <p>Example CURL Request: <pre><code>curl -X POST localhost:3000/api/v1/classify \\\n-H \"Content-Type: application/json\" \\\n-d '{\n        \"text\": \"The new movie is a thrilling adventure in space\",\n        \"candidate_labels\": [\"entertainment\", \"politics\", \"business\"]\n    }'\n</code></pre></p>"},{"location":"text/api/nli/#geniusrise_text.nli.api.NLIAPI.detect_intent","title":"<code>detect_intent(**kwargs)</code>","text":"<p>Detects the intent of the input text from a list of possible intents.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text.</p> required <code>intents</code> <code>List[str]</code> <p>A list of possible intents.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the input text and detected intent with its score.</p> <p>Example CURL Request: <pre><code>/usr/bin/curl -X POST localhost:3000/api/v1/detect_intent \\\n-H \"Content-Type: application/json\" \\\n-d '{\n        \"text\": \"Theres something magical about Recurrent Neural Networks (RNNs). I still remember when I trained my first recurrent network for Image Captioning. Within a few dozen minutes of training my first baby model (with rather arbitrarily-chosen hyperparameters) started to generate very nice looking descriptions of images that were on the edge of making sense. Sometimes the ratio of how simple your model is to the quality of the results you get out of it blows past your expectations, and this was one of those times. What made this result so shocking at the time was that the common wisdom was that RNNs were supposed to be difficult to train (with more experience Ive in fact reached the opposite conclusion). Fast forward about a year: Im training RNNs all the time and Ive witnessed their power and robustness many times, and yet their magical outputs still find ways of amusing me.\",\n        \"intents\": [\"teach\",\"sell\",\"note\",\"advertise\",\"promote\"]\n    }' | jq\n</code></pre></p>"},{"location":"text/api/nli/#geniusrise_text.nli.api.NLIAPI.entailment","title":"<code>entailment(**kwargs)</code>","text":"<p>Endpoint for evaluating the entailment relationship between a premise and a hypothesis. It returns the relationship scores across possible labels like entailment, contradiction, and neutral.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments, typically containing 'premise' and 'hypothesis'.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the premise, hypothesis, and their relationship scores.</p> <p>Example CURL Request: <pre><code>/usr/bin/curl -X POST localhost:3000/api/v1/entailment \\\n-H \"Content-Type: application/json\" \\\\\\\n-d '{\n        \"premise\": \"This a very good entry level smartphone, battery last 2-3 days after fully charged when connected to the internet. No memory lag issue when playing simple hidden object games. Performance is beyond my expectation, i bought it with a good bargain, couldnt ask for more!\",\n        \"hypothesis\": \"the phone has an awesome battery life\"\n    }' | jq\n</code></pre> ```</p>"},{"location":"text/api/nli/#geniusrise_text.nli.api.NLIAPI.fact_checking","title":"<code>fact_checking(**kwargs)</code>","text":"<p>Performs fact checking on a statement given a context.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>str</code> <p>The context or background information.</p> required <code>statement</code> <code>str</code> <p>The statement to fact check.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing fact checking scores.</p> <p>Example CURL Request: <pre><code>/usr/bin/curl -X POST localhost:3000/api/v1/fact_checking \\\n-H \"Content-Type: application/json\" \\\n-d '{\n        \"context\": \"Theres something magical about Recurrent Neural Networks (RNNs). I still remember when I trained my first recurrent network for Image Captioning. Within a few dozen minutes of training my first baby model (with rather arbitrarily-chosen hyperparameters) started to generate very nice looking descriptions of images that were on the edge of making sense. Sometimes the ratio of how simple your model is to the quality of the results you get out of it blows past your expectations, and this was one of those times. What made this result so shocking at the time was that the common wisdom was that RNNs were supposed to be difficult to train (with more experience Ive in fact reached the opposite conclusion). Fast forward about a year: Im training RNNs all the time and Ive witnessed their power and robustness many times, and yet their magical outputs still find ways of amusing me.\",\n        \"statement\": \"The author is looking for a home loan\"\n    }' | jq\n</code></pre></p>"},{"location":"text/api/nli/#geniusrise_text.nli.api.NLIAPI.initialize_pipeline","title":"<code>initialize_pipeline()</code>","text":"<p>Lazy initialization of the NLI Hugging Face pipeline.</p>"},{"location":"text/api/nli/#geniusrise_text.nli.api.NLIAPI.question_answering","title":"<code>question_answering(**kwargs)</code>","text":"<p>Performs question answering for multiple choice questions.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>The question text.</p> required <code>choices</code> <code>List[str]</code> <p>A list of possible answers.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the scores for each answer choice.</p> <p>Example CURL Request: <pre><code>/usr/bin/curl -X POST localhost:3000/api/v1/question_answering \\\n-H \"Content-Type: application/json\" \\\n-d '{\n        \"question\": \"[ML-1T-2] is the dimensional formula of\",\n        \"choices\": [\"force\", \"coefficient of friction\", \"modulus of elasticity\", \"energy\"]\n    }' | jq\n</code></pre></p>"},{"location":"text/api/nli/#geniusrise_text.nli.api.NLIAPI.textual_similarity","title":"<code>textual_similarity(**kwargs)</code>","text":"<p>Evaluates the textual similarity between two texts.</p> <p>Parameters:</p> Name Type Description Default <code>text1</code> <code>str</code> <p>The first text.</p> required <code>text2</code> <code>str</code> <p>The second text.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing similarity score.</p> <p>Example CURL Request: <pre><code>/usr/bin/curl -X POST localhost:3000/api/v1/textual_similarity \\\n-H \"Content-Type: application/json\" \\\n-d '{\n        \"text1\": \"Theres something magical about Recurrent Neural Networks (RNNs). I still remember when I trained my first recurrent network for Image Captioning. Within a few dozen minutes of training my first baby model (with rather arbitrarily-chosen hyperparameters) started to generate very nice looking descriptions of images that were on the edge of making sense. Sometimes the ratio of how simple your model is to the quality of the results you get out of it blows past your expectations, and this was one of those times. What made this result so shocking at the time was that the common wisdom was that RNNs were supposed to be difficult to train (with more experience Ive in fact reached the opposite conclusion). Fast forward about a year: Im training RNNs all the time and Ive witnessed their power and robustness many times, and yet their magical outputs still find ways of amusing me.\",\n        \"text2\": \"There is something magical about training neural networks. Their simplicity coupled with their power is astonishing.\"\n    }' | jq\n</code></pre></p>"},{"location":"text/api/nli/#geniusrise_text.nli.api.NLIAPI.zero_shot_classification","title":"<code>zero_shot_classification(**kwargs)</code>","text":"<p>Performs zero-shot classification using the Hugging Face pipeline. It allows classification of text without explicitly provided labels.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments, typically containing 'premise' and 'hypothesis'.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the premise, hypothesis, and their classification scores.</p> <p>Example CURL Request for zero-shot classification: <pre><code>curl -X POST localhost:3000/api/v1/zero_shot_classification             -H \"Content-Type: application/json\"             -d '{\n        \"premise\": \"A new study shows that the Mediterranean diet is good for heart health.\",\n        \"hypothesis\": \"The study is related to diet and health.\"\n    }' | jq\n</code></pre></p>"},{"location":"text/api/question_answering/","title":"Question Answering","text":"<p>             Bases: <code>TextAPI</code></p>"},{"location":"text/api/question_answering/#geniusrise_text.qa.api.QAAPI.tokenizer","title":"<code>tokenizer: AutoTokenizer</code>  <code>instance-attribute</code>","text":"<p>A class for handling different types of QA models, including traditional QA, TAPAS (Table-based QA), and TAPEX. It utilizes the Hugging Face transformers library to provide state-of-the-art question answering capabilities across various formats of data including plain text and tabular data.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>AutoModelForQuestionAnswering | AutoModelForTableQuestionAnswering</code> <p>The pre-trained QA model (traditional, TAPAS, or TAPEX).</p> <code>tokenizer</code> <code>AutoTokenizer</code> <p>The tokenizer used to preprocess input text.</p> Methods <p>answer(self, **kwargs: Any) -&gt; Dict[str, Any]:     Answers questions based on the provided context (text or table).</p> <p>CLI Usage Example: <pre><code>genius QAAPI rise \\\nbatch \\\n--input_folder ./input \\\nbatch \\\n--output_folder ./output \\\nnone \\\n--id distilbert-base-uncased-distilled-squad-lol \\\nlisten \\\n--args \\\nmodel_name=\"distilbert-base-uncased-distilled-squad\" \\\nmodel_class=\"AutoModelForQuestionAnswering\" \\\ntokenizer_class=\"AutoTokenizer\" \\\nuse_cuda=True \\\nprecision=\"float\" \\\nquantization=0 \\\ndevice_map=\"cuda:0\" \\\nmax_memory=None \\\ntorchscript=False \\\nendpoint=\"*\" \\\nport=3000 \\\ncors_domain=\"http://localhost:3000\" \\\nusername=\"user\" \\\npassword=\"password\"\n</code></pre></p> <pre><code>genius QAAPI rise \\\nbatch \\\n--input_folder ./input \\\nbatch \\\n--output_folder ./output \\\nnone \\\n--id google/tapas-base-finetuned-wtq-lol \\\nlisten \\\n--args \\\nmodel_name=\"google/tapas-base-finetuned-wtq\" \\\nmodel_class=\"AutoModelForTableQuestionAnswering\" \\\ntokenizer_class=\"AutoTokenizer\" \\\nuse_cuda=True \\\nprecision=\"float\" \\\nquantization=0 \\\ndevice_map=\"cuda:0\" \\\nmax_memory=None \\\ntorchscript=False \\\nendpoint=\"*\" \\\nport=3000 \\\ncors_domain=\"http://localhost:3000\" \\\nusername=\"user\" \\\npassword=\"password\"\n</code></pre> <pre><code>genius QAAPI rise \\\nbatch \\\n--input_folder ./input \\\nbatch \\\n--output_folder ./output \\\nnone \\\n--id microsoft/tapex-large-finetuned-wtq-lol \\\nlisten \\\n--args \\\nmodel_name=\"microsoft/tapex-large-finetuned-wtq\" \\\nmodel_class=\"AutoModelForSeq2SeqLM\" \\\ntokenizer_class=\"AutoTokenizer\" \\\nuse_cuda=True \\\nprecision=\"float\" \\\nquantization=0 \\\ndevice_map=\"cuda:0\" \\\nmax_memory=None \\\ntorchscript=False \\\nendpoint=\"*\" \\\nport=3000 \\\ncors_domain=\"http://localhost:3000\" \\\nusername=\"user\" \\\npassword=\"password\"\n</code></pre>"},{"location":"text/api/question_answering/#geniusrise_text.qa.api.QAAPI.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>Initializes the QAAPI with configurations for input, output, and state management.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>Configuration for the input data.</p> required <code>output</code> <code>BatchOutput</code> <p>Configuration for the output data.</p> required <code>state</code> <code>State</code> <p>State management for the API.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for extended functionality.</p> <code>{}</code>"},{"location":"text/api/question_answering/#geniusrise_text.qa.api.QAAPI.answer","title":"<code>answer(**kwargs)</code>","text":"<p>Answers questions based on the provided context (text or table). It adapts to the model type (traditional, TAPAS, TAPEX) and provides answers accordingly.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments, typically containing the 'question' and 'data' (context or table).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the question, context/table, and answer(s).</p> <p>Example CURL Request for Text-based QA: <pre><code>curl -X POST localhost:3000/api/v1/answer \\\n-H \"Content-Type: application/json\" \\\n-d '{\"question\": \"What is the capital of France?\", \"data\": \"France is a country in Europe. Its capital is Paris.\"}'\n</code></pre></p> <p>Example CURL Requests: <pre><code>/usr/bin/curl -X POST localhost:3000/api/v1/answer \\\n-H \"Content-Type: application/json\" \\\n-d '{\n        \"data\": \"Theres something magical about Recurrent Neural Networks (RNNs). I still remember when I trained my first recurrent network for Image Captioning. Within a few dozen minutes of training my first baby model (with rather arbitrarily-chosen hyperparameters) started to generate very nice looking descriptions of images that were on the edge of making sense. Sometimes the ratio of how simple your model is to the quality of the results you get out of it blows past your expectations, and this was one of those times. What made this result so shocking at the time was that the common wisdom was that RNNs were supposed to be difficult to train (with more experience Ive in fact reached the opposite conclusion). Fast forward about a year: Im training RNNs all the time and Ive witnessed their power and robustness many times, and yet their magical outputs still find ways of amusing me.\",\n        \"question\": \"What is the common wisdom about RNNs?\"\n    }' | jq\n</code></pre></p> <pre><code>/usr/bin/curl -X POST localhost:3000/api/v1/answer \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"data\": [\n        {\"Name\": \"Alice\", \"Age\": \"30\"},\n        {\"Name\": \"Bob\", \"Age\": \"25\"}\n    ],\n    \"question\": \"what is their total age?\"\n}\n' | jq\n</code></pre> <pre><code>/usr/bin/curl -X POST localhost:3000/api/v1/answer \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"data\": {\"Actors\": [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"], \"Number of movies\": [\"87\", \"53\", \"69\"]},\n    \"question\": \"how many movies does Leonardo Di Caprio have?\"\n}\n' | jq\n</code></pre>"},{"location":"text/api/question_answering/#geniusrise_text.qa.api.QAAPI.answer_pipeline","title":"<code>answer_pipeline(**kwargs)</code>","text":"<p>Answers questions using the Hugging Face pipeline based on the provided context.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments, typically containing 'question' and 'data'.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the question, context, and the answer.</p> <p>Example CURL Request for QA: <pre><code>curl -X POST localhost:3000/api/v1/answer_pipeline             -H \"Content-Type: application/json\"             -d '{\"question\": \"Who is the CEO of Tesla?\", \"data\": \"Elon Musk is the CEO of Tesla.\"}'\n</code></pre></p>"},{"location":"text/api/question_answering/#geniusrise_text.qa.api.QAAPI.answer_table_question","title":"<code>answer_table_question(data, question, model_type)</code>","text":"<p>Answers a question based on the provided table.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>The table data and other parameters.</p> required <code>question</code> <code>str</code> <p>The question to be answered.</p> required <code>model_type</code> <code>str</code> <p>The type of the model ('tapas' or 'tapex').</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>dict</code> <p>The answer derived from the table.</p>"},{"location":"text/api/question_answering/#geniusrise_text.qa.api.QAAPI.initialize_pipeline","title":"<code>initialize_pipeline()</code>","text":"<p>Lazy initialization of the QA Hugging Face pipeline.</p>"},{"location":"text/api/summarization/","title":"Summarization","text":"<p>             Bases: <code>TextAPI</code></p> <p>A class for serving a Hugging Face-based summarization model. This API provides an interface to submit text and receive a summarized version, utilizing state-of-the-art machine learning models for text summarization.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>AutoModelForSeq2SeqLM</code> <p>The loaded Hugging Face model for summarization.</p> <code>tokenizer</code> <code>AutoTokenizer</code> <p>The tokenizer for preprocessing text.</p> Methods <p>summarize(self, **kwargs: Any) -&gt; Dict[str, Any]:     Summarizes the input text based on the given parameters.</p> <p>CLI Usage: <pre><code>genius SummarizationAPI rise \\\nbatch \\\n--input_folder ./input \\\nbatch \\\n--output_folder ./output \\\nnone \\\n--id facebook/bart-large-cnn-lol \\\nlisten \\\n--args \\\nmodel_name=\"facebook/bart-large-cnn\" \\\nmodel_class=\"AutoModelForSeq2SeqLM\" \\\ntokenizer_class=\"AutoTokenizer\" \\\nuse_cuda=True \\\nprecision=\"float\" \\\nquantization=0 \\\ndevice_map=\"cuda:0\" \\\nmax_memory=None \\\ntorchscript=False \\\nendpoint=\"*\" \\\nport=3000 \\\ncors_domain=\"http://localhost:3000\" \\\nusername=\"user\" \\\npassword=\"password\"\n</code></pre></p>"},{"location":"text/api/summarization/#geniusrise_text.summarization.api.SummarizationAPI.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>Initializes the SummarizationAPI class with input, output, and state configurations.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>Configuration for input data.</p> required <code>output</code> <code>BatchOutput</code> <p>Configuration for output data.</p> required <code>state</code> <code>State</code> <p>State management for API.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for extended functionality.</p> <code>{}</code>"},{"location":"text/api/summarization/#geniusrise_text.summarization.api.SummarizationAPI.initialize_pipeline","title":"<code>initialize_pipeline()</code>","text":"<p>Lazy initialization of the summarization Hugging Face pipeline.</p>"},{"location":"text/api/summarization/#geniusrise_text.summarization.api.SummarizationAPI.summarize","title":"<code>summarize(**kwargs)</code>","text":"<p>Summarizes the input text based on the given parameters using a machine learning model. The method accepts parameters via a POST request and returns the summarized text.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments. Expected to receive these from the POST request's JSON body.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the input text and its summary.</p> <p>Example CURL Requests: <pre><code>/usr/bin/curl -X POST localhost:3000/api/v1/summarize \\\n-H \"Content-Type: application/json\" \\\n-d '{\n        \"text\": \"Theres something magical about Recurrent Neural Networks (RNNs). I still remember when I trained my first recurrent network for Image Captioning. Within a few dozen minutes of training my first baby model (with rather arbitrarily-chosen hyperparameters) started to generate very nice looking descriptions of images that were on the edge of making sense. Sometimes the ratio of how simple your model is to the quality of the results you get out of it blows past your expectations, and this was one of those times. What made this result so shocking at the time was that the common wisdom was that RNNs were supposed to be difficult to train (with more experience Ive in fact reached the opposite conclusion). Fast forward about a year: Im training RNNs all the time and Ive witnessed their power and robustness many times, and yet their magical outputs still find ways of amusing me.\",\n        \"decoding_strategy\": \"generate\",\n        \"bos_token_id\": 0,\n        \"decoder_start_token_id\": 2,\n        \"early_stopping\": true,\n        \"eos_token_id\": 2,\n        \"forced_bos_token_id\": 0,\n        \"forced_eos_token_id\": 2,\n        \"length_penalty\": 2.0,\n        \"max_length\": 142,\n        \"min_length\": 56,\n        \"no_repeat_ngram_size\": 3,\n        \"num_beams\": 4,\n        \"pad_token_id\": 1,\n        \"do_sample\": false\n    }' | jq\n</code></pre></p> <pre><code>/usr/bin/curl -X POST localhost:3000/api/v1/summarize \\\n-H \"Content-Type: application/json\" \\\n-d '{\n        \"text\": \"Theres something magical about Recurrent Neural Networks (RNNs). I still remember when I trained my first recurrent network for Image Captioning. Within a few dozen minutes of training my first baby model (with rather arbitrarily-chosen hyperparameters) started to generate very nice looking descriptions of images that were on the edge of making sense. Sometimes the ratio of how simple your model is to the quality of the results you get out of it blows past your expectations, and this was one of those times. What made this result so shocking at the time was that the common wisdom was that RNNs were supposed to be difficult to train (with more experience Ive in fact reached the opposite conclusion). Fast forward about a year: Im training RNNs all the time and Ive witnessed their power and robustness many times, and yet their magical outputs still find ways of amusing me.\",\n        \"decoding_strategy\": \"generate\",\n        \"early_stopping\": true,\n        \"length_penalty\": 2.0,\n        \"max_length\": 142,\n        \"min_length\": 56,\n        \"no_repeat_ngram_size\": 3,\n        \"num_beams\": 4\n    }' | jq\n</code></pre>"},{"location":"text/api/summarization/#geniusrise_text.summarization.api.SummarizationAPI.summarize_pipeline","title":"<code>summarize_pipeline(**kwargs)</code>","text":"<p>Summarizes the input text using the Hugging Face pipeline based on given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Keyword arguments containing parameters for summarization.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary containing the input text and its summary.</p> <p>Example CURL Request for summarization: <code>curl -X POST localhost:3000/api/v1/summarize_pipeline -H \"Content-Type: application/json\" -d '{\"text\": \"Your long text here\"}'</code></p>"},{"location":"text/api/translation/","title":"Translation","text":"<p>             Bases: <code>TextAPI</code></p> <p>A class for serving a Hugging Face-based translation model as a web API. This API allows users to submit text for translation and receive translated text in the specified target language using advanced machine learning models.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>Configurations and data inputs for the batch process.</p> required <code>output</code> <code>BatchOutput</code> <p>Configurations for output data handling.</p> required <code>state</code> <code>State</code> <p>State management for the translation task.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for extended configurations.</p> <code>{}</code> <p>Example CLI Usage for interacting with the API:</p> <p>To start the API server: <pre><code>genius TranslationAPI rise \\\nbatch \\\n--input_folder ./input \\\nbatch \\\n--output_folder ./output \\\nnone \\\n--id facebook/mbart-large-50-many-to-many-mmt-lol \\\nlisten \\\n--args \\\nmodel_name=\"facebook/mbart-large-50-many-to-many-mmt\" \\\nmodel_class=\"AutoModelForSeq2SeqLM\" \\\ntokenizer_class=\"AutoTokenizer\" \\\nuse_cuda=True \\\nprecision=\"float\" \\\nquantization=0 \\\ndevice_map=\"cuda:0\" \\\nmax_memory=None \\\ntorchscript=False \\\nendpoint=\"*\" \\\nport=3000 \\\ncors_domain=\"http://localhost:3000\" \\\nusername=\"user\" \\\npassword=\"password\"\n</code></pre></p> <p>To translate text using the API: <pre><code>curl -X POST localhost:8080/translate \\\n-H \"Content-Type: application/json\" \\\n-d '{\n        \"text\": \"Hello, world!\",\n        \"source_lang\": \"en\",\n        \"target_lang\": \"fr\",\n        \"decoding_strategy\": \"beam_search\",\n        \"num_beams\": 5\n    }'\n</code></pre></p>"},{"location":"text/api/translation/#geniusrise_text.translation.api.TranslationAPI.initialize_pipeline","title":"<code>initialize_pipeline()</code>","text":"<p>Lazy initialization of the translation Hugging Face pipeline.</p>"},{"location":"text/api/translation/#geniusrise_text.translation.api.TranslationAPI.translate","title":"<code>translate(**kwargs)</code>","text":"<p>Translates text to a specified target language using the underlying Hugging Face model.</p> <p>This endpoint accepts JSON data with the text and language details, processes it through the machine learning model, and returns the translated text.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments, usually empty as parameters are in the POST body.</p> <code>{}</code> POST body parameters <p>text (str): The text to be translated. decoding_strategy (str): Strategy to use for decoding text; e.g., 'beam_search', 'greedy'. Default is 'generate'. source_lang (str): Source language code. target_lang (str): Target language code. Default is 'en'. additional_params (dict): Other model-specific parameters for translation.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary with the original text, target language, and translated text.</p> <p>Example CURL requests:</p> <p>To translate text from English to French: <pre><code>curl -X POST localhost:8080/translate \\\n-H \"Content-Type: application/json\" \\\n-d '{\n        \"text\": \"Hello, world!\",\n        \"source_lang\": \"en\",\n        \"target_lang\": \"fr\",\n        \"decoding_strategy\": \"beam_search\",\n        \"num_beams\": 5\n    }'\n</code></pre></p> <p>To translate text from Spanish to German: <pre><code>/usr/bin/curl -X POST localhost:3000/api/v1/translate \\\n-H \"Content-Type: application/json\" \\\n-d '{\n        \"text\": \"\u0938\u0902\u092f\u0941\u0915\u094d\u0924 \u0930\u093e\u0937\u094d\u091f\u094d\u0930 \u0915\u0947 \u092a\u094d\u0930\u092e\u0941\u0916 \u0915\u093e \u0915\u0939\u0928\u093e \u0939\u0948 \u0915\u093f \u0938\u0940\u0930\u093f\u092f\u093e \u092e\u0947\u0902 \u0915\u094b\u0908 \u0938\u0948\u0928\u094d\u092f \u0938\u092e\u093e\u0927\u093e\u0928 \u0928\u0939\u0940\u0902 \u0939\u0948\",\n        \"source_lang\": \"hi_IN\",\n        \"target_lang\": \"en_XX\",\n        \"decoding_strategy\": \"generate\",\n        \"decoder_start_token_id\": 2,\n        \"early_stopping\": true,\n        \"eos_token_id\": 2,\n        \"forced_eos_token_id\": 2,\n        \"max_length\": 200,\n        \"num_beams\": 5,\n        \"pad_token_id\": 1\n    }' | jq\n</code></pre></p>"},{"location":"text/api/translation/#geniusrise_text.translation.api.TranslationAPI.translate_pipeline","title":"<code>translate_pipeline(**kwargs)</code>","text":"<p>Endpoint for translating text using a pre-initialized Hugging Face translation pipeline. This method is designed to handle translation requests more efficiently by utilizing a preloaded model and tokenizer, reducing the overhead of loading these components for each request.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the original text, source language, target language,             and the translated text.</p> <p>Example CURL Request for translation: <pre><code>curl -X POST localhost:8080/translate_pipeline             -H \"Content-Type: application/json\"             -d '{\n        \"text\": \"Hello, world!\",\n        \"source_lang\": \"en\",\n        \"target_lang\": \"fr\"\n    }'\n</code></pre></p>"},{"location":"text/bulk/base/","title":"Base Fine Tuner","text":"<p>             Bases: <code>Bolt</code></p> <p>TextBulk is a foundational class for enabling bulk processing of text with various generation models. It primarily focuses on using Hugging Face models to provide a robust and efficient framework for large-scale text generation tasks. The class supports various decoding strategies to generate text that can be tailored to specific needs or preferences.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>AutoModelForCausalLM</code> <p>The language model for text generation.</p> <code>tokenizer</code> <code>AutoTokenizer</code> <p>The tokenizer for preparing input data for the model.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>Configuration and data inputs for the batch process.</p> required <code>output</code> <code>BatchOutput</code> <p>Configurations for output data handling.</p> required <code>state</code> <code>State</code> <p>State management for the Bolt.</p> required <code>**kwargs</code> <p>Arbitrary keyword arguments for extended configurations.</p> <code>{}</code> Methods <p>text(**kwargs: Any) -&gt; Dict[str, Any]:     Provides an API endpoint for text generation functionality.     Accepts various parameters for customizing the text generation process.</p> <p>generate(prompt: str, decoding_strategy: str = \"generate\", **generation_params: Any) -&gt; dict:     Generates text based on the provided prompt and parameters. Supports multiple decoding strategies for diverse applications.</p> <p>The class serves as a versatile tool for text generation, supporting various models and configurations. It can be extended or used as is for efficient text generation tasks.</p>"},{"location":"text/bulk/base/#geniusrise_text.base.bulk.TextBulk.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>Initializes the TextBulk with configurations and sets up logging. It prepares the environment for text generation tasks.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>The input data configuration for the text generation task.</p> required <code>output</code> <code>BatchOutput</code> <p>The output data configuration for the results of the text generation.</p> required <code>state</code> <code>State</code> <p>The state configuration for the Bolt, managing its operational status.</p> required <code>**kwargs</code> <p>Additional keyword arguments for extended functionality and model configurations.</p> <code>{}</code>"},{"location":"text/bulk/base/#geniusrise_text.base.bulk.TextBulk.generate","title":"<code>generate(prompt, decoding_strategy='generate', **generation_params)</code>","text":"<p>Generate text completion for the given prompt using the specified decoding strategy.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt to generate text completion for.</p> required <code>decoding_strategy</code> <code>str</code> <p>The decoding strategy to use. Defaults to \"generate\".</p> <code>'generate'</code> <code>**generation_params</code> <code>Any</code> <p>Additional parameters to pass to the decoding strategy.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The generated text completion.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If an error occurs during generation.</p> Supported decoding strategies and their additional parameters <ul> <li>\"generate\": Uses the model's default generation method. (Parameters: max_length, num_beams, etc.)</li> <li>\"greedy_search\": Generates text using a greedy search decoding strategy. Parameters: max_length, eos_token_id, pad_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus.</li> <li>\"contrastive_search\": Generates text using contrastive search decoding strategy. Parameters: top_k, penalty_alpha, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, sequential.</li> <li>\"sample\": Generates text using a sampling decoding strategy. Parameters: do_sample, temperature, top_k, top_p, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus.</li> <li>\"beam_search\": Generates text using beam search decoding strategy. Parameters: num_beams, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus.</li> <li>\"beam_sample\": Generates text using beam search with sampling decoding strategy. Parameters: num_beams, temperature, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus.</li> <li>\"group_beam_search\": Generates text using group beam search decoding strategy. Parameters: num_beams, diversity_penalty, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus.</li> <li>\"constrained_beam_search\": Generates text using constrained beam search decoding strategy. Parameters: num_beams, max_length, constraints, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus.</li> </ul> All generation parameters <ul> <li>max_length: Maximum length the generated tokens can have</li> <li>max_new_tokens: Maximum number of tokens to generate, ignoring prompt tokens</li> <li>min_length: Minimum length of the sequence to be generated</li> <li>min_new_tokens: Minimum number of tokens to generate, ignoring prompt tokens</li> <li>early_stopping: Stopping condition for beam-based methods</li> <li>max_time: Maximum time allowed for computation in seconds</li> <li>do_sample: Whether to use sampling for generation</li> <li>num_beams: Number of beams for beam search</li> <li>num_beam_groups: Number of groups for beam search to ensure diversity</li> <li>penalty_alpha: Balances model confidence and degeneration penalty in contrastive search</li> <li>use_cache: Whether the model should use past key/values attentions to speed up decoding</li> <li>temperature: Modulates next token probabilities</li> <li>top_k: Number of highest probability tokens to keep for top-k-filtering</li> <li>top_p: Smallest set of most probable tokens with cumulative probability &gt;= top_p</li> <li>typical_p: Conditional probability of predicting a target token next</li> <li>epsilon_cutoff: Tokens with a conditional probability &gt; epsilon_cutoff will be sampled</li> <li>eta_cutoff: Eta sampling, a hybrid of locally typical sampling and epsilon sampling</li> <li>diversity_penalty: Penalty subtracted from a beam's score if it generates a token same as any other group</li> <li>repetition_penalty: Penalty for repetition of ngrams</li> <li>encoder_repetition_penalty: Penalty on sequences not in the original input</li> <li>length_penalty: Exponential penalty to the length for beam-based generation</li> <li>no_repeat_ngram_size: All ngrams of this size can only occur once</li> <li>bad_words_ids: List of token ids that are not allowed to be generated</li> <li>force_words_ids: List of token ids that must be generated</li> <li>renormalize_logits: Renormalize the logits after applying all logits processors</li> <li>constraints: Custom constraints for generation</li> <li>forced_bos_token_id: Token ID to force as the first generated token</li> <li>forced_eos_token_id: Token ID to force as the last generated token</li> <li>remove_invalid_values: Remove possible NaN and inf outputs</li> <li>exponential_decay_length_penalty: Exponentially increasing length penalty after a certain number of tokens</li> <li>suppress_tokens: Tokens that will be suppressed during generation</li> <li>begin_suppress_tokens: Tokens that will be suppressed at the beginning of generation</li> <li>forced_decoder_ids: Mapping from generation indices to token indices that will be forced</li> <li>sequence_bias: Maps a sequence of tokens to its bias term</li> <li>guidance_scale: Guidance scale for classifier free guidance (CFG)</li> <li>low_memory: Switch to sequential topk for contrastive search to reduce peak memory</li> <li>num_return_sequences: Number of independently computed returned sequences for each batch element</li> <li>output_attentions: Whether to return the attentions tensors of all layers</li> <li>output_hidden_states: Whether to return the hidden states of all layers</li> <li>output_scores: Whether to return the prediction scores</li> <li>return_dict_in_generate: Whether to return a ModelOutput instead of a plain tuple</li> <li>pad_token_id: The id of the padding token</li> <li>bos_token_id: The id of the beginning-of-sequence token</li> <li>eos_token_id: The id of the end-of-sequence token</li> <li>max_length: The maximum length of the sequence to be generated</li> <li>eos_token_id: End-of-sequence token ID</li> <li>pad_token_id: Padding token ID</li> <li>output_attentions: Return attention tensors of all attention layers if True</li> <li>output_hidden_states: Return hidden states of all layers if True</li> <li>output_scores: Return prediction scores if True</li> <li>return_dict_in_generate: Return a ModelOutput instead of a plain tuple if True</li> <li>synced_gpus: Continue running the while loop until max_length for ZeRO stage 3 if True</li> <li>top_k: Size of the candidate set for re-ranking in contrastive search</li> <li>penalty_alpha: Degeneration penalty; active when larger than 0</li> <li>eos_token_id: End-of-sequence token ID(s)</li> <li>sequential: Switch to sequential topk hidden state computation to reduce memory if True</li> <li>do_sample: Use sampling for generation if True</li> <li>temperature: Temperature for sampling</li> <li>top_p: Cumulative probability for top-p-filtering</li> <li>diversity_penalty: Penalty for reducing similarity across different beam groups</li> <li>constraints: List of constraints to apply during beam search</li> <li>synced_gpus: Whether to continue running the while loop until max_length (needed for ZeRO stage 3)</li> </ul>"},{"location":"text/bulk/base/#geniusrise_text.base.bulk.TextBulk.load_models","title":"<code>load_models(model_name, tokenizer_name, model_revision=None, tokenizer_revision=None, model_class='AutoModelForCausalLM', tokenizer_class='AutoTokenizer', use_cuda=False, precision='float16', quantization=0, device_map='auto', max_memory={0: '24GB'}, torchscript=False, compile=False, awq_enabled=False, flash_attention=False, better_transformers=False, **model_args)</code>","text":"<p>Loads and configures the specified model and tokenizer for text generation. It ensures the models are optimized for inference.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name or path of the model to load.</p> required <code>tokenizer_name</code> <code>str</code> <p>The name or path of the tokenizer to load.</p> required <code>model_revision</code> <code>Optional[str]</code> <p>The specific model revision to load (e.g., a commit hash).</p> <code>None</code> <code>tokenizer_revision</code> <code>Optional[str]</code> <p>The specific tokenizer revision to load (e.g., a commit hash).</p> <code>None</code> <code>model_class</code> <code>str</code> <p>The class of the model to be loaded.</p> <code>'AutoModelForCausalLM'</code> <code>tokenizer_class</code> <code>str</code> <p>The class of the tokenizer to be loaded.</p> <code>'AutoTokenizer'</code> <code>use_cuda</code> <code>bool</code> <p>Flag to utilize CUDA for GPU acceleration.</p> <code>False</code> <code>precision</code> <code>str</code> <p>The desired precision for computations (\"float32\", \"float16\", etc.).</p> <code>'float16'</code> <code>quantization</code> <code>int</code> <p>The bit level for model quantization (0 for none, 8 for 8-bit quantization).</p> <code>0</code> <code>device_map</code> <code>str | Dict | None</code> <p>The specific device(s) to use for model operations.</p> <code>'auto'</code> <code>max_memory</code> <code>Dict</code> <p>A dictionary defining the maximum memory to allocate for the model.</p> <code>{0: '24GB'}</code> <code>torchscript</code> <code>bool</code> <p>Flag to enable TorchScript for model optimization.</p> <code>False</code> <code>compile</code> <code>bool</code> <p>Flag to enable JIT compilation of the model.</p> <code>False</code> <code>awq_enabled</code> <code>bool</code> <p>Flag to enable AWQ (Adaptive Weight Quantization).</p> <code>False</code> <code>flash_attention</code> <code>bool</code> <p>Flag to enable Flash Attention optimization for faster processing.</p> <code>False</code> <code>better_transformers</code> <code>bool</code> <p>Flag to enable Better Transformers optimization for faster processing.</p> <code>False</code> <code>**model_args</code> <code>Any</code> <p>Additional arguments to pass to the model during its loading.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[AutoModelForCausalLM, AutoTokenizer]</code> <p>Tuple[AutoModelForCausalLM, AutoTokenizer]: The loaded model and tokenizer ready for text generation.</p>"},{"location":"text/bulk/base/#geniusrise_text.base.bulk.TextBulk.load_models_llama_cpp","title":"<code>load_models_llama_cpp(model, filename, local_dir=None, n_gpu_layers=0, split_mode=llama_cpp.LLAMA_SPLIT_LAYER, main_gpu=0, tensor_split=None, vocab_only=False, use_mmap=True, use_mlock=False, kv_overrides=None, seed=llama_cpp.LLAMA_DEFAULT_SEED, n_ctx=512, n_batch=512, n_threads=None, n_threads_batch=None, rope_scaling_type=llama_cpp.LLAMA_ROPE_SCALING_UNSPECIFIED, rope_freq_base=0.0, rope_freq_scale=0.0, yarn_ext_factor=-1.0, yarn_attn_factor=1.0, yarn_beta_fast=32.0, yarn_beta_slow=1.0, yarn_orig_ctx=0, mul_mat_q=True, logits_all=False, embedding=False, offload_kqv=True, last_n_tokens_size=64, lora_base=None, lora_scale=1.0, lora_path=None, numa=False, chat_format=None, chat_handler=None, draft_model=None, tokenizer=None, verbose=True, **kwargs)</code>","text":"<p>Initializes and loads LLaMA model with llama.cpp backend, along with an optional tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Huggingface ID to the LLaMA model.</p> required <code>filename</code> <code>Optional[str]</code> <p>A filename or glob pattern to match the model file in the repo.</p> required <code>local_dir</code> <code>Optional[Union[str, os.PathLike[str]]]</code> <p>The local directory to save the model to.</p> <code>None</code> <code>n_gpu_layers</code> <code>int</code> <p>Number of layers to offload to GPU. Default is 0.</p> <code>0</code> <code>split_mode</code> <code>int</code> <p>Split mode for distributing model across GPUs.</p> <code>llama_cpp.LLAMA_SPLIT_LAYER</code> <code>main_gpu</code> <code>int</code> <p>Main GPU index.</p> <code>0</code> <code>tensor_split</code> <code>Optional[List[float]]</code> <p>Tensor split configuration.</p> <code>None</code> <code>vocab_only</code> <code>bool</code> <p>Whether to load vocabulary only.</p> <code>False</code> <code>use_mmap</code> <code>bool</code> <p>Use memory-mapped files for model loading.</p> <code>True</code> <code>use_mlock</code> <code>bool</code> <p>Lock model data in RAM.</p> <code>False</code> <code>kv_overrides</code> <code>Optional[Dict[str, Union[bool, int, float]]]</code> <p>Key-value pairs for model overrides.</p> <code>None</code> <code>seed</code> <code>int</code> <p>Random seed for initialization.</p> <code>llama_cpp.LLAMA_DEFAULT_SEED</code> <code>n_ctx</code> <code>int</code> <p>Number of context tokens.</p> <code>512</code> <code>n_batch</code> <code>int</code> <p>Batch size for processing prompts.</p> <code>512</code> <code>n_threads</code> <code>Optional[int]</code> <p>Number of threads for generation.</p> <code>None</code> <code>n_threads_batch</code> <code>Optional[int]</code> <p>Number of threads for batch processing.</p> <code>None</code> <code>rope_scaling_type</code> <code>Optional[int]</code> <p>RoPE scaling type.</p> <code>llama_cpp.LLAMA_ROPE_SCALING_UNSPECIFIED</code> <code>rope_freq_base</code> <code>float</code> <p>Base frequency for RoPE.</p> <code>0.0</code> <code>rope_freq_scale</code> <code>float</code> <p>Frequency scaling for RoPE.</p> <code>0.0</code> <code>yarn_ext_factor</code> <code>float</code> <p>YaRN extrapolation mix factor.</p> <code>-1.0</code> <code>yarn_attn_factor</code> <code>float</code> <p>YaRN attention factor.</p> <code>1.0</code> <code>yarn_beta_fast</code> <code>float</code> <p>YaRN beta fast parameter.</p> <code>32.0</code> <code>yarn_beta_slow</code> <code>float</code> <p>YaRN beta slow parameter.</p> <code>1.0</code> <code>yarn_orig_ctx</code> <code>int</code> <p>Original context size for YaRN.</p> <code>0</code> <code>mul_mat_q</code> <code>bool</code> <p>Whether to multiply matrices for queries.</p> <code>True</code> <code>logits_all</code> <code>bool</code> <p>Return logits for all tokens.</p> <code>False</code> <code>embedding</code> <code>bool</code> <p>Enable embedding mode only.</p> <code>False</code> <code>offload_kqv</code> <code>bool</code> <p>Offload K, Q, V matrices to GPU.</p> <code>True</code> <code>last_n_tokens_size</code> <code>int</code> <p>Size for the last_n_tokens buffer.</p> <code>64</code> <code>lora_base</code> <code>Optional[str]</code> <p>Base model path for LoRA.</p> <code>None</code> <code>lora_scale</code> <code>float</code> <p>Scale factor for LoRA adjustments.</p> <code>1.0</code> <code>lora_path</code> <code>Optional[str]</code> <p>Path to LoRA adjustments.</p> <code>None</code> <code>numa</code> <code>Union[bool, int]</code> <p>NUMA configuration.</p> <code>False</code> <code>chat_format</code> <code>Optional[str]</code> <p>Chat format configuration.</p> <code>None</code> <code>chat_handler</code> <code>Optional[llama_cpp.LlamaChatCompletionHandler]</code> <p>Handler for chat completions.</p> <code>None</code> <code>draft_model</code> <code>Optional[llama_cpp.LlamaDraftModel]</code> <p>Draft model for speculative decoding.</p> <code>None</code> <code>tokenizer</code> <code>Optional[PreTrainedTokenizerBase]</code> <p>Custom tokenizer instance.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Enable verbose logging.</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[LlamaCPP, Optional[PreTrainedTokenizerBase]]</code> <p>Tuple[LlamaCPP, Optional[PreTrainedTokenizerBase]]: The loaded LLaMA model and tokenizer.</p>"},{"location":"text/bulk/base/#geniusrise_text.base.bulk.TextBulk.load_models_vllm","title":"<code>load_models_vllm(model, tokenizer, tokenizer_mode='auto', trust_remote_code=True, download_dir=None, load_format='auto', dtype='auto', seed=42, revision=None, tokenizer_revision=None, max_model_len=1024, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, block_size=16, gpu_memory_utilization=0.9, swap_space=4, cache_dtype='auto', sliding_window=None, pipeline_parallel_size=1, tensor_parallel_size=1, worker_use_ray=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, max_num_batched_tokens=None, max_num_seqs=64, max_paddings=512, device='cuda', max_lora_rank=None, max_loras=None, max_cpu_loras=None, lora_dtype=None, lora_extra_vocab_size=0, placement_group=None, log_stats=False, batched_inference=False)</code>","text":"<p>Initializes and loads models using VLLM configurations with specific parameters.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Name or path of the Hugging Face model to use.</p> required <code>tokenizer</code> <code>str</code> <p>Name or path of the Hugging Face tokenizer to use.</p> required <code>tokenizer_mode</code> <code>str</code> <p>Tokenizer mode. \"auto\" will use the fast tokenizer if available, \"slow\" will always use the slow tokenizer.</p> <code>'auto'</code> <code>trust_remote_code</code> <code>bool</code> <p>Trust remote code (e.g., from Hugging Face) when downloading the model and tokenizer.</p> <code>True</code> <code>download_dir</code> <code>Optional[str]</code> <p>Directory to download and load the weights, default to the default cache directory of Hugging Face.</p> <code>None</code> <code>load_format</code> <code>str</code> <p>The format of the model weights to load. Options include \"auto\", \"pt\", \"safetensors\", \"npcache\", \"dummy\".</p> <code>'auto'</code> <code>dtype</code> <code>Union[str, torch.dtype]</code> <p>Data type for model weights and activations. Options include \"auto\", torch.float32, torch.float16, etc.</p> <code>'auto'</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility.</p> <code>42</code> <code>revision</code> <code>Optional[str]</code> <p>The specific model version to use. Can be a branch name, a tag name, or a commit id.</p> <code>None</code> <code>code_revision</code> <code>Optional[str]</code> <p>The specific revision to use for the model code on Hugging Face Hub.</p> required <code>tokenizer_revision</code> <code>Optional[str]</code> <p>The specific tokenizer version to use.</p> <code>None</code> <code>max_model_len</code> <code>Optional[int]</code> <p>Maximum length of a sequence (including prompt and output). If None, will be derived from the model.</p> <code>1024</code> <code>quantization</code> <code>Optional[str]</code> <p>Quantization method that was used to quantize the model weights. If None, we assume the model weights are not quantized.</p> <code>None</code> <code>enforce_eager</code> <code>bool</code> <p>Whether to enforce eager execution. If True, disables CUDA graph and always execute the model in eager mode.</p> <code>False</code> <code>max_context_len_to_capture</code> <code>Optional[int]</code> <p>Maximum context length covered by CUDA graphs. When larger, falls back to eager mode.</p> <code>8192</code> <code>block_size</code> <code>int</code> <p>Size of a cache block in number of tokens.</p> <code>16</code> <code>gpu_memory_utilization</code> <code>float</code> <p>Fraction of GPU memory to use for the VLLM execution.</p> <code>0.9</code> <code>swap_space</code> <code>int</code> <p>Size of the CPU swap space per GPU (in GiB).</p> <code>4</code> <code>cache_dtype</code> <code>str</code> <p>Data type for KV cache storage.</p> <code>'auto'</code> <code>sliding_window</code> <code>Optional[int]</code> <p>Configuration for sliding window if applicable.</p> <code>None</code> <code>pipeline_parallel_size</code> <code>int</code> <p>Number of pipeline parallel groups.</p> <code>1</code> <code>tensor_parallel_size</code> <code>int</code> <p>Number of tensor parallel groups.</p> <code>1</code> <code>worker_use_ray</code> <code>bool</code> <p>Whether to use Ray for model workers. Required if either pipeline_parallel_size or tensor_parallel_size is greater than 1.</p> <code>False</code> <code>max_parallel_loading_workers</code> <code>Optional[int]</code> <p>Maximum number of workers for loading the model in parallel to avoid RAM OOM.</p> <code>None</code> <code>disable_custom_all_reduce</code> <code>bool</code> <p>Disable custom all-reduce kernel and fall back to NCCL.</p> <code>False</code> <code>max_num_batched_tokens</code> <code>Optional[int]</code> <p>Maximum number of tokens to be processed in a single iteration.</p> <code>None</code> <code>max_num_seqs</code> <code>int</code> <p>Maximum number of sequences to be processed in a single iteration.</p> <code>64</code> <code>max_paddings</code> <code>int</code> <p>Maximum number of paddings to be added to a batch.</p> <code>512</code> <code>device</code> <code>str</code> <p>Device configuration, typically \"cuda\" or \"cpu\".</p> <code>'cuda'</code> <code>max_lora_rank</code> <code>Optional[int]</code> <p>Maximum rank for LoRA adjustments.</p> <code>None</code> <code>max_loras</code> <code>Optional[int]</code> <p>Maximum number of LoRA adjustments.</p> <code>None</code> <code>max_cpu_loras</code> <code>Optional[int]</code> <p>Maximum number of LoRA adjustments stored on CPU.</p> <code>None</code> <code>lora_dtype</code> <code>Optional[torch.dtype]</code> <p>Data type for LoRA parameters.</p> <code>None</code> <code>lora_extra_vocab_size</code> <code>Optional[int]</code> <p>Additional vocabulary size for LoRA.</p> <code>0</code> <code>placement_group</code> <code>Optional[PlacementGroup]</code> <p>Ray placement group for distributed execution. Required for distributed execution.</p> <code>None</code> <code>log_stats</code> <code>bool</code> <p>Whether to log statistics during model operation.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>LLMEngine</code> <code>AsyncLLMEngine | LLM</code> <p>An instance of the LLMEngine class initialized with the given configurations.</p>"},{"location":"text/bulk/classification/","title":"Classification","text":"<p>             Bases: <code>TextBulk</code></p> <p>TextClassificationBulk is designed to handle bulk text classification tasks using Hugging Face models efficiently and effectively. It allows for processing large datasets, utilizing state-of-the-art machine learning models to provide accurate classification of text data into predefined labels.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>Configuration and data inputs for the batch process.</p> required <code>output</code> <code>BatchOutput</code> <p>Configurations for output data handling.</p> required <code>state</code> <code>State</code> <p>State management for the classification task.</p> required <code>**kwargs</code> <p>Arbitrary keyword arguments for extended configurations.</p> <code>{}</code> <p>Example CLI Usage: <pre><code>genius TextClassificationBulk rise \\\nbatch \\\n--input_folder ./input \\\nbatch \\\n--output_folder ./output \\\nnone \\\n--id cardiffnlp/twitter-roberta-base-hate-multiclass-latest-lol \\\nclassify \\\n--args \\\nmodel_name=\"cardiffnlp/twitter-roberta-base-hate-multiclass-latest\" \\\nmodel_class=\"AutoModelForSequenceClassification\" \\\ntokenizer_class=\"AutoTokenizer\" \\\nuse_cuda=True \\\nprecision=\"bfloat16\" \\\nquantization=0 \\\ndevice_map=\"auto\" \\\nmax_memory=None \\\ntorchscript=False\n</code></pre></p>"},{"location":"text/bulk/classification/#geniusrise_text.classification.bulk.TextClassificationBulk.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>Initializes the TextClassificationBulk class with input, output, and state configurations.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>Configuration for the input data.</p> required <code>output</code> <code>BatchOutput</code> <p>Configuration for the output data.</p> required <code>state</code> <code>State</code> <p>State management for the classification task.</p> required <code>**kwargs</code> <p>Additional keyword arguments for extended functionality.</p> <code>{}</code>"},{"location":"text/bulk/classification/#geniusrise_text.classification.bulk.TextClassificationBulk.classify","title":"<code>classify(model_name, model_class='AutoModelForSequenceClassification', tokenizer_class='AutoTokenizer', use_cuda=False, precision='float', quantization=0, device_map='auto', max_memory={0: '24GB'}, torchscript=False, compile=False, awq_enabled=False, flash_attention=False, batch_size=32, notification_email=None, **kwargs)</code>","text":"<p>Perform bulk classification using the specified model and tokenizer. This method handles the entire classification process including loading the model, processing input data, predicting classifications, and saving the results.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name or path of the model.</p> required <code>model_class</code> <code>str</code> <p>Class name of the model (default \"AutoModelForSequenceClassification\").</p> <code>'AutoModelForSequenceClassification'</code> <code>tokenizer_class</code> <code>str</code> <p>Class name of the tokenizer (default \"AutoTokenizer\").</p> <code>'AutoTokenizer'</code> <code>use_cuda</code> <code>bool</code> <p>Whether to use CUDA for model inference (default False).</p> <code>False</code> <code>precision</code> <code>str</code> <p>Precision for model computation (default \"float\").</p> <code>'float'</code> <code>quantization</code> <code>int</code> <p>Level of quantization for optimizing model size and speed (default 0).</p> <code>0</code> <code>device_map</code> <code>str | Dict | None</code> <p>Specific device to use for computation (default \"auto\").</p> <code>'auto'</code> <code>max_memory</code> <code>Dict</code> <p>Maximum memory configuration for devices.</p> <code>{0: '24GB'}</code> <code>torchscript</code> <code>bool</code> <p>Whether to use a TorchScript-optimized version of the pre-trained language model. Defaults to False.</p> <code>False</code> <code>compile</code> <code>bool</code> <p>Whether to compile the model before fine-tuning. Defaults to True.</p> <code>False</code> <code>awq_enabled</code> <code>bool</code> <p>Whether to enable AWQ optimization (default False).</p> <code>False</code> <code>flash_attention</code> <code>bool</code> <p>Whether to use flash attention optimization (default False).</p> <code>False</code> <code>batch_size</code> <code>int</code> <p>Number of classifications to process simultaneously (default 32).</p> <code>32</code> <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments for model and generation configurations.</p> <code>{}</code>"},{"location":"text/bulk/classification/#geniusrise_text.classification.bulk.TextClassificationBulk.load_dataset","title":"<code>load_dataset(dataset_path, max_length=512, **kwargs)</code>","text":"<p>Load a classification dataset from a directory.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the dataset directory.</p> required <code>max_length</code> <code>int</code> <p>The maximum length for tokenization. Defaults to 512.</p> <code>512</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Optional[Dataset]</code> <p>The loaded dataset.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there was an error loading the dataset.</p>"},{"location":"text/bulk/classification/#geniusrise_text.classification.bulk.TextClassificationBulk.load_dataset--supported-data-formats-and-structures","title":"Supported Data Formats and Structures:","text":""},{"location":"text/bulk/classification/#geniusrise_text.classification.bulk.TextClassificationBulk.load_dataset--jsonl","title":"JSONL","text":"<p>Each line is a JSON object representing an example. <pre><code>{\"text\": \"The text content\"}\n</code></pre></p>"},{"location":"text/bulk/classification/#geniusrise_text.classification.bulk.TextClassificationBulk.load_dataset--csv","title":"CSV","text":"<p>Should contain 'text' columns. <pre><code>text\n\"The text content\"\n</code></pre></p>"},{"location":"text/bulk/classification/#geniusrise_text.classification.bulk.TextClassificationBulk.load_dataset--parquet","title":"Parquet","text":"<p>Should contain 'text' columns.</p>"},{"location":"text/bulk/classification/#geniusrise_text.classification.bulk.TextClassificationBulk.load_dataset--json","title":"JSON","text":"<p>An array of dictionaries with 'text' keys. <pre><code>[{\"text\": \"The text content\"}]\n</code></pre></p>"},{"location":"text/bulk/classification/#geniusrise_text.classification.bulk.TextClassificationBulk.load_dataset--xml","title":"XML","text":"<p>Each 'record' element should contain 'text' child elements. <pre><code>&lt;record&gt;\n&lt;text&gt;The text content&lt;/text&gt;\n&lt;/record&gt;\n</code></pre></p>"},{"location":"text/bulk/classification/#geniusrise_text.classification.bulk.TextClassificationBulk.load_dataset--yaml","title":"YAML","text":"<p>Each document should be a dictionary with 'text' keys. <pre><code>- text: \"The text content\"\n</code></pre></p>"},{"location":"text/bulk/classification/#geniusrise_text.classification.bulk.TextClassificationBulk.load_dataset--tsv","title":"TSV","text":"<p>Should contain 'text' columns separated by tabs.</p>"},{"location":"text/bulk/classification/#geniusrise_text.classification.bulk.TextClassificationBulk.load_dataset--excel-xls-xlsx","title":"Excel (.xls, .xlsx)","text":"<p>Should contain 'text' columns.</p>"},{"location":"text/bulk/classification/#geniusrise_text.classification.bulk.TextClassificationBulk.load_dataset--sqlite-db","title":"SQLite (.db)","text":"<p>Should contain a table with 'text' columns.</p>"},{"location":"text/bulk/classification/#geniusrise_text.classification.bulk.TextClassificationBulk.load_dataset--feather","title":"Feather","text":"<p>Should contain 'text' columns.</p>"},{"location":"text/bulk/instruction_tuning/","title":"Instruction Tuning","text":"<p>             Bases: <code>TextBulk</code></p> <p>InstructionBulk is a class designed to perform bulk text generation tasks using Hugging Face's instruction-tuned language models. It is optimized for large-scale text generation, providing an efficient interface to use state-of-the-art machine learning models for generating text based on a set of instructions or prompts.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>Any</code> <p>The loaded, pre-trained instruction-tuned language model.</p> <code>tokenizer</code> <code>Any</code> <p>The tokenizer for processing text compatible with the model.</p> Methods <p>load_dataset(dataset_path: str, max_length: int = 1024, **kwargs) -&gt; Optional[Dataset]:     Loads a dataset for text generation tasks from the specified directory.</p> <p>perform(model_name: str, **kwargs: Any) -&gt; None:     Performs bulk text generation using the specified model and tokenizer.</p> <p>Example CLI Usage: <pre><code>genius InstructionBulk rise \\\nbatch \\\n--input_s3_bucket geniusrise-test \\\n--input_s3_folder input/chat \\\nbatch \\\n--output_s3_bucket geniusrise-test \\\n--output_s3_folder output/chat \\\npostgres \\\n--postgres_host 127.0.0.1 \\\n--postgres_port 5432 \\\n--postgres_user postgres \\\n--postgres_password postgres \\\n--postgres_database geniusrise\\\n--postgres_table state \\\n--id mistralai/Mistral-7B-Instruct-v0.1-lol \\\nperform \\\n--args \\\nmodel_name=\"mistralai/Mistral-7B-Instruct-v0.1\" \\\nmodel_class=\"AutoModelForCausalLM\" \\\ntokenizer_class=\"AutoTokenizer\" \\\nuse_cuda=True \\\nprecision=\"bfloat16\" \\\nquantization=0 \\\ndevice_map=\"auto\" \\\nmax_memory=None \\\ntorchscript=False \\\ndecoding_strategy=\"generate\" \\\ngeneration_max_new_tokens=100 \\\ngeneration_do_sample=true\n</code></pre></p> <p>or using VLLM: <pre><code>genius InstructionBulk rise \\\nbatch \\\n--input_s3_bucket geniusrise-test \\\n--input_s3_folder input/chat \\\nbatch \\\n--output_s3_bucket geniusrise-test \\\n--output_s3_folder output/chat \\\nnone \\\n--id mistralai/Mistral-7B-Instruct-v0.1 \\\nperform_vllm \\\n--args \\\nmodel_name=\"mistralai/Mistral-7B-Instruct-v0.1\" \\\nuse_cuda=True \\\nprecision=\"bfloat16\" \\\nquantization=0 \\\ndevice_map=\"auto\" \\\ngeneration_temperature=0.7 \\\ngeneration_top_p=1.0 \\\ngeneration_n=1 \\\ngeneration_max_tokens=50 \\\ngeneration_stream=false \\\ngeneration_presence_penalty=0.0 \\\ngeneration_frequency_penalty=0.0\n</code></pre></p> <p>or using llama.cpp: <pre><code>genius InstructionBulk rise \\\nbatch \\\n--input_s3_bucket geniusrise-test \\\n--input_s3_folder input/chat \\\nbatch \\\n--output_s3_bucket geniusrise-test \\\n--output_s3_folder output/chat \\\nnone \\\n--id mistralai/Mistral-7B-Instruct-v0.1 \\\nperform_llama_cpp \\\n--args \\\nmodel=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\" \\\nfilename=\"mistral-7b-instruct-v0.2.Q4_K_M.gguf\" \\\nn_gpu_layers=35  \\\ngeneration_temperature=0.7 \\\ngeneration_top_p=0.95 \\\ngeneration_top_k=40 \\\ngeneration_max_tokens=50 \\\ngeneration_repeat_penalty=0.1\n</code></pre></p>"},{"location":"text/bulk/instruction_tuning/#geniusrise_text.instruction.bulk.InstructionBulk.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>Initializes the InstructionBulk class with input, output, and state configurations for bulk text generation.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>Configuration for input data handling.</p> required <code>output</code> <code>BatchOutput</code> <p>Configuration for output data handling.</p> required <code>state</code> <code>State</code> <p>State management for the text generation task.</p> required <code>**kwargs</code> <p>Additional keyword arguments for extended functionalities.</p> <code>{}</code>"},{"location":"text/bulk/instruction_tuning/#geniusrise_text.instruction.bulk.InstructionBulk.load_dataset","title":"<code>load_dataset(dataset_path, max_length=1024, **kwargs)</code>","text":"<p>Loads a dataset from the specified path. This method supports various data formats including JSON, CSV, Parquet, and others. It's designed to facilitate the bulk processing of text data for generation tasks.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>Path to the directory containing the dataset files.</p> required <code>max_length</code> <code>int</code> <p>Maximum token length for text processing (default is 1024).</p> <code>1024</code> <code>**kwargs</code> <p>Additional keyword arguments for dataset loading.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[Dataset]</code> <p>Optional[Dataset]: A Dataset object if loading is successful; otherwise, None.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If an error occurs during dataset loading.</p>"},{"location":"text/bulk/instruction_tuning/#geniusrise_text.instruction.bulk.InstructionBulk.load_dataset--supported-data-formats-and-structures","title":"Supported Data Formats and Structures:","text":""},{"location":"text/bulk/instruction_tuning/#geniusrise_text.instruction.bulk.InstructionBulk.load_dataset--jsonl","title":"JSONL","text":"<p>Each line is a JSON object representing an example. <pre><code>{\"instruction\": \"The instruction\"}\n</code></pre></p>"},{"location":"text/bulk/instruction_tuning/#geniusrise_text.instruction.bulk.InstructionBulk.load_dataset--csv","title":"CSV","text":"<p>Should contain 'instruction' columns. <pre><code>instruction\n\"The instruction\"\n</code></pre></p>"},{"location":"text/bulk/instruction_tuning/#geniusrise_text.instruction.bulk.InstructionBulk.load_dataset--parquet","title":"Parquet","text":"<p>Should contain 'instruction' columns.</p>"},{"location":"text/bulk/instruction_tuning/#geniusrise_text.instruction.bulk.InstructionBulk.load_dataset--json","title":"JSON","text":"<p>An array of dictionaries with 'instruction' keys. <pre><code>[{\"instruction\": \"The instruction\"}]\n</code></pre></p>"},{"location":"text/bulk/instruction_tuning/#geniusrise_text.instruction.bulk.InstructionBulk.load_dataset--xml","title":"XML","text":"<p>Each 'record' element should contain 'instruction' child elements. <pre><code>&lt;record&gt;\n&lt;instruction&gt;The instruction&lt;/instruction&gt;\n&lt;/record&gt;\n</code></pre></p>"},{"location":"text/bulk/instruction_tuning/#geniusrise_text.instruction.bulk.InstructionBulk.load_dataset--yaml","title":"YAML","text":"<p>Each document should be a dictionary with 'instruction' keys. <pre><code>- instruction: \"The instruction\"\n</code></pre></p>"},{"location":"text/bulk/instruction_tuning/#geniusrise_text.instruction.bulk.InstructionBulk.load_dataset--tsv","title":"TSV","text":"<p>Should contain 'instruction' columns separated by tabs.</p>"},{"location":"text/bulk/instruction_tuning/#geniusrise_text.instruction.bulk.InstructionBulk.load_dataset--excel-xls-xlsx","title":"Excel (.xls, .xlsx)","text":"<p>Should contain 'instruction' columns.</p>"},{"location":"text/bulk/instruction_tuning/#geniusrise_text.instruction.bulk.InstructionBulk.load_dataset--sqlite-db","title":"SQLite (.db)","text":"<p>Should contain a table with 'instruction' columns.</p>"},{"location":"text/bulk/instruction_tuning/#geniusrise_text.instruction.bulk.InstructionBulk.load_dataset--feather","title":"Feather","text":"<p>Should contain 'instruction' columns.</p>"},{"location":"text/bulk/instruction_tuning/#geniusrise_text.instruction.bulk.InstructionBulk.perform","title":"<code>perform(model_name, model_class='AutoModelForCausalLM', tokenizer_class='AutoTokenizer', use_cuda=False, precision='float16', quantization=0, device_map='auto', max_memory={0: '24GB'}, torchscript=False, compile=False, awq_enabled=False, flash_attention=False, decoding_strategy='generate', notification_email=None, **kwargs)</code>","text":"<p>Performs text generation in bulk using a specified instruction-tuned model. This method handles the entire process, including model loading, prompt processing, text generation, and saving the results.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name or path of the instruction-tuned model.</p> required <code>model_class</code> <code>str</code> <p>The class of the language model. Defaults to \"AutoModelForCausalLM\".</p> <code>'AutoModelForCausalLM'</code> <code>tokenizer_class</code> <code>str</code> <p>The class of the tokenizer. Defaults to \"AutoTokenizer\".</p> <code>'AutoTokenizer'</code> <code>use_cuda</code> <code>bool</code> <p>Whether to use CUDA for model inference. Defaults to False.</p> <code>False</code> <code>precision</code> <code>str</code> <p>Precision for model computation. Defaults to \"float16\".</p> <code>'float16'</code> <code>quantization</code> <code>int</code> <p>Level of quantization for optimizing model size and speed. Defaults to 0.</p> <code>0</code> <code>device_map</code> <code>str | Dict | None</code> <p>Specific device to use for computation. Defaults to \"auto\".</p> <code>'auto'</code> <code>max_memory</code> <code>Dict</code> <p>Maximum memory configuration for devices. Defaults to {0: \"24GB\"}.</p> <code>{0: '24GB'}</code> <code>torchscript</code> <code>bool</code> <p>Whether to use a TorchScript-optimized version of the pre-trained language model. Defaults to False.</p> <code>False</code> <code>compile</code> <code>bool</code> <p>Whether to compile the model before fine-tuning. Defaults to True.</p> <code>False</code> <code>awq_enabled</code> <code>bool</code> <p>Whether to enable AWQ optimization. Defaults to False.</p> <code>False</code> <code>flash_attention</code> <code>bool</code> <p>Whether to use flash attention optimization. Defaults to False.</p> <code>False</code> <code>decoding_strategy</code> <code>str</code> <p>Strategy for decoding the completion. Defaults to \"generate\".</p> <code>'generate'</code> <code>**kwargs</code> <code>Any</code> <p>Configuration and additional arguments for text generation such as model class, tokenizer class,       precision, device map, and other generation-related parameters.</p> <code>{}</code> Note <p>Additional arguments are passed directly to the model and tokenizer initialization and the generation method.</p>"},{"location":"text/bulk/instruction_tuning/#geniusrise_text.instruction.bulk.InstructionBulk.perform_llama_cpp","title":"<code>perform_llama_cpp(model, filename=None, local_dir=None, n_gpu_layers=0, split_mode=llama_cpp.LLAMA_SPLIT_LAYER, main_gpu=0, tensor_split=None, vocab_only=False, use_mmap=True, use_mlock=False, kv_overrides=None, seed=llama_cpp.LLAMA_DEFAULT_SEED, n_ctx=512, n_batch=512, n_threads=None, n_threads_batch=None, rope_scaling_type=llama_cpp.LLAMA_ROPE_SCALING_UNSPECIFIED, rope_freq_base=0.0, rope_freq_scale=0.0, yarn_ext_factor=-1.0, yarn_attn_factor=1.0, yarn_beta_fast=32.0, yarn_beta_slow=1.0, yarn_orig_ctx=0, mul_mat_q=True, logits_all=False, embedding=False, offload_kqv=True, last_n_tokens_size=64, lora_base=None, lora_scale=1.0, lora_path=None, numa=False, chat_format=None, chat_handler=None, draft_model=None, tokenizer=None, verbose=True, notification_email=None, **kwargs)</code>","text":"<p>Performs bulk text generation using the LLaMA model with llama.cpp backend. This method handles the entire process, including model loading, prompt processing, text generation, and saving the results.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Path or identifier for the LLaMA model.</p> required <code>filename</code> <code>Optional[str]</code> <p>Optional filename or glob pattern to match the model file.</p> <code>None</code> <code>local_dir</code> <code>Optional[Union[str, os.PathLike[str]]]</code> <p>Local directory to save the model files.</p> <code>None</code> <code>n_gpu_layers</code> <code>int</code> <p>Number of layers to offload to GPU.</p> <code>0</code> <code>split_mode</code> <code>int</code> <p>Split mode for distributing model across GPUs.</p> <code>llama_cpp.LLAMA_SPLIT_LAYER</code> <code>main_gpu</code> <code>int</code> <p>Main GPU index.</p> <code>0</code> <code>tensor_split</code> <code>Optional[List[float]]</code> <p>Configuration for tensor splitting across GPUs.</p> <code>None</code> <code>vocab_only</code> <code>bool</code> <p>Whether to load only the vocabulary.</p> <code>False</code> <code>use_mmap</code> <code>bool</code> <p>Use memory-mapped files for model loading.</p> <code>True</code> <code>use_mlock</code> <code>bool</code> <p>Lock model data in RAM to prevent swapping.</p> <code>False</code> <code>kv_overrides</code> <code>Optional[Dict[str, Union[bool, int, float]]]</code> <p>Key-value pairs for overriding model config.</p> <code>None</code> <code>seed</code> <code>int</code> <p>Seed for random number generation.</p> <code>llama_cpp.LLAMA_DEFAULT_SEED</code> <code>n_ctx</code> <code>int</code> <p>Number of context tokens for generation.</p> <code>512</code> <code>n_batch</code> <code>int</code> <p>Batch size for processing.</p> <code>512</code> <code>n_threads</code> <code>Optional[int]</code> <p>Number of threads for generation.</p> <code>None</code> <code>n_threads_batch</code> <code>Optional[int]</code> <p>Number of threads for batch processing.</p> <code>None</code> <code>rope_scaling_type</code> <code>Optional[int]</code> <p>Scaling type for RoPE.</p> <code>llama_cpp.LLAMA_ROPE_SCALING_UNSPECIFIED</code> <code>rope_freq_base</code> <code>float</code> <p>Base frequency for RoPE.</p> <code>0.0</code> <code>rope_freq_scale</code> <code>float</code> <p>Frequency scaling for RoPE.</p> <code>0.0</code> <code>yarn_ext_factor</code> <code>float</code> <p>YaRN extrapolation factor.</p> <code>-1.0</code> <code>yarn_attn_factor</code> <code>float</code> <p>YaRN attention factor.</p> <code>1.0</code> <code>yarn_beta_fast</code> <code>float</code> <p>YaRN beta fast parameter.</p> <code>32.0</code> <code>yarn_beta_slow</code> <code>float</code> <p>YaRN beta slow parameter.</p> <code>1.0</code> <code>yarn_orig_ctx</code> <code>int</code> <p>Original context size for YaRN.</p> <code>0</code> <code>mul_mat_q</code> <code>bool</code> <p>Multiply matrices for queries.</p> <code>True</code> <code>logits_all</code> <code>bool</code> <p>Return logits for all tokens.</p> <code>False</code> <code>embedding</code> <code>bool</code> <p>Enable embedding mode.</p> <code>False</code> <code>offload_kqv</code> <code>bool</code> <p>Offload K, Q, V matrices to GPU.</p> <code>True</code> <code>last_n_tokens_size</code> <code>int</code> <p>Size for the last_n_tokens buffer.</p> <code>64</code> <code>lora_base</code> <code>Optional[str]</code> <p>Base model path for LoRA.</p> <code>None</code> <code>lora_scale</code> <code>float</code> <p>Scale factor for LoRA adjustments.</p> <code>1.0</code> <code>lora_path</code> <code>Optional[str]</code> <p>Path for LoRA adjustments.</p> <code>None</code> <code>numa</code> <code>Union[bool, int]</code> <p>NUMA configuration.</p> <code>False</code> <code>chat_format</code> <code>Optional[str]</code> <p>Chat format configuration.</p> <code>None</code> <code>chat_handler</code> <code>Optional[llama_cpp.llama_chat_format.LlamaChatCompletionHandler]</code> <p>Handler for chat completions.</p> <code>None</code> <code>draft_model</code> <code>Optional[llama_cpp.LlamaDraftModel]</code> <p>Draft model for speculative decoding.</p> <code>None</code> <code>tokenizer</code> <code>Optional[PreTrainedTokenizerBase]</code> <p>Custom tokenizer instance.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Enable verbose logging.</p> <code>True</code> <code>notification_email</code> <code>Optional[str]</code> <p>Email to send notifications upon completion.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments for model loading and text generation.</p> <code>{}</code>"},{"location":"text/bulk/instruction_tuning/#geniusrise_text.instruction.bulk.InstructionBulk.perform_vllm","title":"<code>perform_vllm(model_name, use_cuda=False, precision='float16', quantization=0, device_map='auto', vllm_tokenizer_mode='auto', vllm_download_dir=None, vllm_load_format='auto', vllm_seed=42, vllm_max_model_len=1024, vllm_enforce_eager=False, vllm_max_context_len_to_capture=8192, vllm_block_size=16, vllm_gpu_memory_utilization=0.9, vllm_swap_space=4, vllm_sliding_window=None, vllm_pipeline_parallel_size=1, vllm_tensor_parallel_size=1, vllm_worker_use_ray=False, vllm_max_parallel_loading_workers=None, vllm_disable_custom_all_reduce=False, vllm_max_num_batched_tokens=None, vllm_max_num_seqs=64, vllm_max_paddings=512, vllm_max_lora_rank=None, vllm_max_loras=None, vllm_max_cpu_loras=None, vllm_lora_extra_vocab_size=0, vllm_placement_group=None, vllm_log_stats=False, notification_email=None, batch_size=32, **kwargs)</code>","text":"<p>Performs bulk text generation using the Versatile Language Learning Model (VLLM) with specified parameters for fine-tuning model behavior, including quantization and parallel processing settings. This method is designed to process large datasets efficiently by leveraging VLLM capabilities for generating high-quality text completions based on provided prompts.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name or path of the VLLM model to use for text generation.</p> required <code>use_cuda</code> <code>bool</code> <p>Flag indicating whether to use CUDA for GPU acceleration.</p> <code>False</code> <code>precision</code> <code>str</code> <p>Precision of computations, can be \"float16\", \"bfloat16\", etc.</p> <code>'float16'</code> <code>quantization</code> <code>int</code> <p>Level of quantization for model weights, 0 for none.</p> <code>0</code> <code>device_map</code> <code>str | Dict | None</code> <p>Specific device(s) to use for model inference.</p> <code>'auto'</code> <code>vllm_tokenizer_mode</code> <code>str</code> <p>Mode of the tokenizer (\"auto\", \"fast\", or \"slow\").</p> <code>'auto'</code> <code>vllm_download_dir</code> <code>Optional[str]</code> <p>Directory to download and load the model and tokenizer.</p> <code>None</code> <code>vllm_load_format</code> <code>str</code> <p>Format to load the model, e.g., \"auto\", \"pt\".</p> <code>'auto'</code> <code>vllm_seed</code> <code>int</code> <p>Seed for random number generation.</p> <code>42</code> <code>vllm_max_model_len</code> <code>int</code> <p>Maximum sequence length the model can handle.</p> <code>1024</code> <code>vllm_enforce_eager</code> <code>bool</code> <p>Enforce eager execution instead of using optimization techniques.</p> <code>False</code> <code>vllm_max_context_len_to_capture</code> <code>int</code> <p>Maximum context length for CUDA graph capture.</p> <code>8192</code> <code>vllm_block_size</code> <code>int</code> <p>Block size for caching mechanism.</p> <code>16</code> <code>vllm_gpu_memory_utilization</code> <code>float</code> <p>Fraction of GPU memory to use.</p> <code>0.9</code> <code>vllm_swap_space</code> <code>int</code> <p>Amount of swap space to use in GiB.</p> <code>4</code> <code>vllm_sliding_window</code> <code>Optional[int]</code> <p>Size of the sliding window for processing.</p> <code>None</code> <code>vllm_pipeline_parallel_size</code> <code>int</code> <p>Number of pipeline parallel groups.</p> <code>1</code> <code>vllm_tensor_parallel_size</code> <code>int</code> <p>Number of tensor parallel groups.</p> <code>1</code> <code>vllm_worker_use_ray</code> <code>bool</code> <p>Whether to use Ray for model workers.</p> <code>False</code> <code>vllm_max_parallel_loading_workers</code> <code>Optional[int]</code> <p>Maximum number of workers for parallel loading.</p> <code>None</code> <code>vllm_disable_custom_all_reduce</code> <code>bool</code> <p>Disable custom all-reduce kernel and fall back to NCCL.</p> <code>False</code> <code>vllm_max_num_batched_tokens</code> <code>Optional[int]</code> <p>Maximum number of tokens to be processed in a single iteration.</p> <code>None</code> <code>vllm_max_num_seqs</code> <code>int</code> <p>Maximum number of sequences to be processed in a single iteration.</p> <code>64</code> <code>vllm_max_paddings</code> <code>int</code> <p>Maximum number of paddings to be added to a batch.</p> <code>512</code> <code>vllm_max_lora_rank</code> <code>Optional[int]</code> <p>Maximum rank for LoRA adjustments.</p> <code>None</code> <code>vllm_max_loras</code> <code>Optional[int]</code> <p>Maximum number of LoRA adjustments.</p> <code>None</code> <code>vllm_max_cpu_loras</code> <code>Optional[int]</code> <p>Maximum number of LoRA adjustments stored on CPU.</p> <code>None</code> <code>vllm_lora_extra_vocab_size</code> <code>int</code> <p>Additional vocabulary size for LoRA.</p> <code>0</code> <code>vllm_placement_group</code> <code>Optional[dict]</code> <p>Ray placement group for distributed execution.</p> <code>None</code> <code>vllm_log_stats</code> <code>bool</code> <p>Whether to log statistics during model operation.</p> <code>False</code> <code>notification_email</code> <code>Optional[str]</code> <p>Email to send notifications upon completion.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Number of prompts to process in each batch for efficient memory usage.</p> <code>32</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for generation settings like temperature, top_p, etc.</p> <code>{}</code> <p>This method automates the loading of large datasets, generation of text completions, and saving results, facilitating efficient and scalable text generation tasks.</p>"},{"location":"text/bulk/language_model/","title":"Language Model","text":"<p>             Bases: <code>TextBulk</code></p> <p>LanguageModelBulk is designed for large-scale text generation using Hugging Face language models in a bulk processing manner. It's particularly useful for tasks such as bulk content creation, summarization, or any other scenario where large datasets need to be processed with a language model.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>Any</code> <p>The loaded language model used for text generation.</p> <code>tokenizer</code> <code>Any</code> <p>The tokenizer corresponding to the language model, used for processing input text.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>Configuration for the input data.</p> required <code>output</code> <code>BatchOutput</code> <p>Configuration for the output data.</p> required <code>state</code> <code>State</code> <p>State management for the API.</p> required <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments for extended functionality.</p> <code>{}</code> <p>CLI Usage Example: <pre><code>genius LanguageModelBulk rise \\\nbatch \\\n--input_s3_bucket geniusrise-test \\\n--input_s3_folder input/lm \\\nbatch \\\n--output_s3_bucket geniusrise-test \\\n--output_s3_folder output/lm \\\npostgres \\\n--postgres_host 127.0.0.1 \\\n--postgres_port 5432 \\\n--postgres_user postgres \\\n--postgres_password postgres \\\n--postgres_database geniusrise\\\n--postgres_table state \\\n--id mistralai/Mistral-7B-Instruct-v0.1-lol \\\ncomplete \\\n--args \\\nmodel_name=\"mistralai/Mistral-7B-Instruct-v0.1\" \\\nmodel_class=\"AutoModelForCausalLM\" \\\ntokenizer_class=\"AutoTokenizer\" \\\nuse_cuda=True \\\nprecision=\"bfloat16\" \\\nquantization=0 \\\ndevice_map=\"auto\" \\\nmax_memory=None \\\ntorchscript=False \\\ndecoding_strategy=\"generate\" \\\ngeneration_max_new_tokens=100 \\\ngeneration_do_sample=true\n</code></pre></p> <p>or using VLLM: <pre><code>genius LanguageModelBulk rise \\\nbatch \\\n--input_s3_bucket geniusrise-test \\\n--input_s3_folder input/lm \\\nbatch \\\n--output_s3_bucket geniusrise-test \\\n--output_s3_folder output/lm \\\nnone \\\n--id mistralai/Mistral-7B-v0.1 \\\ncomplete_vllm \\\n--args \\\nmodel_name=\"mistralai/Mistral-7B-v0.1\" \\\nuse_cuda=True \\\nprecision=\"bfloat16\" \\\nquantization=0 \\\ndevice_map=\"auto\" \\\nvllm_enforce_eager=True \\\ngeneration_temperature=0.7 \\\ngeneration_top_p=1.0 \\\ngeneration_n=1 \\\ngeneration_max_tokens=50 \\\ngeneration_stream=false \\\ngeneration_presence_penalty=0.0 \\\ngeneration_frequency_penalty=0.0\n</code></pre></p> <p>or using llama.cpp: <pre><code>genius LanguageModelBulk rise \\\nbatch \\\n--input_s3_bucket geniusrise-test \\\n--input_s3_folder input/chat \\\nbatch \\\n--output_s3_bucket geniusrise-test \\\n--output_s3_folder output/chat \\\nnone \\\ncomplete_llama_cpp \\\n--args \\\nmodel=\"TheBloke/Mistral-7B-v0.1-GGUF\" \\\nfilename=\"mistral-7b-v0.1.Q4_K_M.gguf\" \\\nn_gpu_layers=35  \\\nn_ctx=32768 \\\ngeneration_temperature=0.7 \\\ngeneration_top_p=0.95 \\\ngeneration_top_k=40 \\\ngeneration_max_tokens=50 \\\ngeneration_repeat_penalty=0.1\n</code></pre></p>"},{"location":"text/bulk/language_model/#geniusrise_text.language_model.bulk.LanguageModelBulk.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>Initializes the LanguageModelBulk object with the specified configurations for input, output, and state.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>Configuration and data inputs for the bulk process.</p> required <code>output</code> <code>BatchOutput</code> <p>Configurations for output data handling.</p> required <code>state</code> <code>State</code> <p>State management for the bulk process.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for extended configurations.</p> <code>{}</code>"},{"location":"text/bulk/language_model/#geniusrise_text.language_model.bulk.LanguageModelBulk.complete","title":"<code>complete(model_name, model_class='AutoModelForCausalLM', tokenizer_class='AutoTokenizer', use_cuda=False, precision='float16', quantization=0, device_map='auto', max_memory={0: '24GB'}, torchscript=False, compile=False, awq_enabled=False, flash_attention=False, decoding_strategy='generate', notification_email=None, **kwargs)</code>","text":"<p>Performs text completion on the loaded dataset using the specified model and tokenizer. The method handles the entire process, including model loading, text generation, and saving the results.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the language model to use for text completion.</p> required <code>model_class</code> <code>str</code> <p>The class of the language model. Defaults to \"AutoModelForCausalLM\".</p> <code>'AutoModelForCausalLM'</code> <code>tokenizer_class</code> <code>str</code> <p>The class of the tokenizer. Defaults to \"AutoTokenizer\".</p> <code>'AutoTokenizer'</code> <code>use_cuda</code> <code>bool</code> <p>Whether to use CUDA for model inference. Defaults to False.</p> <code>False</code> <code>precision</code> <code>str</code> <p>Precision for model computation. Defaults to \"float16\".</p> <code>'float16'</code> <code>quantization</code> <code>int</code> <p>Level of quantization for optimizing model size and speed. Defaults to 0.</p> <code>0</code> <code>device_map</code> <code>str | Dict | None</code> <p>Specific device to use for computation. Defaults to \"auto\".</p> <code>'auto'</code> <code>max_memory</code> <code>Dict</code> <p>Maximum memory configuration for devices. Defaults to {0: \"24GB\"}.</p> <code>{0: '24GB'}</code> <code>torchscript</code> <code>bool</code> <p>Whether to use a TorchScript-optimized version of the pre-trained language model. Defaults to False.</p> <code>False</code> <code>compile</code> <code>bool</code> <p>Whether to compile the model before fine-tuning. Defaults to True.</p> <code>False</code> <code>awq_enabled</code> <code>bool</code> <p>Whether to enable AWQ optimization. Defaults to False.</p> <code>False</code> <code>flash_attention</code> <code>bool</code> <p>Whether to use flash attention optimization. Defaults to False.</p> <code>False</code> <code>decoding_strategy</code> <code>str</code> <p>Strategy for decoding the completion. Defaults to \"generate\".</p> <code>'generate'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for text generation.</p> <code>{}</code>"},{"location":"text/bulk/language_model/#geniusrise_text.language_model.bulk.LanguageModelBulk.complete_llama_cpp","title":"<code>complete_llama_cpp(model, filename=None, local_dir=None, n_gpu_layers=0, split_mode=llama_cpp.LLAMA_SPLIT_LAYER, main_gpu=0, tensor_split=None, vocab_only=False, use_mmap=True, use_mlock=False, kv_overrides=None, seed=llama_cpp.LLAMA_DEFAULT_SEED, n_ctx=512, n_batch=512, n_threads=None, n_threads_batch=None, rope_scaling_type=llama_cpp.LLAMA_ROPE_SCALING_UNSPECIFIED, rope_freq_base=0.0, rope_freq_scale=0.0, yarn_ext_factor=-1.0, yarn_attn_factor=1.0, yarn_beta_fast=32.0, yarn_beta_slow=1.0, yarn_orig_ctx=0, mul_mat_q=True, logits_all=False, embedding=False, offload_kqv=True, last_n_tokens_size=64, lora_base=None, lora_scale=1.0, lora_path=None, numa=False, chat_format=None, chat_handler=None, draft_model=None, tokenizer=None, verbose=True, notification_email=None, **kwargs)</code>","text":"<p>Performs bulk text generation using the LLaMA model with llama.cpp backend. This method handles the entire process, including model loading, prompt processing, text generation, and saving the results.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Path or identifier for the LLaMA model.</p> required <code>filename</code> <code>Optional[str]</code> <p>Optional filename or glob pattern to match the model file.</p> <code>None</code> <code>local_dir</code> <code>Optional[Union[str, os.PathLike[str]]]</code> <p>Local directory to save the model files.</p> <code>None</code> <code>n_gpu_layers</code> <code>int</code> <p>Number of layers to offload to GPU.</p> <code>0</code> <code>split_mode</code> <code>int</code> <p>Split mode for distributing model across GPUs.</p> <code>llama_cpp.LLAMA_SPLIT_LAYER</code> <code>main_gpu</code> <code>int</code> <p>Main GPU index.</p> <code>0</code> <code>tensor_split</code> <code>Optional[List[float]]</code> <p>Configuration for tensor splitting across GPUs.</p> <code>None</code> <code>vocab_only</code> <code>bool</code> <p>Whether to load only the vocabulary.</p> <code>False</code> <code>use_mmap</code> <code>bool</code> <p>Use memory-mapped files for model loading.</p> <code>True</code> <code>use_mlock</code> <code>bool</code> <p>Lock model data in RAM to prevent swapping.</p> <code>False</code> <code>kv_overrides</code> <code>Optional[Dict[str, Union[bool, int, float]]]</code> <p>Key-value pairs for overriding model config.</p> <code>None</code> <code>seed</code> <code>int</code> <p>Seed for random number generation.</p> <code>llama_cpp.LLAMA_DEFAULT_SEED</code> <code>n_ctx</code> <code>int</code> <p>Number of context tokens for generation.</p> <code>512</code> <code>n_batch</code> <code>int</code> <p>Batch size for processing.</p> <code>512</code> <code>n_threads</code> <code>Optional[int]</code> <p>Number of threads for generation.</p> <code>None</code> <code>n_threads_batch</code> <code>Optional[int]</code> <p>Number of threads for batch processing.</p> <code>None</code> <code>rope_scaling_type</code> <code>Optional[int]</code> <p>Scaling type for RoPE.</p> <code>llama_cpp.LLAMA_ROPE_SCALING_UNSPECIFIED</code> <code>rope_freq_base</code> <code>float</code> <p>Base frequency for RoPE.</p> <code>0.0</code> <code>rope_freq_scale</code> <code>float</code> <p>Frequency scaling for RoPE.</p> <code>0.0</code> <code>yarn_ext_factor</code> <code>float</code> <p>YaRN extrapolation factor.</p> <code>-1.0</code> <code>yarn_attn_factor</code> <code>float</code> <p>YaRN attention factor.</p> <code>1.0</code> <code>yarn_beta_fast</code> <code>float</code> <p>YaRN beta fast parameter.</p> <code>32.0</code> <code>yarn_beta_slow</code> <code>float</code> <p>YaRN beta slow parameter.</p> <code>1.0</code> <code>yarn_orig_ctx</code> <code>int</code> <p>Original context size for YaRN.</p> <code>0</code> <code>mul_mat_q</code> <code>bool</code> <p>Multiply matrices for queries.</p> <code>True</code> <code>logits_all</code> <code>bool</code> <p>Return logits for all tokens.</p> <code>False</code> <code>embedding</code> <code>bool</code> <p>Enable embedding mode.</p> <code>False</code> <code>offload_kqv</code> <code>bool</code> <p>Offload K, Q, V matrices to GPU.</p> <code>True</code> <code>last_n_tokens_size</code> <code>int</code> <p>Size for the last_n_tokens buffer.</p> <code>64</code> <code>lora_base</code> <code>Optional[str]</code> <p>Base model path for LoRA.</p> <code>None</code> <code>lora_scale</code> <code>float</code> <p>Scale factor for LoRA adjustments.</p> <code>1.0</code> <code>lora_path</code> <code>Optional[str]</code> <p>Path for LoRA adjustments.</p> <code>None</code> <code>numa</code> <code>Union[bool, int]</code> <p>NUMA configuration.</p> <code>False</code> <code>chat_format</code> <code>Optional[str]</code> <p>Chat format configuration.</p> <code>None</code> <code>chat_handler</code> <code>Optional[llama_cpp.llama_chat_format.LlamaChatCompletionHandler]</code> <p>Handler for chat completions.</p> <code>None</code> <code>draft_model</code> <code>Optional[llama_cpp.LlamaDraftModel]</code> <p>Draft model for speculative decoding.</p> <code>None</code> <code>tokenizer</code> <code>Optional[PreTrainedTokenizerBase]</code> <p>Custom tokenizer instance.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Enable verbose logging.</p> <code>True</code> <code>notification_email</code> <code>Optional[str]</code> <p>Email to send notifications upon completion.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments for model loading and text generation.</p> <code>{}</code>"},{"location":"text/bulk/language_model/#geniusrise_text.language_model.bulk.LanguageModelBulk.complete_vllm","title":"<code>complete_vllm(model_name, use_cuda=False, precision='float16', quantization=0, device_map='auto', vllm_tokenizer_mode='auto', vllm_download_dir=None, vllm_load_format='auto', vllm_seed=42, vllm_max_model_len=1024, vllm_enforce_eager=False, vllm_max_context_len_to_capture=8192, vllm_block_size=16, vllm_gpu_memory_utilization=0.9, vllm_swap_space=4, vllm_sliding_window=None, vllm_pipeline_parallel_size=1, vllm_tensor_parallel_size=1, vllm_worker_use_ray=False, vllm_max_parallel_loading_workers=None, vllm_disable_custom_all_reduce=False, vllm_max_num_batched_tokens=None, vllm_max_num_seqs=64, vllm_max_paddings=512, vllm_max_lora_rank=None, vllm_max_loras=None, vllm_max_cpu_loras=None, vllm_lora_extra_vocab_size=0, vllm_placement_group=None, vllm_log_stats=False, notification_email=None, batch_size=32, **kwargs)</code>","text":"<p>Performs bulk text generation using the Versatile Language Learning Model (VLLM) with specified parameters for fine-tuning model behavior, including quantization and parallel processing settings. This method is designed to process large datasets efficiently by leveraging VLLM capabilities for generating high-quality text completions based on provided prompts.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name or path of the VLLM model to use for text generation.</p> required <code>use_cuda</code> <code>bool</code> <p>Flag indicating whether to use CUDA for GPU acceleration.</p> <code>False</code> <code>precision</code> <code>str</code> <p>Precision of computations, can be \"float16\", \"bfloat16\", etc.</p> <code>'float16'</code> <code>quantization</code> <code>int</code> <p>Level of quantization for model weights, 0 for none.</p> <code>0</code> <code>device_map</code> <code>str | Dict | None</code> <p>Specific device(s) to use for model inference.</p> <code>'auto'</code> <code>vllm_tokenizer_mode</code> <code>str</code> <p>Mode of the tokenizer (\"auto\", \"fast\", or \"slow\").</p> <code>'auto'</code> <code>vllm_download_dir</code> <code>Optional[str]</code> <p>Directory to download and load the model and tokenizer.</p> <code>None</code> <code>vllm_load_format</code> <code>str</code> <p>Format to load the model, e.g., \"auto\", \"pt\".</p> <code>'auto'</code> <code>vllm_seed</code> <code>int</code> <p>Seed for random number generation.</p> <code>42</code> <code>vllm_max_model_len</code> <code>int</code> <p>Maximum sequence length the model can handle.</p> <code>1024</code> <code>vllm_enforce_eager</code> <code>bool</code> <p>Enforce eager execution instead of using optimization techniques.</p> <code>False</code> <code>vllm_max_context_len_to_capture</code> <code>int</code> <p>Maximum context length for CUDA graph capture.</p> <code>8192</code> <code>vllm_block_size</code> <code>int</code> <p>Block size for caching mechanism.</p> <code>16</code> <code>vllm_gpu_memory_utilization</code> <code>float</code> <p>Fraction of GPU memory to use.</p> <code>0.9</code> <code>vllm_swap_space</code> <code>int</code> <p>Amount of swap space to use in GiB.</p> <code>4</code> <code>vllm_sliding_window</code> <code>Optional[int]</code> <p>Size of the sliding window for processing.</p> <code>None</code> <code>vllm_pipeline_parallel_size</code> <code>int</code> <p>Number of pipeline parallel groups.</p> <code>1</code> <code>vllm_tensor_parallel_size</code> <code>int</code> <p>Number of tensor parallel groups.</p> <code>1</code> <code>vllm_worker_use_ray</code> <code>bool</code> <p>Whether to use Ray for model workers.</p> <code>False</code> <code>vllm_max_parallel_loading_workers</code> <code>Optional[int]</code> <p>Maximum number of workers for parallel loading.</p> <code>None</code> <code>vllm_disable_custom_all_reduce</code> <code>bool</code> <p>Disable custom all-reduce kernel and fall back to NCCL.</p> <code>False</code> <code>vllm_max_num_batched_tokens</code> <code>Optional[int]</code> <p>Maximum number of tokens to be processed in a single iteration.</p> <code>None</code> <code>vllm_max_num_seqs</code> <code>int</code> <p>Maximum number of sequences to be processed in a single iteration.</p> <code>64</code> <code>vllm_max_paddings</code> <code>int</code> <p>Maximum number of paddings to be added to a batch.</p> <code>512</code> <code>vllm_max_lora_rank</code> <code>Optional[int]</code> <p>Maximum rank for LoRA adjustments.</p> <code>None</code> <code>vllm_max_loras</code> <code>Optional[int]</code> <p>Maximum number of LoRA adjustments.</p> <code>None</code> <code>vllm_max_cpu_loras</code> <code>Optional[int]</code> <p>Maximum number of LoRA adjustments stored on CPU.</p> <code>None</code> <code>vllm_lora_extra_vocab_size</code> <code>int</code> <p>Additional vocabulary size for LoRA.</p> <code>0</code> <code>vllm_placement_group</code> <code>Optional[dict]</code> <p>Ray placement group for distributed execution.</p> <code>None</code> <code>vllm_log_stats</code> <code>bool</code> <p>Whether to log statistics during model operation.</p> <code>False</code> <code>notification_email</code> <code>Optional[str]</code> <p>Email to send notifications upon completion.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Number of prompts to process in each batch for efficient memory usage.</p> <code>32</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for generation settings like temperature, top_p, etc.</p> <code>{}</code> <p>This method automates the loading of large datasets, generation of text completions, and saving results, facilitating efficient and scalable text generation tasks.</p>"},{"location":"text/bulk/language_model/#geniusrise_text.language_model.bulk.LanguageModelBulk.load_dataset","title":"<code>load_dataset(dataset_path, max_length=512, **kwargs)</code>","text":"<p>Load a completion dataset from a directory.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the dataset directory.</p> required <code>max_length</code> <code>int</code> <p>The maximum length for tokenization. Defaults to 512.</p> <code>512</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the underlying dataset loading functions.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Optional[Dataset]</code> <p>The loaded dataset.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there was an error loading the dataset.</p>"},{"location":"text/bulk/language_model/#geniusrise_text.language_model.bulk.LanguageModelBulk.load_dataset--supported-data-formats-and-structures","title":"Supported Data Formats and Structures:","text":""},{"location":"text/bulk/language_model/#geniusrise_text.language_model.bulk.LanguageModelBulk.load_dataset--dataset-files-saved-by-hugging-face-datasets-library","title":"Dataset files saved by Hugging Face datasets library","text":"<p>The directory should contain 'dataset_info.json' and other related files.</p>"},{"location":"text/bulk/language_model/#geniusrise_text.language_model.bulk.LanguageModelBulk.load_dataset--jsonl","title":"JSONL","text":"<p>Each line is a JSON object representing an example. <pre><code>{\"text\": \"The text content\"}\n</code></pre></p>"},{"location":"text/bulk/language_model/#geniusrise_text.language_model.bulk.LanguageModelBulk.load_dataset--csv","title":"CSV","text":"<p>Should contain 'text' column. <pre><code>text\n\"The text content\"\n</code></pre></p>"},{"location":"text/bulk/language_model/#geniusrise_text.language_model.bulk.LanguageModelBulk.load_dataset--parquet","title":"Parquet","text":"<p>Should contain 'text' column.</p>"},{"location":"text/bulk/language_model/#geniusrise_text.language_model.bulk.LanguageModelBulk.load_dataset--json","title":"JSON","text":"<p>An array of dictionaries with 'text' key. <pre><code>[{\"text\": \"The text content\"}]\n</code></pre></p>"},{"location":"text/bulk/language_model/#geniusrise_text.language_model.bulk.LanguageModelBulk.load_dataset--xml","title":"XML","text":"<p>Each 'record' element should contain 'text' child element. <pre><code>&lt;record&gt;\n&lt;text&gt;The text content&lt;/text&gt;\n&lt;/record&gt;\n</code></pre></p>"},{"location":"text/bulk/language_model/#geniusrise_text.language_model.bulk.LanguageModelBulk.load_dataset--yaml","title":"YAML","text":"<p>Each document should be a dictionary with 'text' key. <pre><code>- text: \"The text content\"\n</code></pre></p>"},{"location":"text/bulk/language_model/#geniusrise_text.language_model.bulk.LanguageModelBulk.load_dataset--tsv","title":"TSV","text":"<p>Should contain 'text' column separated by tabs.</p>"},{"location":"text/bulk/language_model/#geniusrise_text.language_model.bulk.LanguageModelBulk.load_dataset--excel-xls-xlsx","title":"Excel (.xls, .xlsx)","text":"<p>Should contain 'text' column.</p>"},{"location":"text/bulk/language_model/#geniusrise_text.language_model.bulk.LanguageModelBulk.load_dataset--sqlite-db","title":"SQLite (.db)","text":"<p>Should contain a table with 'text' column.</p>"},{"location":"text/bulk/language_model/#geniusrise_text.language_model.bulk.LanguageModelBulk.load_dataset--feather","title":"Feather","text":"<p>Should contain 'text' column.</p>"},{"location":"text/bulk/ner/","title":"Named Entity Recognition","text":"<p>             Bases: <code>TextBulk</code></p> <p>NamedEntityRecognitionBulk is a class designed for bulk processing of Named Entity Recognition (NER) tasks. It leverages state-of-the-art NER models from Hugging Face's transformers library to identify and classify entities such as person names, locations, organizations, and other types of entities from a large corpus of text.</p> <p>This class provides functionalities to load large datasets, configure NER models, and perform entity recognition in bulk, making it suitable for processing large volumes of text data efficiently.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>Any</code> <p>The NER model loaded for entity recognition tasks.</p> <code>tokenizer</code> <code>Any</code> <p>The tokenizer used for text pre-processing in alignment with the model.</p> <p>Example CLI Usage: <pre><code>genius NamedEntityRecognitionBulk rise \\\nbatch \\\n--input_folder ./input \\\nbatch \\\n--output_folder ./output \\\nnone \\\n--id dslim/bert-large-NER-lol \\\nrecognize_entities \\\n--args \\\nmodel_name=\"dslim/bert-large-NER\" \\\nmodel_class=\"AutoModelForTokenClassification\" \\\ntokenizer_class=\"AutoTokenizer\" \\\nuse_cuda=True \\\nprecision=\"float\" \\\nquantization=0 \\\ndevice_map=\"cuda:0\" \\\nmax_memory=None \\\ntorchscript=False\n</code></pre></p>"},{"location":"text/bulk/ner/#geniusrise_text.ner.bulk.NamedEntityRecognitionBulk.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>Initializes the NamedEntityRecognitionBulk class with specified input, output, and state configurations. Sets up the NER model and tokenizer for bulk entity recognition tasks.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>The input data configuration.</p> required <code>output</code> <code>BatchOutput</code> <p>The output data configuration.</p> required <code>state</code> <code>State</code> <p>The state management for the API.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for extended functionality.</p> <code>{}</code>"},{"location":"text/bulk/ner/#geniusrise_text.ner.bulk.NamedEntityRecognitionBulk.load_dataset","title":"<code>load_dataset(dataset_path, **kwargs)</code>","text":"<p>Loads a dataset from the specified directory path. The method supports various data formats and structures, ensuring that the dataset is properly formatted for NER tasks.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the dataset directory.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to handle specific dataset loading scenarios.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[Dataset]</code> <p>Optional[Dataset]: The loaded dataset or None if an error occurs during loading.</p>"},{"location":"text/bulk/ner/#geniusrise_text.ner.bulk.NamedEntityRecognitionBulk.load_dataset--supported-data-formats-and-structures","title":"Supported Data Formats and Structures:","text":""},{"location":"text/bulk/ner/#geniusrise_text.ner.bulk.NamedEntityRecognitionBulk.load_dataset--hugging-face-dataset","title":"Hugging Face Dataset","text":"<p>Dataset files saved by the Hugging Face datasets library.</p>"},{"location":"text/bulk/ner/#geniusrise_text.ner.bulk.NamedEntityRecognitionBulk.load_dataset--jsonl","title":"JSONL","text":"<p>Each line is a JSON object representing an example. <pre><code>{\"tokens\": [\"token1\", \"token2\", ...]}\n</code></pre></p>"},{"location":"text/bulk/ner/#geniusrise_text.ner.bulk.NamedEntityRecognitionBulk.load_dataset--csv","title":"CSV","text":"<p>Should contain 'tokens' columns. <pre><code>tokens\n\"['token1', 'token2', ...]\"\n</code></pre></p>"},{"location":"text/bulk/ner/#geniusrise_text.ner.bulk.NamedEntityRecognitionBulk.load_dataset--parquet","title":"Parquet","text":"<p>Should contain 'tokens' columns.</p>"},{"location":"text/bulk/ner/#geniusrise_text.ner.bulk.NamedEntityRecognitionBulk.load_dataset--json","title":"JSON","text":"<p>An array of dictionaries with 'tokens' keys. <pre><code>[{\"tokens\": [\"token1\", \"token2\", ...]}]\n</code></pre></p>"},{"location":"text/bulk/ner/#geniusrise_text.ner.bulk.NamedEntityRecognitionBulk.load_dataset--xml","title":"XML","text":"<p>Each 'record' element should contain 'tokens' child elements. <pre><code>&lt;record&gt;\n&lt;tokens&gt;token1 token2 ...&lt;/tokens&gt;\n&lt;/record&gt;\n</code></pre></p>"},{"location":"text/bulk/ner/#geniusrise_text.ner.bulk.NamedEntityRecognitionBulk.load_dataset--yaml","title":"YAML","text":"<p>Each document should be a dictionary with 'tokens' keys. <pre><code>- tokens: [\"token1\", \"token2\", ...]\n</code></pre></p>"},{"location":"text/bulk/ner/#geniusrise_text.ner.bulk.NamedEntityRecognitionBulk.load_dataset--tsv","title":"TSV","text":"<p>Should contain 'tokens' columns separated by tabs.</p>"},{"location":"text/bulk/ner/#geniusrise_text.ner.bulk.NamedEntityRecognitionBulk.load_dataset--excel-xls-xlsx","title":"Excel (.xls, .xlsx)","text":"<p>Should contain 'tokens' columns.</p>"},{"location":"text/bulk/ner/#geniusrise_text.ner.bulk.NamedEntityRecognitionBulk.load_dataset--sqlite-db","title":"SQLite (.db)","text":"<p>Should contain a table with 'tokens' columns.</p>"},{"location":"text/bulk/ner/#geniusrise_text.ner.bulk.NamedEntityRecognitionBulk.load_dataset--feather","title":"Feather","text":"<p>Should contain 'tokens' columns.</p>"},{"location":"text/bulk/ner/#geniusrise_text.ner.bulk.NamedEntityRecognitionBulk.recognize_entities","title":"<code>recognize_entities(model_name, max_length=512, model_class='AutoModelForSeq2SeqLM', tokenizer_class='AutoTokenizer', use_cuda=False, precision='float16', quantization=0, device_map='auto', max_memory={0: '24GB'}, torchscript=False, compile=False, awq_enabled=False, flash_attention=False, batch_size=32, notification_email=None, **kwargs)</code>","text":"<p>Performs bulk named entity recognition on the loaded dataset. The method processes the text in batches, applying the NER model to recognize entities.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name or path of the NER model.</p> required <code>max_length</code> <code>int</code> <p>The maximum sequence length for the tokenizer.</p> <code>512</code> <code>model_class</code> <code>str</code> <p>The class of the model, defaults to \"AutoModelForTokenClassification\".</p> <code>'AutoModelForSeq2SeqLM'</code> <code>tokenizer_class</code> <code>str</code> <p>The class of the tokenizer, defaults to \"AutoTokenizer\".</p> <code>'AutoTokenizer'</code> <code>use_cuda</code> <code>bool</code> <p>Whether to use CUDA for model inference, defaults to False.</p> <code>False</code> <code>precision</code> <code>str</code> <p>Model computation precision, defaults to \"float16\".</p> <code>'float16'</code> <code>quantization</code> <code>int</code> <p>Level of quantization for model size and speed optimization, defaults to 0.</p> <code>0</code> <code>device_map</code> <code>str | Dict | None</code> <p>Specific device configuration for computation, defaults to \"auto\".</p> <code>'auto'</code> <code>max_memory</code> <code>Dict</code> <p>Maximum memory configuration for the devices.</p> <code>{0: '24GB'}</code> <code>torchscript</code> <code>bool</code> <p>Whether to use a TorchScript-optimized version of the pre-trained language model. Defaults to False.</p> <code>False</code> <code>compile</code> <code>bool</code> <p>Whether to compile the model before fine-tuning. Defaults to True.</p> <code>False</code> <code>awq_enabled</code> <code>bool</code> <p>Whether to enable AWQ optimization, defaults to False.</p> <code>False</code> <code>flash_attention</code> <code>bool</code> <p>Whether to use flash attention optimization, defaults to False.</p> <code>False</code> <code>batch_size</code> <code>int</code> <p>Number of documents to process simultaneously, defaults to 32.</p> <code>32</code> <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments for additional configuration.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>The method processes the dataset and saves the predictions without returning any value.</p>"},{"location":"text/bulk/nli/","title":"Natural Language Inference","text":"<p>             Bases: <code>TextBulk</code></p> <p>The NLIBulk class provides functionality for large-scale natural language inference (NLI) processing using Hugging Face transformers. It allows users to load datasets, configure models, and perform inference on batches of premise-hypothesis pairs.</p> <p>Attributes:</p> Name Type Description <code>input</code> <code>BatchInput</code> <p>Configuration and data inputs for the batch process.</p> <code>output</code> <code>BatchOutput</code> <p>Configurations for output data handling.</p> <code>state</code> <code>State</code> <p>State management for the inference task.</p> <p>Example CLI Usage: <pre><code>genius NLIBulk rise \\\nbatch \\\n--input_s3_bucket geniusrise-test \\\n--input_s3_folder input/nli \\\nbatch \\\n--output_s3_bucket geniusrise-test \\\n--output_s3_folder output/nli \\\npostgres \\\n--postgres_host 127.0.0.1 \\\n--postgres_port 5432 \\\n--postgres_user postgres \\\n--postgres_password postgres \\\n--postgres_database geniusrise\\\n--postgres_table state \\\n--id MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7-lol \\\ninfer \\\n--args \\\nmodel_name=\"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\" \\\nmodel_class=\"AutoModelForSequenceClassification\" \\\ntokenizer_class=\"AutoTokenizer\" \\\nuse_cuda=True \\\nprecision=\"float\" \\\nquantization=0 \\\ndevice_map=\"cuda:0\" \\\nmax_memory=None \\\ntorchscript=False\n</code></pre></p>"},{"location":"text/bulk/nli/#geniusrise_text.nli.bulk.NLIBulk.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>Initializes the NLIBulk class with the specified input, output, and state configurations.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>The input data.</p> required <code>output</code> <code>BatchOutput</code> <p>The output data.</p> required <code>state</code> <code>State</code> <p>The state data.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"text/bulk/nli/#geniusrise_text.nli.bulk.NLIBulk.infer","title":"<code>infer(model_name, max_length=512, model_class='AutoModelForSeq2SeqLM', tokenizer_class='AutoTokenizer', use_cuda=False, precision='float16', quantization=0, device_map='auto', max_memory={0: '24GB'}, torchscript=False, compile=False, awq_enabled=False, flash_attention=False, batch_size=32, notification_email=None, **kwargs)</code>","text":"<p>Performs NLI inference on a loaded dataset using the specified model. The method processes the data in batches and saves the results to the configured output path.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name or path of the NLI model.</p> required <code>max_length</code> <code>int</code> <p>Maximum length of the sequences for tokenization purposes. Defaults to 512.</p> <code>512</code> <code>model_class</code> <code>str</code> <p>Class name of the model (e.g., \"AutoModelForSequenceClassification\"). Defaults to \"AutoModelForSeq2SeqLM\".</p> <code>'AutoModelForSeq2SeqLM'</code> <code>tokenizer_class</code> <code>str</code> <p>Class name of the tokenizer (e.g., \"AutoTokenizer\"). Defaults to \"AutoTokenizer\".</p> <code>'AutoTokenizer'</code> <code>use_cuda</code> <code>bool</code> <p>Whether to use CUDA for model inference. Defaults to False.</p> <code>False</code> <code>precision</code> <code>str</code> <p>Precision for model computation (e.g., \"float16\"). Defaults to \"float16\".</p> <code>'float16'</code> <code>quantization</code> <code>int</code> <p>Level of quantization for optimizing model size and speed. Defaults to 0.</p> <code>0</code> <code>device_map</code> <code>str | Dict | None</code> <p>Specific device to use for computation. Defaults to \"auto\".</p> <code>'auto'</code> <code>max_memory</code> <code>Dict</code> <p>Maximum memory configuration for devices. Defaults to {0: \"24GB\"}.</p> <code>{0: '24GB'}</code> <code>torchscript</code> <code>bool</code> <p>Whether to use a TorchScript-optimized version of the pre-trained language model. Defaults to False.</p> <code>False</code> <code>compile</code> <code>bool</code> <p>Whether to compile the model before fine-tuning. Defaults to True.</p> <code>False</code> <code>awq_enabled</code> <code>bool</code> <p>Whether to enable AWQ optimization. Defaults to False.</p> <code>False</code> <code>flash_attention</code> <code>bool</code> <p>Whether to use flash attention optimization. Defaults to False.</p> <code>False</code> <code>batch_size</code> <code>int</code> <p>Number of premise-hypothesis pairs to process simultaneously. Defaults to 32.</p> <code>32</code> <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments for model and generation configurations.</p> <code>{}</code> <p>```</p>"},{"location":"text/bulk/nli/#geniusrise_text.nli.bulk.NLIBulk.load_dataset","title":"<code>load_dataset(dataset_path, max_length=512, **kwargs)</code>","text":"<p>Load a commonsense reasoning dataset from a directory.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the dataset directory or file.</p> required <code>max_length</code> <code>int</code> <p>Maximum length of text sequences for tokenization purposes. Defaults to 512.</p> <code>512</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Optional[Dataset]</code> <p>The loaded dataset.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there was an error loading the dataset.</p>"},{"location":"text/bulk/nli/#geniusrise_text.nli.bulk.NLIBulk.load_dataset--supported-data-formats-and-structures","title":"Supported Data Formats and Structures:","text":""},{"location":"text/bulk/nli/#geniusrise_text.nli.bulk.NLIBulk.load_dataset--hugging-face-dataset","title":"Hugging Face Dataset","text":"<p>Dataset files saved by the Hugging Face datasets library.</p>"},{"location":"text/bulk/nli/#geniusrise_text.nli.bulk.NLIBulk.load_dataset--jsonl","title":"JSONL","text":"<p>Each line is a JSON object representing an example. <pre><code>{\"premise\": \"The premise text\", \"hypothesis\": \"The hypothesis text\"}\n</code></pre></p>"},{"location":"text/bulk/nli/#geniusrise_text.nli.bulk.NLIBulk.load_dataset--csv","title":"CSV","text":"<p>Should contain 'premise' and 'hypothesis' columns. <pre><code>premise,hypothesis\n\"The premise text\",\"The hypothesis text\"\n</code></pre></p>"},{"location":"text/bulk/nli/#geniusrise_text.nli.bulk.NLIBulk.load_dataset--parquet","title":"Parquet","text":"<p>Should contain 'premise' and 'hypothesis' columns.</p>"},{"location":"text/bulk/nli/#geniusrise_text.nli.bulk.NLIBulk.load_dataset--json","title":"JSON","text":"<p>An array of dictionaries with 'premise' and 'hypothesis' keys. <pre><code>[{\"premise\": \"The premise text\", \"hypothesis\": \"The hypothesis text\"}]\n</code></pre></p>"},{"location":"text/bulk/nli/#geniusrise_text.nli.bulk.NLIBulk.load_dataset--xml","title":"XML","text":"<p>Each 'record' element should contain 'premise' and 'hypothesis' child elements. <pre><code>&lt;record&gt;\n&lt;premise&gt;The premise text&lt;/premise&gt;\n&lt;hypothesis&gt;The hypothesis text&lt;/hypothesis&gt;\n&lt;/record&gt;\n</code></pre></p>"},{"location":"text/bulk/nli/#geniusrise_text.nli.bulk.NLIBulk.load_dataset--yaml","title":"YAML","text":"<p>Each document should be a dictionary with 'premise' and 'hypothesis' keys. <pre><code>- premise: \"The premise text\"\nhypothesis: \"The hypothesis text\"\n</code></pre></p>"},{"location":"text/bulk/nli/#geniusrise_text.nli.bulk.NLIBulk.load_dataset--tsv","title":"TSV","text":"<p>Should contain 'premise' and 'hypothesis' columns separated by tabs.</p>"},{"location":"text/bulk/nli/#geniusrise_text.nli.bulk.NLIBulk.load_dataset--excel-xls-xlsx","title":"Excel (.xls, .xlsx)","text":"<p>Should contain 'premise' and 'hypothesis' columns.</p>"},{"location":"text/bulk/nli/#geniusrise_text.nli.bulk.NLIBulk.load_dataset--sqlite-db","title":"SQLite (.db)","text":"<p>Should contain a table with 'premise' and 'hypothesis' columns.</p>"},{"location":"text/bulk/nli/#geniusrise_text.nli.bulk.NLIBulk.load_dataset--feather","title":"Feather","text":"<p>Should contain 'premise' and 'hypothesis' columns.</p>"},{"location":"text/bulk/question_answering/","title":"Question Answering","text":"<p>             Bases: <code>TextBulk</code></p> <p>QABulk is a class designed for managing bulk question-answering tasks using Hugging Face models. It is capable of handling both traditional text-based QA and table-based QA (using TAPAS and TAPEX models), providing a versatile solution for automated question answering at scale.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>Configuration and data inputs for batch processing.</p> required <code>output</code> <code>BatchOutput</code> <p>Configurations for output data handling.</p> required <code>state</code> <code>State</code> <p>State management for the bulk QA task.</p> required <code>**kwargs</code> <p>Arbitrary keyword arguments for extended functionality.</p> <code>{}</code> <p>Example CLI Usage: <pre><code># For traditional text-based QA:\ngenius QABulk rise \\\nbatch \\\n--input_s3_bucket geniusrise-test \\\n--input_s3_folder input/qa-traditional \\\nbatch \\\n--output_s3_bucket geniusrise-test \\\n--output_s3_folder output/qa-traditional \\\npostgres \\\n--postgres_host 127.0.0.1 \\\n--postgres_port 5432 \\\n--postgres_user postgres \\\n--postgres_password postgres \\\n--postgres_database geniusrise\\\n--postgres_table state \\\n--id distilbert-base-uncased-distilled-squad-lol \\\nanswer_questions \\\n--args \\\nmodel_name=\"distilbert-base-uncased-distilled-squad\" \\\nmodel_class=\"AutoModelForQuestionAnswering\" \\\ntokenizer_class=\"AutoTokenizer\" \\\nuse_cuda=True \\\nprecision=\"bfloat16\" \\\nquantization=0 \\\ndevice_map=\"cuda:0\" \\\nmax_memory=None \\\ntorchscript=False\n\n# For table-based QA using TAPAS:\ngenius QABulk rise \\\nbatch \\\n--input_s3_bucket geniusrise-test \\\n--input_s3_folder input/qa-table \\\nbatch \\\n--output_s3_bucket geniusrise-test \\\n--output_s3_folder output/qa-table \\\npostgres \\\n--postgres_host 127.0.0.1 \\\n--postgres_port 5432 \\\n--postgres_user postgres \\\n--postgres_password postgres \\\n--postgres_database geniusrise\\\n--postgres_table state \\\n--id google/tapas-base-finetuned-wtq-lol \\\nanswer_questions \\\n--args \\\nmodel_name=\"google/tapas-base-finetuned-wtq\" \\\nmodel_class=\"AutoModelForTableQuestionAnswering\" \\\ntokenizer_class=\"AutoTokenizer\" \\\nuse_cuda=True \\\nprecision=\"float\" \\\nquantization=0 \\\ndevice_map=\"cuda:0\" \\\nmax_memory=None \\\ntorchscript=False\n\n# For table-based QA using TAPEX:\ngenius QABulk rise \\\nbatch \\\n--input_s3_bucket geniusrise-test \\\n--input_s3_folder input/qa-table \\\nbatch \\\n--output_s3_bucket geniusrise-test \\\n--output_s3_folder output/qa-table \\\npostgres \\\n--postgres_host 127.0.0.1 \\\n--postgres_port 5432 \\\n--postgres_user postgres \\\n--postgres_password postgres \\\n--postgres_database geniusrise\\\n--postgres_table state \\\n--id microsoft/tapex-large-finetuned-wtq-lol \\\nanswer_questions \\\n--args \\\nmodel_name=\"microsoft/tapex-large-finetuned-wtq\" \\\nmodel_class=\"AutoModelForSeq2SeqLM\" \\\ntokenizer_class=\"AutoTokenizer\" \\\nuse_cuda=True \\\nprecision=\"float\" \\\nquantization=0 \\\ndevice_map=\"cuda:0\" \\\nmax_memory=None \\\ntorchscript=False\n</code></pre></p>"},{"location":"text/bulk/question_answering/#geniusrise_text.qa.bulk.QABulk.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>Initializes the QABulk class with configurations for input, output, and state.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>Configuration for the input data.</p> required <code>output</code> <code>BatchOutput</code> <p>Configuration for the output data.</p> required <code>state</code> <code>State</code> <p>State management for the QA task.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for extended functionality.</p> <code>{}</code>"},{"location":"text/bulk/question_answering/#geniusrise_text.qa.bulk.QABulk.answer_questions","title":"<code>answer_questions(model_name, model_class='AutoModelForQuestionAnswering', tokenizer_class='AutoTokenizer', use_cuda=False, precision='float16', quantization=0, device_map='auto', max_memory={0: '24GB'}, torchscript=False, compile=False, awq_enabled=False, flash_attention=False, batch_size=32, notification_email=None, **kwargs)</code>","text":"<p>Perform bulk question-answering using the specified model and tokenizer. This method can handle various types of QA models including traditional, TAPAS, and TAPEX.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name or path of the question-answering model.</p> required <code>model_class</code> <code>str</code> <p>Class name of the model (e.g., \"AutoModelForQuestionAnswering\").</p> <code>'AutoModelForQuestionAnswering'</code> <code>tokenizer_class</code> <code>str</code> <p>Class name of the tokenizer (e.g., \"AutoTokenizer\").</p> <code>'AutoTokenizer'</code> <code>use_cuda</code> <code>bool</code> <p>Whether to use CUDA for model inference. Defaults to False.</p> <code>False</code> <code>precision</code> <code>str</code> <p>Precision for model computation. Defaults to \"float16\".</p> <code>'float16'</code> <code>quantization</code> <code>int</code> <p>Level of quantization for optimizing model size and speed. Defaults to 0.</p> <code>0</code> <code>device_map</code> <code>str | Dict | None</code> <p>Specific device to use for computation. Defaults to \"auto\".</p> <code>'auto'</code> <code>max_memory</code> <code>Dict</code> <p>Maximum memory configuration for devices. Defaults to {0: \"24GB\"}.</p> <code>{0: '24GB'}</code> <code>torchscript</code> <code>bool</code> <p>Whether to use a TorchScript-optimized version of the pre-trained language model. Defaults to False.</p> <code>False</code> <code>compile</code> <code>bool</code> <p>Whether to compile the model before fine-tuning. Defaults to True.</p> <code>False</code> <code>awq_enabled</code> <code>bool</code> <p>Whether to enable AWQ optimization. Defaults to False.</p> <code>False</code> <code>flash_attention</code> <code>bool</code> <p>Whether to use flash attention optimization. Defaults to False.</p> <code>False</code> <code>batch_size</code> <code>int</code> <p>Number of questions to process simultaneously. Defaults to 32.</p> <code>32</code> <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments for model and generation configurations.</p> <code>{}</code> Processing <p>The method processes the data in batches, utilizing the appropriate model based on the model name and generating answers for the questions provided in the dataset.</p>"},{"location":"text/bulk/question_answering/#geniusrise_text.qa.bulk.QABulk.load_dataset","title":"<code>load_dataset(dataset_path, max_length=512, **kwargs)</code>","text":"<p>Load a dataset from a directory.</p>"},{"location":"text/bulk/question_answering/#geniusrise_text.qa.bulk.QABulk.load_dataset--supported-data-formats-and-structures","title":"Supported Data Formats and Structures:","text":""},{"location":"text/bulk/question_answering/#geniusrise_text.qa.bulk.QABulk.load_dataset--jsonl","title":"JSONL","text":"<p>Each line is a JSON object representing an example. <pre><code>{\"context\": \"The context content\", \"question\": \"The question\"}\n</code></pre></p>"},{"location":"text/bulk/question_answering/#geniusrise_text.qa.bulk.QABulk.load_dataset--csv","title":"CSV","text":"<p>Should contain 'context' and 'question' columns. <pre><code>context,question\n\"The context content\",\"The question\"\n</code></pre></p>"},{"location":"text/bulk/question_answering/#geniusrise_text.qa.bulk.QABulk.load_dataset--parquet","title":"Parquet","text":"<p>Should contain 'context' and 'question' columns.</p>"},{"location":"text/bulk/question_answering/#geniusrise_text.qa.bulk.QABulk.load_dataset--json","title":"JSON","text":"<p>An array of dictionaries with 'context' and 'question' keys. <pre><code>[{\"context\": \"The context content\", \"question\": \"The question\"}]\n</code></pre></p>"},{"location":"text/bulk/question_answering/#geniusrise_text.qa.bulk.QABulk.load_dataset--xml","title":"XML","text":"<p>Each 'record' element should contain 'context' and 'question' elements. <pre><code>&lt;record&gt;\n&lt;context&gt;The context content&lt;/context&gt;\n&lt;question&gt;The question&lt;/question&gt;\n&lt;/record&gt;\n</code></pre></p>"},{"location":"text/bulk/question_answering/#geniusrise_text.qa.bulk.QABulk.load_dataset--yaml","title":"YAML","text":"<p>Each document should be a dictionary with 'context' and 'question' keys. <pre><code>- context: \"The context content\"\nquestion: \"The question\"\n</code></pre></p>"},{"location":"text/bulk/question_answering/#geniusrise_text.qa.bulk.QABulk.load_dataset--tsv","title":"TSV","text":"<p>Should contain 'context' and 'question' columns separated by tabs.</p>"},{"location":"text/bulk/question_answering/#geniusrise_text.qa.bulk.QABulk.load_dataset--excel-xls-xlsx","title":"Excel (.xls, .xlsx)","text":"<p>Should contain 'context' and 'question' columns.</p>"},{"location":"text/bulk/question_answering/#geniusrise_text.qa.bulk.QABulk.load_dataset--sqlite-db","title":"SQLite (.db)","text":"<p>Should contain a table with 'context' and 'question' columns.</p>"},{"location":"text/bulk/question_answering/#geniusrise_text.qa.bulk.QABulk.load_dataset--feather","title":"Feather","text":"<p>Should contain 'context' and 'question' columns.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the dataset directory.</p> required <code>pad_on_right</code> <code>bool</code> <p>Whether to pad on the right.</p> required <code>max_length</code> <code>int</code> <p>The maximum length of the sequences.</p> <code>512</code> <code>doc_stride</code> <code>int</code> <p>The document stride.</p> required <code>evaluate_squadv2</code> <code>bool</code> <p>Whether to evaluate using SQuAD v2 metrics.</p> required <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Optional[Dataset]</code> <p>The loaded dataset.</p>"},{"location":"text/bulk/summarization/","title":"Summarization","text":"<p>             Bases: <code>TextBulk</code></p> <p>SummarizationBulk is a class for managing bulk text summarization tasks using Hugging Face models. It is designed to handle large-scale summarization tasks efficiently and effectively, utilizing state-of-the-art machine learning models to provide high-quality summaries.</p> <p>The class provides methods to load datasets, configure summarization models, and execute bulk summarization tasks.</p> <p>Example CLI Usage: <pre><code>genius SummarizationBulk rise \\\nbatch \\\n--input_s3_bucket geniusrise-test \\\n--input_s3_folder input/summz \\\nbatch \\\n--output_s3_bucket geniusrise-test \\\n--output_s3_folder output/summz \\\npostgres \\\n--postgres_host 127.0.0.1 \\\n--postgres_port 5432 \\\n--postgres_user postgres \\\n--postgres_password postgres \\\n--postgres_database geniusrise\\\n--postgres_table state \\\n--id facebook/bart-large-cnn-lol \\\nsummarize \\\n--args \\\nmodel_name=\"facebook/bart-large-cnn\" \\\nmodel_class=\"AutoModelForSeq2SeqLM\" \\\ntokenizer_class=\"AutoTokenizer\" \\\nuse_cuda=True \\\nprecision=\"float\" \\\nquantization=0 \\\ndevice_map=\"cuda:0\" \\\nmax_memory=None \\\ntorchscript=False \\\ngeneration_bos_token_id=0 \\\ngeneration_decoder_start_token_id=2 \\\ngeneration_early_stopping=true \\\ngeneration_eos_token_id=2 \\\ngeneration_forced_bos_token_id=0 \\\ngeneration_forced_eos_token_id=2 \\\ngeneration_length_penalty=2.0 \\\ngeneration_max_length=142 \\\ngeneration_min_length=56 \\\ngeneration_no_repeat_ngram_size=3 \\\ngeneration_num_beams=4 \\\ngeneration_pad_token_id=1 \\\ngeneration_do_sample=false\n</code></pre></p>"},{"location":"text/bulk/summarization/#geniusrise_text.summarization.bulk.SummarizationBulk.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>Initializes the SummarizationBulk class.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>The input data configuration.</p> required <code>output</code> <code>BatchOutput</code> <p>The output data configuration.</p> required <code>state</code> <code>State</code> <p>The state configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"text/bulk/summarization/#geniusrise_text.summarization.bulk.SummarizationBulk.load_dataset","title":"<code>load_dataset(dataset_path, max_length=512, **kwargs)</code>","text":"<p>Load a dataset from a directory.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the dataset directory.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[Dataset]</code> <p>Dataset | DatasetDict: The loaded dataset.</p>"},{"location":"text/bulk/summarization/#geniusrise_text.summarization.bulk.SummarizationBulk.load_dataset--supported-data-formats-and-structures","title":"Supported Data Formats and Structures:","text":""},{"location":"text/bulk/summarization/#geniusrise_text.summarization.bulk.SummarizationBulk.load_dataset--jsonl","title":"JSONL","text":"<p>Each line is a JSON object representing an example. <pre><code>{\"text\": \"The text content\"}\n</code></pre></p>"},{"location":"text/bulk/summarization/#geniusrise_text.summarization.bulk.SummarizationBulk.load_dataset--csv","title":"CSV","text":"<p>Should contain a 'text' column. <pre><code>text\n\"The text content\"\n</code></pre></p>"},{"location":"text/bulk/summarization/#geniusrise_text.summarization.bulk.SummarizationBulk.load_dataset--parquet","title":"Parquet","text":"<p>Should contain a 'text' column.</p>"},{"location":"text/bulk/summarization/#geniusrise_text.summarization.bulk.SummarizationBulk.load_dataset--json","title":"JSON","text":"<p>An array of dictionaries with a 'text' key. <pre><code>[{\"text\": \"The text content\"}]\n</code></pre></p>"},{"location":"text/bulk/summarization/#geniusrise_text.summarization.bulk.SummarizationBulk.load_dataset--xml","title":"XML","text":"<p>Each 'record' element should contain 'text' child element. <pre><code>&lt;record&gt;\n&lt;text&gt;The text content&lt;/text&gt;\n&lt;/record&gt;\n</code></pre></p>"},{"location":"text/bulk/summarization/#geniusrise_text.summarization.bulk.SummarizationBulk.load_dataset--yaml","title":"YAML","text":"<p>Each document should be a dictionary with a 'text' key. <pre><code>- text: \"The text content\"\n</code></pre></p>"},{"location":"text/bulk/summarization/#geniusrise_text.summarization.bulk.SummarizationBulk.load_dataset--tsv","title":"TSV","text":"<p>Should contain a 'text' column.</p>"},{"location":"text/bulk/summarization/#geniusrise_text.summarization.bulk.SummarizationBulk.load_dataset--excel-xls-xlsx","title":"Excel (.xls, .xlsx)","text":"<p>Should contain a 'text' column.</p>"},{"location":"text/bulk/summarization/#geniusrise_text.summarization.bulk.SummarizationBulk.load_dataset--sqlite-db","title":"SQLite (.db)","text":"<p>Should contain a table with a 'text' column.</p>"},{"location":"text/bulk/summarization/#geniusrise_text.summarization.bulk.SummarizationBulk.load_dataset--feather","title":"Feather","text":"<p>Should contain a 'text' column.</p>"},{"location":"text/bulk/summarization/#geniusrise_text.summarization.bulk.SummarizationBulk.summarize","title":"<code>summarize(model_name, model_class='AutoModelForSeq2SeqLM', tokenizer_class='AutoTokenizer', use_cuda=False, precision='float16', quantization=0, device_map='auto', max_memory={0: '24GB'}, torchscript=False, compile=False, awq_enabled=False, flash_attention=False, batch_size=32, max_length=512, notification_email=None, **kwargs)</code>","text":"<p>Perform bulk summarization using the specified model and tokenizer. This method handles the entire summarization process including loading the model, processing input data, generating summarization, and saving the results.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name or path of the translation model.</p> required <code>origin</code> <code>str</code> <p>Source language ISO code.</p> required <code>target</code> <code>str</code> <p>Target language ISO code.</p> required <code>max_length</code> <code>int</code> <p>Maximum length of the tokens (default 512).</p> <code>512</code> <code>model_class</code> <code>str</code> <p>Class name of the model (default \"AutoModelForSeq2SeqLM\").</p> <code>'AutoModelForSeq2SeqLM'</code> <code>tokenizer_class</code> <code>str</code> <p>Class name of the tokenizer (default \"AutoTokenizer\").</p> <code>'AutoTokenizer'</code> <code>use_cuda</code> <code>bool</code> <p>Whether to use CUDA for model inference (default False).</p> <code>False</code> <code>precision</code> <code>str</code> <p>Precision for model computation (default \"float16\").</p> <code>'float16'</code> <code>quantization</code> <code>int</code> <p>Level of quantization for optimizing model size and speed (default 0).</p> <code>0</code> <code>device_map</code> <code>str | Dict | None</code> <p>Specific device to use for computation (default \"auto\").</p> <code>'auto'</code> <code>max_memory</code> <code>Dict</code> <p>Maximum memory configuration for devices.</p> <code>{0: '24GB'}</code> <code>torchscript</code> <code>bool</code> <p>Whether to use a TorchScript-optimized version of the pre-trained language model. Defaults to False.</p> <code>False</code> <code>compile</code> <code>bool</code> <p>Whether to compile the model before fine-tuning. Defaults to True.</p> <code>False</code> <code>awq_enabled</code> <code>bool</code> <p>Whether to enable AWQ optimization (default False).</p> <code>False</code> <code>flash_attention</code> <code>bool</code> <p>Whether to use flash attention optimization (default False).</p> <code>False</code> <code>batch_size</code> <code>int</code> <p>Number of translations to process simultaneously (default 32).</p> <code>32</code> <code>max_length</code> <code>int</code> <p>Maximum lenght of the summary to be generated (default 512).</p> <code>512</code> <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments for model and generation configurations.</p> <code>{}</code>"},{"location":"text/bulk/translation/","title":"Translation","text":"<p>             Bases: <code>TextBulk</code></p> <p>TranslationBulk is a class for managing bulk translations using Hugging Face models. It is designed to handle large-scale translation tasks efficiently and effectively, using state-of-the-art machine learning models to provide high-quality translations for various language pairs.</p> <p>This class provides methods for loading datasets, configuring translation models, and executing bulk translation tasks.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>Configuration and data inputs for batch processing.</p> required <code>output</code> <code>BatchOutput</code> <p>Configuration for output data handling.</p> required <code>state</code> <code>State</code> <p>State management for translation tasks.</p> required <code>**kwargs</code> <p>Arbitrary keyword arguments for extended functionality.</p> <code>{}</code> <p>Example CLI Usage for Bulk Translation Task:</p> <pre><code>genius TranslationBulk rise \\\nbatch \\\n--input_s3_bucket geniusrise-test \\\n--input_s3_folder input/trans \\\nbatch \\\n--output_s3_bucket geniusrise-test \\\n--output_s3_folder output/trans \\\npostgres \\\n--postgres_host 127.0.0.1 \\\n--postgres_port 5432 \\\n--postgres_user postgres \\\n--postgres_password postgres \\\n--postgres_database geniusrise\\\n--postgres_table state \\\n--id facebook/mbart-large-50-many-to-many-mmt-lol \\\ntranslate \\\n--args \\\nmodel_name=\"facebook/mbart-large-50-many-to-many-mmt\" \\\nmodel_class=\"AutoModelForSeq2SeqLM\" \\\ntokenizer_class=\"AutoTokenizer\" \\\norigin=\"hi_IN\" \\\ntarget=\"en_XX\" \\\nuse_cuda=True \\\nprecision=\"float\" \\\nquantization=0 \\\ndevice_map=\"cuda:0\" \\\nmax_memory=None \\\ntorchscript=False \\\ngenerate_decoder_start_token_id=2 \\\ngenerate_early_stopping=true \\\ngenerate_eos_token_id=2 \\\ngenerate_forced_eos_token_id=2 \\\ngenerate_max_length=200 \\\ngenerate_num_beams=5 \\\ngenerate_pad_token_id=1\n</code></pre>"},{"location":"text/bulk/translation/#geniusrise_text.translation.bulk.TranslationBulk.load_dataset","title":"<code>load_dataset(dataset_path, max_length=512, origin='en', target='hi', **kwargs)</code>","text":"<p>Load a dataset from a directory.</p>"},{"location":"text/bulk/translation/#geniusrise_text.translation.bulk.TranslationBulk.load_dataset--supported-data-formats-and-structures-for-translation-tasks","title":"Supported Data Formats and Structures for Translation Tasks:","text":"<p>Note: All examples are assuming the source as \"en\", refer to the specific model for this parameter.</p>"},{"location":"text/bulk/translation/#geniusrise_text.translation.bulk.TranslationBulk.load_dataset--jsonl","title":"JSONL","text":"<p>Each line is a JSON object representing an example. <pre><code>{\n\"translation\": {\n\"en\": \"English text\"\n}\n}\n</code></pre></p>"},{"location":"text/bulk/translation/#geniusrise_text.translation.bulk.TranslationBulk.load_dataset--csv","title":"CSV","text":"<p>Should contain 'en' column. <pre><code>en\n\"English text\"\n</code></pre></p>"},{"location":"text/bulk/translation/#geniusrise_text.translation.bulk.TranslationBulk.load_dataset--parquet","title":"Parquet","text":"<p>Should contain 'en' column.</p>"},{"location":"text/bulk/translation/#geniusrise_text.translation.bulk.TranslationBulk.load_dataset--json","title":"JSON","text":"<p>An array of dictionaries with 'en' key. <pre><code>[\n{\n\"en\": \"English text\"\n}\n]\n</code></pre></p>"},{"location":"text/bulk/translation/#geniusrise_text.translation.bulk.TranslationBulk.load_dataset--xml","title":"XML","text":"<p>Each 'record' element should contain 'en' child elements. <pre><code>&lt;record&gt;\n&lt;en&gt;English text&lt;/en&gt;\n&lt;/record&gt;\n</code></pre></p>"},{"location":"text/bulk/translation/#geniusrise_text.translation.bulk.TranslationBulk.load_dataset--yaml","title":"YAML","text":"<p>Each document should be a dictionary with 'en' key. <pre><code>- en: \"English text\"\n</code></pre></p>"},{"location":"text/bulk/translation/#geniusrise_text.translation.bulk.TranslationBulk.load_dataset--tsv","title":"TSV","text":"<p>Should contain 'en' column separated by tabs.</p>"},{"location":"text/bulk/translation/#geniusrise_text.translation.bulk.TranslationBulk.load_dataset--excel-xls-xlsx","title":"Excel (.xls, .xlsx)","text":"<p>Should contain 'en' column.</p>"},{"location":"text/bulk/translation/#geniusrise_text.translation.bulk.TranslationBulk.load_dataset--sqlite-db","title":"SQLite (.db)","text":"<p>Should contain a table with 'en' column.</p>"},{"location":"text/bulk/translation/#geniusrise_text.translation.bulk.TranslationBulk.load_dataset--feather","title":"Feather","text":"<p>Should contain 'en' column.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the directory containing the dataset files.</p> required <code>max_length</code> <code>int</code> <p>The maximum length for tokenization. Defaults to 512.</p> <code>512</code> <code>origin</code> <code>str</code> <p>The origin language. Defaults to 'en'.</p> <code>'en'</code> <code>target</code> <code>str</code> <p>The target language. Defaults to 'hi'.</p> <code>'hi'</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>DatasetDict</code> <code>Optional[Dataset]</code> <p>The loaded dataset.</p>"},{"location":"text/bulk/translation/#geniusrise_text.translation.bulk.TranslationBulk.translate","title":"<code>translate(model_name, origin, target, max_length=512, model_class='AutoModelForSeq2SeqLM', tokenizer_class='AutoTokenizer', use_cuda=False, precision='float16', quantization=0, device_map='auto', max_memory={0: '24GB'}, torchscript=False, compile=False, awq_enabled=False, flash_attention=False, batch_size=32, notification_email=None, **kwargs)</code>","text":"<p>Perform bulk translation using the specified model and tokenizer. This method handles the entire translation process including loading the model, processing input data, generating translations, and saving the results.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name or path of the translation model.</p> required <code>origin</code> <code>str</code> <p>Source language ISO code.</p> required <code>target</code> <code>str</code> <p>Target language ISO code.</p> required <code>max_length</code> <code>int</code> <p>Maximum length of the tokens (default 512).</p> <code>512</code> <code>model_class</code> <code>str</code> <p>Class name of the model (default \"AutoModelForSeq2SeqLM\").</p> <code>'AutoModelForSeq2SeqLM'</code> <code>tokenizer_class</code> <code>str</code> <p>Class name of the tokenizer (default \"AutoTokenizer\").</p> <code>'AutoTokenizer'</code> <code>use_cuda</code> <code>bool</code> <p>Whether to use CUDA for model inference (default False).</p> <code>False</code> <code>precision</code> <code>str</code> <p>Precision for model computation (default \"float16\").</p> <code>'float16'</code> <code>quantization</code> <code>int</code> <p>Level of quantization for optimizing model size and speed (default 0).</p> <code>0</code> <code>device_map</code> <code>str | Dict | None</code> <p>Specific device to use for computation (default \"auto\").</p> <code>'auto'</code> <code>max_memory</code> <code>Dict</code> <p>Maximum memory configuration for devices.</p> <code>{0: '24GB'}</code> <code>torchscript</code> <code>bool</code> <p>Whether to use a TorchScript-optimized version of the pre-trained language model. Defaults to False.</p> <code>False</code> <code>compile</code> <code>bool</code> <p>Whether to compile the model before fine-tuning. Defaults to True.</p> <code>False</code> <code>awq_enabled</code> <code>bool</code> <p>Whether to enable AWQ optimization (default False).</p> <code>False</code> <code>flash_attention</code> <code>bool</code> <p>Whether to use flash attention optimization (default False).</p> <code>False</code> <code>batch_size</code> <code>int</code> <p>Number of translations to process simultaneously (default 32).</p> <code>32</code> <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments for model and generation configurations.</p> <code>{}</code>"},{"location":"text/fine_tune/base/","title":"Base Fine Tuner","text":"<p>             Bases: <code>Bolt</code></p> <p>A bolt for fine-tuning Hugging Face models.</p> <p>This bolt uses the Hugging Face Transformers library to fine-tune a pre-trained model. It uses the <code>Trainer</code> class from the Transformers library to handle the training.</p>"},{"location":"text/fine_tune/base/#geniusrise_text.base.fine_tune.TextFineTuner.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>Initialize the bolt.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>The batch input data.</p> required <code>output</code> <code>BatchOutput</code> <p>The output data.</p> required <code>state</code> <code>State</code> <p>The state manager.</p> required <code>evaluate</code> <code>bool</code> <p>Whether to evaluate the model. Defaults to False.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"text/fine_tune/base/#geniusrise_text.base.fine_tune.TextFineTuner.compute_metrics","title":"<code>compute_metrics(eval_pred)</code>","text":"<p>Compute metrics for evaluation. This class implements a simple classification evaluation, tasks should ideally override this.</p> <p>Parameters:</p> Name Type Description Default <code>eval_pred</code> <code>EvalPrediction</code> <p>The evaluation predictions.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[Dict[str, float]] | Dict[str, float]</code> <p>The computed metrics.</p>"},{"location":"text/fine_tune/base/#geniusrise_text.base.fine_tune.TextFineTuner.fine_tune","title":"<code>fine_tune(model_name, tokenizer_name, num_train_epochs, per_device_batch_size, model_class='AutoModel', tokenizer_class='AutoTokenizer', device_map='auto', precision='bfloat16', quantization=None, lora_config=None, use_accelerate=False, use_trl=False, accelerate_no_split_module_classes=[], compile=False, evaluate=False, save_steps=500, save_total_limit=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, map_data=None, use_huggingface_dataset=False, huggingface_dataset='', hf_repo_id=None, hf_commit_message=None, hf_token=None, hf_private=True, hf_create_pr=False, notification_email='', learning_rate=1e-05, **kwargs)</code>","text":"<p>Fine-tunes a pre-trained Hugging Face model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the pre-trained model.</p> required <code>tokenizer_name</code> <code>str</code> <p>The name of the pre-trained tokenizer.</p> required <code>num_train_epochs</code> <code>int</code> <p>The total number of training epochs to perform.</p> required <code>per_device_batch_size</code> <code>int</code> <p>The batch size per device during training.</p> required <code>model_class</code> <code>str</code> <p>The model class to use. Defaults to \"AutoModel\".</p> <code>'AutoModel'</code> <code>tokenizer_class</code> <code>str</code> <p>The tokenizer class to use. Defaults to \"AutoTokenizer\".</p> <code>'AutoTokenizer'</code> <code>device_map</code> <code>str | dict</code> <p>The device map for distributed training. Defaults to \"auto\".</p> <code>'auto'</code> <code>precision</code> <code>str</code> <p>The precision to use for training. Defaults to \"bfloat16\".</p> <code>'bfloat16'</code> <code>quantization</code> <code>int</code> <p>The quantization level to use for training. Defaults to None.</p> <code>None</code> <code>lora_config</code> <code>dict</code> <p>Configuration for PEFT LoRA optimization. Defaults to None.</p> <code>None</code> <code>use_accelerate</code> <code>bool</code> <p>Whether to use accelerate for distributed training. Defaults to False.</p> <code>False</code> <code>use_trl</code> <code>bool</code> <p>Whether to use TRL for training. Defaults to False.</p> <code>False</code> <code>accelerate_no_split_module_classes</code> <code>List[str]</code> <p>The module classes to not split during distributed training. Defaults to [].</p> <code>[]</code> <code>evaluate</code> <code>bool</code> <p>Whether to evaluate the model after training. Defaults to False.</p> <code>False</code> <code>compile</code> <code>bool</code> <p>Whether to compile the model before fine-tuning. Defaults to True.</p> <code>False</code> <code>save_steps</code> <code>int</code> <p>Number of steps between checkpoints. Defaults to 500.</p> <code>500</code> <code>save_total_limit</code> <code>Optional[int]</code> <p>Maximum number of checkpoints to keep. Older checkpoints are deleted. Defaults to None.</p> <code>None</code> <code>load_best_model_at_end</code> <code>bool</code> <p>Whether to load the best model (according to evaluation) at the end of training. Defaults to False.</p> <code>False</code> <code>metric_for_best_model</code> <code>Optional[str]</code> <p>The metric to use to compare models. Defaults to None.</p> <code>None</code> <code>greater_is_better</code> <code>Optional[bool]</code> <p>Whether a larger value of the metric indicates a better model. Defaults to None.</p> <code>None</code> <code>use_huggingface_dataset</code> <code>bool</code> <p>Whether to load a dataset from huggingface hub.</p> <code>False</code> <code>huggingface_dataset</code> <code>str</code> <p>The huggingface dataset to use.</p> <code>''</code> <code>map_data</code> <code>Callable</code> <p>A function to map data before training. Defaults to None.</p> <code>None</code> <code>hf_repo_id</code> <code>str</code> <p>The Hugging Face repo ID. Defaults to None.</p> <code>None</code> <code>hf_commit_message</code> <code>str</code> <p>The Hugging Face commit message. Defaults to None.</p> <code>None</code> <code>hf_token</code> <code>str</code> <p>The Hugging Face token. Defaults to None.</p> <code>None</code> <code>hf_private</code> <code>bool</code> <p>Whether to make the repo private. Defaults to True.</p> <code>True</code> <code>hf_create_pr</code> <code>bool</code> <p>Whether to create a pull request. Defaults to False.</p> <code>False</code> <code>notification_email</code> <code>str</code> <p>Whether to notify after job is complete. Defaults to None.</p> <code>''</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for backpropagation.</p> <code>1e-05</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <p>None</p>"},{"location":"text/fine_tune/base/#geniusrise_text.base.fine_tune.TextFineTuner.load_dataset","title":"<code>load_dataset(dataset_path, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Load a dataset from a file.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the dataset file.</p> required <code>split</code> <code>str</code> <p>The split to load. Defaults to None.</p> required <code>**kwargs</code> <p>Additional keyword arguments to pass to the <code>load_dataset</code> method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dataset | DatasetDict | Optional[Dataset]</code> <p>Union[Dataset, DatasetDict, None]: The loaded dataset.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method should be overridden by subclasses.</p>"},{"location":"text/fine_tune/base/#geniusrise_text.base.fine_tune.TextFineTuner.load_models","title":"<code>load_models(model_name, tokenizer_name, model_class='AutoModel', tokenizer_class='AutoTokenizer', device_map='auto', precision='bfloat16', quantization=None, lora_config=None, use_accelerate=False, accelerate_no_split_module_classes=[], **kwargs)</code>","text":"<p>Load the model and tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model to be loaded.</p> required <code>tokenizer_name</code> <code>str</code> <p>The name of the tokenizer to be loaded. Defaults to None.</p> required <code>model_class</code> <code>str</code> <p>The class of the model. Defaults to \"AutoModel\".</p> <code>'AutoModel'</code> <code>tokenizer_class</code> <code>str</code> <p>The class of the tokenizer. Defaults to \"AutoTokenizer\".</p> <code>'AutoTokenizer'</code> <code>device</code> <code>Union[str, torch.device]</code> <p>The device to be used. Defaults to \"cuda\".</p> required <code>precision</code> <code>str</code> <p>The precision to be used. Choose from 'float32', 'float16', 'bfloat16'. Defaults to \"float32\".</p> <code>'bfloat16'</code> <code>quantization</code> <code>Optional[int]</code> <p>The quantization to be used. Defaults to None.</p> <code>None</code> <code>lora_config</code> <code>Optional[dict]</code> <p>The LoRA configuration to be used. Defaults to None.</p> <code>None</code> <code>use_accelerate</code> <code>bool</code> <p>Whether to use accelerate. Defaults to False.</p> <code>False</code> <code>accelerate_no_split_module_classes</code> <code>List[str]</code> <p>The list of no split module classes to be used. Defaults to [].</p> <code>[]</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported precision is chosen.</p> <p>Returns:</p> Type Description <p>None</p>"},{"location":"text/fine_tune/base/#geniusrise_text.base.fine_tune.TextFineTuner.preprocess_data","title":"<code>preprocess_data(**kwargs)</code>","text":"<p>Load and preprocess the dataset</p>"},{"location":"text/fine_tune/base/#geniusrise_text.base.fine_tune.TextFineTuner.upload_to_hf_hub","title":"<code>upload_to_hf_hub(hf_repo_id=None, hf_commit_message=None, hf_token=None, hf_private=None, hf_create_pr=None)</code>","text":"<p>Upload the model and tokenizer to Hugging Face Hub.</p>"},{"location":"text/fine_tune/classification/","title":"Classification","text":"<p>             Bases: <code>TextFineTuner</code></p> <p>A bolt for fine-tuning Hugging Face models for text classification tasks.</p> <p>This class extends the <code>TextFineTuner</code> and specializes in fine-tuning models for text classification. It provides additional functionalities for loading and preprocessing text classification datasets in various formats.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>The batch input data.</p> required <code>output</code> <code>OutputConfig</code> <p>The output data.</p> required <code>state</code> <code>State</code> <p>The state manager.</p> required <p>CLI Usage:</p> <pre><code>genius TextClassificationFineTuner rise \\\nbatch \\\n--input_folder ./input \\\nbatch \\\n--output_folder ./output \\\nnone \\\n--id cardiffnlp/twitter-roberta-base-hate-multiclass-latest-lol \\\nfine_tune \\\n--args \\\nmodel_name=my_model \\\ntokenizer_name=my_tokenizer \\\nnum_train_epochs=3 \\\nper_device_train_batch_size=8 \\\ndata_max_length=512\n</code></pre>"},{"location":"text/fine_tune/classification/#geniusrise_text.classification.fine_tune.TextClassificationFineTuner.compute_metrics","title":"<code>compute_metrics(eval_pred)</code>","text":"<p>Compute metrics for evaluation. This class implements a simple classification evaluation, tasks should ideally override this.</p> <p>Parameters:</p> Name Type Description Default <code>eval_pred</code> <code>EvalPrediction</code> <p>The evaluation predictions.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Union[Optional[Dict[str, float]], Dict[str, float]]</code> <p>The computed metrics.</p>"},{"location":"text/fine_tune/classification/#geniusrise_text.classification.fine_tune.TextClassificationFineTuner.load_dataset","title":"<code>load_dataset(dataset_path, max_length=512, **kwargs)</code>","text":"<p>Load a classification dataset from a directory.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the dataset directory.</p> required <code>max_length</code> <code>int</code> <p>The maximum length for tokenization. Defaults to 512.</p> <code>512</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Optional[Dataset]</code> <p>The loaded dataset.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there was an error loading the dataset.</p>"},{"location":"text/fine_tune/classification/#geniusrise_text.classification.fine_tune.TextClassificationFineTuner.load_dataset--supported-data-formats-and-structures","title":"Supported Data Formats and Structures:","text":""},{"location":"text/fine_tune/classification/#geniusrise_text.classification.fine_tune.TextClassificationFineTuner.load_dataset--jsonl","title":"JSONL","text":"<p>Each line is a JSON object representing an example. <pre><code>{\"text\": \"The text content\", \"label\": \"The label\"}\n</code></pre></p>"},{"location":"text/fine_tune/classification/#geniusrise_text.classification.fine_tune.TextClassificationFineTuner.load_dataset--csv","title":"CSV","text":"<p>Should contain 'text' and 'label' columns. <pre><code>text,label\n\"The text content\",\"The label\"\n</code></pre></p>"},{"location":"text/fine_tune/classification/#geniusrise_text.classification.fine_tune.TextClassificationFineTuner.load_dataset--parquet","title":"Parquet","text":"<p>Should contain 'text' and 'label' columns.</p>"},{"location":"text/fine_tune/classification/#geniusrise_text.classification.fine_tune.TextClassificationFineTuner.load_dataset--json","title":"JSON","text":"<p>An array of dictionaries with 'text' and 'label' keys. <pre><code>[{\"text\": \"The text content\", \"label\": \"The label\"}]\n</code></pre></p>"},{"location":"text/fine_tune/classification/#geniusrise_text.classification.fine_tune.TextClassificationFineTuner.load_dataset--xml","title":"XML","text":"<p>Each 'record' element should contain 'text' and 'label' child elements. <pre><code>&lt;record&gt;\n&lt;text&gt;The text content&lt;/text&gt;\n&lt;label&gt;The label&lt;/label&gt;\n&lt;/record&gt;\n</code></pre></p>"},{"location":"text/fine_tune/classification/#geniusrise_text.classification.fine_tune.TextClassificationFineTuner.load_dataset--yaml","title":"YAML","text":"<p>Each document should be a dictionary with 'text' and 'label' keys. <pre><code>- text: \"The text content\"\nlabel: \"The label\"\n</code></pre></p>"},{"location":"text/fine_tune/classification/#geniusrise_text.classification.fine_tune.TextClassificationFineTuner.load_dataset--tsv","title":"TSV","text":"<p>Should contain 'text' and 'label' columns separated by tabs.</p>"},{"location":"text/fine_tune/classification/#geniusrise_text.classification.fine_tune.TextClassificationFineTuner.load_dataset--excel-xls-xlsx","title":"Excel (.xls, .xlsx)","text":"<p>Should contain 'text' and 'label' columns.</p>"},{"location":"text/fine_tune/classification/#geniusrise_text.classification.fine_tune.TextClassificationFineTuner.load_dataset--sqlite-db","title":"SQLite (.db)","text":"<p>Should contain a table with 'text' and 'label' columns.</p>"},{"location":"text/fine_tune/classification/#geniusrise_text.classification.fine_tune.TextClassificationFineTuner.load_dataset--feather","title":"Feather","text":"<p>Should contain 'text' and 'label' columns.</p>"},{"location":"text/fine_tune/instruction_tuning/","title":"Instruction Tuning","text":"<p>             Bases: <code>TextFineTuner</code></p> <p>A bolt for fine-tuning Hugging Face models on instruction tuning tasks.</p> <p>This class inherits from <code>TextFineTuner</code> and specializes in fine-tuning models for instruction-based tasks. It provides additional methods for loading and preparing datasets in various formats, as well as computing custom metrics.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>The batch input data.</p> required <code>output</code> <code>OutputConfig</code> <p>The output data.</p> required <code>state</code> <code>State</code> <p>The state manager.</p> required <p>Attributes:</p> Name Type Description <code>max_length</code> <code>int</code> <p>The maximum length for tokenization.</p> <p>CLI Usage:</p> <pre><code>    genius InstructionFineTuner rise \\\nbatch \\\n--input_folder ./input \\\nbatch \\\n--output_folder ./output \\\nnone \\\n--id mistralai/Mistral-7B-Instruct-v0.1-lol \\\nfine_tune \\\n--args \\\nmodel_name=my_model \\\ntokenizer_name=my_tokenizer \\\nnum_train_epochs=3 \\\nper_device_train_batch_size=8 \\\ndata_max_length=512\n</code></pre>"},{"location":"text/fine_tune/instruction_tuning/#geniusrise_text.instruction.fine_tune.InstructionFineTuner.compute_metrics","title":"<code>compute_metrics(eval_pred)</code>","text":"<p>Compute evaluation metrics for the model's predictions.</p> <p>This method takes the model's predictions and ground truth labels, converts them to text, and then computes the BLEU score for evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>eval_pred</code> <code>EvalPrediction</code> <p>A named tuple containing <code>predictions</code> and <code>label_ids</code>. - <code>predictions</code>: The logits predicted by the model of shape (batch_size, sequence_length, num_classes). - <code>label_ids</code>: The ground truth labels of shape (batch_size, sequence_length).</p> required <p>Returns:</p> Type Description <code>Optional[Dict[str, float]]</code> <p>Optional[Dict[str, float]]: A dictionary containing the BLEU score. Returns None if an exception occurs.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the tokenizer is not initialized.</p>"},{"location":"text/fine_tune/instruction_tuning/#geniusrise_text.instruction.fine_tune.InstructionFineTuner.load_dataset","title":"<code>load_dataset(dataset_path, max_length=512, **kwargs)</code>","text":"<p>Load an instruction tuning dataset from a directory.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the dataset directory.</p> required <code>max_length</code> <code>int</code> <p>The maximum length for tokenization. Defaults to 512.</p> <code>512</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Union[Dataset, Dict]</code> <p>The loaded dataset.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there was an error loading the dataset.</p>"},{"location":"text/fine_tune/instruction_tuning/#geniusrise_text.instruction.fine_tune.InstructionFineTuner.load_dataset--supported-data-formats-and-structures","title":"Supported Data Formats and Structures:","text":""},{"location":"text/fine_tune/instruction_tuning/#geniusrise_text.instruction.fine_tune.InstructionFineTuner.load_dataset--jsonl","title":"JSONL","text":"<p>Each line is a JSON object representing an example. <pre><code>{\"instruction\": \"The instruction\", \"output\": \"The output\"}\n</code></pre></p>"},{"location":"text/fine_tune/instruction_tuning/#geniusrise_text.instruction.fine_tune.InstructionFineTuner.load_dataset--csv","title":"CSV","text":"<p>Should contain 'instruction' and 'output' columns. <pre><code>instruction,output\n\"The instruction\",\"The output\"\n</code></pre></p>"},{"location":"text/fine_tune/instruction_tuning/#geniusrise_text.instruction.fine_tune.InstructionFineTuner.load_dataset--parquet","title":"Parquet","text":"<p>Should contain 'instruction' and 'output' columns.</p>"},{"location":"text/fine_tune/instruction_tuning/#geniusrise_text.instruction.fine_tune.InstructionFineTuner.load_dataset--json","title":"JSON","text":"<p>An array of dictionaries with 'instruction' and 'output' keys. <pre><code>[{\"instruction\": \"The instruction\", \"output\": \"The output\"}]\n</code></pre></p>"},{"location":"text/fine_tune/instruction_tuning/#geniusrise_text.instruction.fine_tune.InstructionFineTuner.load_dataset--xml","title":"XML","text":"<p>Each 'record' element should contain 'instruction' and 'output' child elements. <pre><code>&lt;record&gt;\n&lt;instruction&gt;The instruction&lt;/instruction&gt;\n&lt;output&gt;The output&lt;/output&gt;\n&lt;/record&gt;\n</code></pre></p>"},{"location":"text/fine_tune/instruction_tuning/#geniusrise_text.instruction.fine_tune.InstructionFineTuner.load_dataset--yaml","title":"YAML","text":"<p>Each document should be a dictionary with 'instruction' and 'output' keys. <pre><code>- instruction: \"The instruction\"\noutput: \"The output\"\n</code></pre></p>"},{"location":"text/fine_tune/instruction_tuning/#geniusrise_text.instruction.fine_tune.InstructionFineTuner.load_dataset--tsv","title":"TSV","text":"<p>Should contain 'instruction' and 'output' columns separated by tabs.</p>"},{"location":"text/fine_tune/instruction_tuning/#geniusrise_text.instruction.fine_tune.InstructionFineTuner.load_dataset--excel-xls-xlsx","title":"Excel (.xls, .xlsx)","text":"<p>Should contain 'instruction' and 'output' columns.</p>"},{"location":"text/fine_tune/instruction_tuning/#geniusrise_text.instruction.fine_tune.InstructionFineTuner.load_dataset--sqlite-db","title":"SQLite (.db)","text":"<p>Should contain a table with 'instruction' and 'output' columns.</p>"},{"location":"text/fine_tune/instruction_tuning/#geniusrise_text.instruction.fine_tune.InstructionFineTuner.load_dataset--feather","title":"Feather","text":"<p>Should contain 'instruction' and 'output' columns.</p>"},{"location":"text/fine_tune/instruction_tuning/#geniusrise_text.instruction.fine_tune.InstructionFineTuner.prepare_train_features","title":"<code>prepare_train_features(examples)</code>","text":"<p>Tokenize the examples and prepare the features for training.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>dict</code> <p>A dictionary of examples.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict</code> <p>The processed features.</p>"},{"location":"text/fine_tune/language_model/","title":"Language Model","text":"<p>             Bases: <code>TextFineTuner</code></p> <p>A bolt for fine-tuning Hugging Face models on language modeling tasks.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>The batch input data.</p> required <code>output</code> <code>OutputConfig</code> <p>The output data.</p> required <code>state</code> <code>State</code> <p>The state manager.</p> required <p>CLI Usage:</p> <pre><code>    genius LanguageModelFineTuner rise \\\nbatch \\\n--input_s3_bucket geniusrise-test \\\n--input_s3_folder input/lm \\\nbatch \\\n--output_s3_bucket geniusrise-test \\\n--output_s3_folder output/lm \\\npostgres \\\n--postgres_host 127.0.0.1 \\\n--postgres_port 5432 \\\n--postgres_user postgres \\\n--postgres_password postgres \\\n--postgres_database geniusrise\\\n--postgres_table state \\\n--id mistralai/Mistral-7B-Instruct-v0.1-lol \\\nfine_tune \\\n--args \\\nmodel_name=my_model \\\ntokenizer_name=my_tokenizer \\\nnum_train_epochs=3 \\\nper_device_train_batch_size=8 \\\ndata_max_length=512\n</code></pre>"},{"location":"text/fine_tune/language_model/#geniusrise_text.language_model.fine_tune.LanguageModelFineTuner.compute_metrics","title":"<code>compute_metrics(eval_pred)</code>","text":"<p>Compute evaluation metrics for the model's predictions.</p> <p>This method takes the model's predictions and ground truth labels, converts them to text, and then computes the BLEU score for evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>eval_pred</code> <code>EvalPrediction</code> <p>A named tuple containing <code>predictions</code> and <code>label_ids</code>. - <code>predictions</code>: The logits predicted by the model of shape (batch_size, sequence_length, num_classes). - <code>label_ids</code>: The ground truth labels of shape (batch_size, sequence_length).</p> required <p>Returns:</p> Type Description <code>Optional[Dict[str, float]]</code> <p>Optional[Dict[str, float]]: A dictionary containing the BLEU score. Returns None if an exception occurs.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the tokenizer is not initialized.</p>"},{"location":"text/fine_tune/language_model/#geniusrise_text.language_model.fine_tune.LanguageModelFineTuner.data_collator","title":"<code>data_collator(examples)</code>","text":"<p>Customize the data collator.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <p>The examples to collate.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>The collated data.</p>"},{"location":"text/fine_tune/language_model/#geniusrise_text.language_model.fine_tune.LanguageModelFineTuner.load_dataset","title":"<code>load_dataset(dataset_path, masked=False, max_length=512, **kwargs)</code>","text":"<p>Load a language modeling dataset from a directory.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the dataset directory.</p> required <code>masked</code> <code>bool</code> <p>Whether to use masked language modeling. Defaults to True.</p> <code>False</code> <code>max_length</code> <code>int</code> <p>The maximum length for tokenization. Defaults to 512.</p> <code>512</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <p>The loaded dataset.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there was an error loading the dataset.</p>"},{"location":"text/fine_tune/language_model/#geniusrise_text.language_model.fine_tune.LanguageModelFineTuner.load_dataset--supported-data-formats-and-structures","title":"Supported Data Formats and Structures:","text":""},{"location":"text/fine_tune/language_model/#geniusrise_text.language_model.fine_tune.LanguageModelFineTuner.load_dataset--dataset-files-saved-by-hugging-face-datasets-library","title":"Dataset files saved by Hugging Face datasets library","text":"<p>The directory should contain 'dataset_info.json' and other related files.</p>"},{"location":"text/fine_tune/language_model/#geniusrise_text.language_model.fine_tune.LanguageModelFineTuner.load_dataset--jsonl","title":"JSONL","text":"<p>Each line is a JSON object representing an example. <pre><code>{\"text\": \"The text content\"}\n</code></pre></p>"},{"location":"text/fine_tune/language_model/#geniusrise_text.language_model.fine_tune.LanguageModelFineTuner.load_dataset--csv","title":"CSV","text":"<p>Should contain 'text' column. <pre><code>text\n\"The text content\"\n</code></pre></p>"},{"location":"text/fine_tune/language_model/#geniusrise_text.language_model.fine_tune.LanguageModelFineTuner.load_dataset--parquet","title":"Parquet","text":"<p>Should contain 'text' column.</p>"},{"location":"text/fine_tune/language_model/#geniusrise_text.language_model.fine_tune.LanguageModelFineTuner.load_dataset--json","title":"JSON","text":"<p>An array of dictionaries with 'text' key. <pre><code>[{\"text\": \"The text content\"}]\n</code></pre></p>"},{"location":"text/fine_tune/language_model/#geniusrise_text.language_model.fine_tune.LanguageModelFineTuner.load_dataset--xml","title":"XML","text":"<p>Each 'record' element should contain 'text' child element. <pre><code>&lt;record&gt;\n&lt;text&gt;The text content&lt;/text&gt;\n&lt;/record&gt;\n</code></pre></p>"},{"location":"text/fine_tune/language_model/#geniusrise_text.language_model.fine_tune.LanguageModelFineTuner.load_dataset--yaml","title":"YAML","text":"<p>Each document should be a dictionary with 'text' key. <pre><code>- text: \"The text content\"\n</code></pre></p>"},{"location":"text/fine_tune/language_model/#geniusrise_text.language_model.fine_tune.LanguageModelFineTuner.load_dataset--tsv","title":"TSV","text":"<p>Should contain 'text' column separated by tabs.</p>"},{"location":"text/fine_tune/language_model/#geniusrise_text.language_model.fine_tune.LanguageModelFineTuner.load_dataset--excel-xls-xlsx","title":"Excel (.xls, .xlsx)","text":"<p>Should contain 'text' column.</p>"},{"location":"text/fine_tune/language_model/#geniusrise_text.language_model.fine_tune.LanguageModelFineTuner.load_dataset--sqlite-db","title":"SQLite (.db)","text":"<p>Should contain a table with 'text' column.</p>"},{"location":"text/fine_tune/language_model/#geniusrise_text.language_model.fine_tune.LanguageModelFineTuner.load_dataset--feather","title":"Feather","text":"<p>Should contain 'text' column.</p>"},{"location":"text/fine_tune/language_model/#geniusrise_text.language_model.fine_tune.LanguageModelFineTuner.prepare_train_features","title":"<code>prepare_train_features(examples)</code>","text":"<p>Tokenize the examples and prepare the features for training.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>dict</code> <p>A dictionary of examples.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>The processed features.</p>"},{"location":"text/fine_tune/ner/","title":"Named Entity Recognition","text":"<p>             Bases: <code>TextFineTuner</code></p> <p>A bolt for fine-tuning Hugging Face models on named entity recognition tasks.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>The batch input data.</p> required <code>output</code> <code>OutputConfig</code> <p>The output data.</p> required <code>state</code> <code>State</code> <p>The state manager.</p> required <p>CLI Usage:</p> <pre><code>    genius NamedEntityRecognitionFineTuner rise \\\nbatch \\\n--input_folder ./input \\\nbatch \\\n--output_folder ./output \\\nnone \\\n--id dslim/bert-large-NER-lol \\\nfine_tune \\\n--args \\\nmodel_name=my_model \\\ntokenizer_name=my_tokenizer \\\nnum_train_epochs=3 \\\nper_device_train_batch_size=8\n</code></pre>"},{"location":"text/fine_tune/ner/#geniusrise_text.ner.fine_tune.NamedEntityRecognitionFineTuner.data_collator","title":"<code>data_collator(examples)</code>","text":"<p>Customize the data collator.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>List[Dict[str, torch.Tensor]]</code> <p>The examples to collate.</p> required <p>Returns:</p> Type Description <code>Dict[str, torch.Tensor]</code> <p>Dict[str, torch.Tensor]: The collated data.</p>"},{"location":"text/fine_tune/ner/#geniusrise_text.ner.fine_tune.NamedEntityRecognitionFineTuner.load_dataset","title":"<code>load_dataset(dataset_path, label_list=[], **kwargs)</code>","text":"<p>Load a named entity recognition dataset from a directory.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the dataset directory.</p> required <code>label_list</code> <code>List[str]</code> <p>The list of labels for named entity recognition. Defaults to [].</p> <code>[]</code> <p>Returns:</p> Name Type Description <code>DatasetDict</code> <code>Union[Dataset, DatasetDict, None]</code> <p>The loaded dataset.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there was an error loading the dataset.</p>"},{"location":"text/fine_tune/ner/#geniusrise_text.ner.fine_tune.NamedEntityRecognitionFineTuner.load_dataset--supported-data-formats-and-structures","title":"Supported Data Formats and Structures:","text":""},{"location":"text/fine_tune/ner/#geniusrise_text.ner.fine_tune.NamedEntityRecognitionFineTuner.load_dataset--hugging-face-dataset","title":"Hugging Face Dataset","text":"<p>Dataset files saved by the Hugging Face datasets library.</p>"},{"location":"text/fine_tune/ner/#geniusrise_text.ner.fine_tune.NamedEntityRecognitionFineTuner.load_dataset--jsonl","title":"JSONL","text":"<p>Each line is a JSON object representing an example. <pre><code>{\"tokens\": [\"token1\", \"token2\", ...], \"ner_tags\": [0, 1, ...]}\n</code></pre></p>"},{"location":"text/fine_tune/ner/#geniusrise_text.ner.fine_tune.NamedEntityRecognitionFineTuner.load_dataset--csv","title":"CSV","text":"<p>Should contain 'tokens' and 'ner_tags' columns. <pre><code>tokens,ner_tags\n\"['token1', 'token2', ...]\", \"[0, 1, ...]\"\n</code></pre></p>"},{"location":"text/fine_tune/ner/#geniusrise_text.ner.fine_tune.NamedEntityRecognitionFineTuner.load_dataset--parquet","title":"Parquet","text":"<p>Should contain 'tokens' and 'ner_tags' columns.</p>"},{"location":"text/fine_tune/ner/#geniusrise_text.ner.fine_tune.NamedEntityRecognitionFineTuner.load_dataset--json","title":"JSON","text":"<p>An array of dictionaries with 'tokens' and 'ner_tags' keys. <pre><code>[{\"tokens\": [\"token1\", \"token2\", ...], \"ner_tags\": [0, 1, ...]}]\n</code></pre></p>"},{"location":"text/fine_tune/ner/#geniusrise_text.ner.fine_tune.NamedEntityRecognitionFineTuner.load_dataset--xml","title":"XML","text":"<p>Each 'record' element should contain 'tokens' and 'ner_tags' child elements. <pre><code>&lt;record&gt;\n&lt;tokens&gt;token1 token2 ...&lt;/tokens&gt;\n&lt;ner_tags&gt;0 1 ...&lt;/ner_tags&gt;\n&lt;/record&gt;\n</code></pre></p>"},{"location":"text/fine_tune/ner/#geniusrise_text.ner.fine_tune.NamedEntityRecognitionFineTuner.load_dataset--yaml","title":"YAML","text":"<p>Each document should be a dictionary with 'tokens' and 'ner_tags' keys. <pre><code>- tokens: [\"token1\", \"token2\", ...]\nner_tags: [0, 1, ...]\n</code></pre></p>"},{"location":"text/fine_tune/ner/#geniusrise_text.ner.fine_tune.NamedEntityRecognitionFineTuner.load_dataset--tsv","title":"TSV","text":"<p>Should contain 'tokens' and 'ner_tags' columns separated by tabs.</p>"},{"location":"text/fine_tune/ner/#geniusrise_text.ner.fine_tune.NamedEntityRecognitionFineTuner.load_dataset--excel-xls-xlsx","title":"Excel (.xls, .xlsx)","text":"<p>Should contain 'tokens' and 'ner_tags' columns.</p>"},{"location":"text/fine_tune/ner/#geniusrise_text.ner.fine_tune.NamedEntityRecognitionFineTuner.load_dataset--sqlite-db","title":"SQLite (.db)","text":"<p>Should contain a table with 'tokens' and 'ner_tags' columns.</p>"},{"location":"text/fine_tune/ner/#geniusrise_text.ner.fine_tune.NamedEntityRecognitionFineTuner.load_dataset--feather","title":"Feather","text":"<p>Should contain 'tokens' and 'ner_tags' columns.</p>"},{"location":"text/fine_tune/ner/#geniusrise_text.ner.fine_tune.NamedEntityRecognitionFineTuner.prepare_train_features","title":"<code>prepare_train_features(examples)</code>","text":"<p>Tokenize the examples and prepare the features for training.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>Dict[str, Union[List[str], List[int]]]</code> <p>A dictionary of examples.</p> required <p>Returns:</p> Type Description <code>Dict[str, Union[List[str], List[int]]]</code> <p>Dict[str, Union[List[str], List[int]]]: The processed features.</p>"},{"location":"text/fine_tune/nli/","title":"Natural Language Inference","text":"<p>             Bases: <code>TextFineTuner</code></p> <p>A bolt for fine-tuning Hugging Face models for text classification tasks.</p> <p>This class extends the <code>TextFineTuner</code> and specializes in fine-tuning models for text classification. It provides additional functionalities for loading and preprocessing text classification datasets in various formats.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>The batch input data.</p> required <code>output</code> <code>OutputConfig</code> <p>The output data.</p> required <code>state</code> <code>State</code> <p>The state manager.</p> required <p>CLI Usage:</p> <pre><code>    genius NLIFineTuner rise \\\nbatch \\\n--input_folder ./input \\\nbatch \\\n--output_folder ./output \\\nnone \\\n--id MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7-lol\n        fine_tune \\\n--args \\\nmodel_name=my_model \\\ntokenizer_name=my_tokenizer \\\nnum_train_epochs=3 \\\nper_device_train_batch_size=8\n</code></pre>"},{"location":"text/fine_tune/nli/#geniusrise_text.nli.fine_tune.NLIFineTuner.data_collator","title":"<code>data_collator(examples)</code>","text":"<p>Customize the data collator.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>Dict</code> <p>The examples to collate.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict</code> <p>The collated data.</p>"},{"location":"text/fine_tune/nli/#geniusrise_text.nli.fine_tune.NLIFineTuner.load_dataset","title":"<code>load_dataset(dataset_path, **kwargs)</code>","text":"<p>Load a commonsense reasoning dataset from a directory.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the dataset directory.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Union[Dataset, DatasetDict, None]</code> <p>The loaded dataset.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there was an error loading the dataset.</p>"},{"location":"text/fine_tune/nli/#geniusrise_text.nli.fine_tune.NLIFineTuner.load_dataset--supported-data-formats-and-structures","title":"Supported Data Formats and Structures:","text":""},{"location":"text/fine_tune/nli/#geniusrise_text.nli.fine_tune.NLIFineTuner.load_dataset--hugging-face-dataset","title":"Hugging Face Dataset","text":"<p>Dataset files saved by the Hugging Face datasets library.</p>"},{"location":"text/fine_tune/nli/#geniusrise_text.nli.fine_tune.NLIFineTuner.load_dataset--jsonl","title":"JSONL","text":"<p>Each line is a JSON object representing an example. <pre><code>{\"premise\": \"The premise text\", \"hypothesis\": \"The hypothesis text\", \"label\": 0 or 1 or 2}\n</code></pre></p>"},{"location":"text/fine_tune/nli/#geniusrise_text.nli.fine_tune.NLIFineTuner.load_dataset--csv","title":"CSV","text":"<p>Should contain 'premise', 'hypothesis', and 'label' columns. <pre><code>premise,hypothesis,label\n\"The premise text\",\"The hypothesis text\",0\n</code></pre></p>"},{"location":"text/fine_tune/nli/#geniusrise_text.nli.fine_tune.NLIFineTuner.load_dataset--parquet","title":"Parquet","text":"<p>Should contain 'premise', 'hypothesis', and 'label' columns.</p>"},{"location":"text/fine_tune/nli/#geniusrise_text.nli.fine_tune.NLIFineTuner.load_dataset--json","title":"JSON","text":"<p>An array of dictionaries with 'premise', 'hypothesis', and 'label' keys. <pre><code>[{\"premise\": \"The premise text\", \"hypothesis\": \"The hypothesis text\", \"label\": 0}]\n</code></pre></p>"},{"location":"text/fine_tune/nli/#geniusrise_text.nli.fine_tune.NLIFineTuner.load_dataset--xml","title":"XML","text":"<p>Each 'record' element should contain 'premise', 'hypothesis', and 'label' child elements. <pre><code>&lt;record&gt;\n&lt;premise&gt;The premise text&lt;/premise&gt;\n&lt;hypothesis&gt;The hypothesis text&lt;/hypothesis&gt;\n&lt;label&gt;0&lt;/label&gt;\n&lt;/record&gt;\n</code></pre></p>"},{"location":"text/fine_tune/nli/#geniusrise_text.nli.fine_tune.NLIFineTuner.load_dataset--yaml","title":"YAML","text":"<p>Each document should be a dictionary with 'premise', 'hypothesis', and 'label' keys. <pre><code>- premise: \"The premise text\"\nhypothesis: \"The hypothesis text\"\nlabel: 0\n</code></pre></p>"},{"location":"text/fine_tune/nli/#geniusrise_text.nli.fine_tune.NLIFineTuner.load_dataset--tsv","title":"TSV","text":"<p>Should contain 'premise', 'hypothesis', and 'label' columns separated by tabs.</p>"},{"location":"text/fine_tune/nli/#geniusrise_text.nli.fine_tune.NLIFineTuner.load_dataset--excel-xls-xlsx","title":"Excel (.xls, .xlsx)","text":"<p>Should contain 'premise', 'hypothesis', and 'label' columns.</p>"},{"location":"text/fine_tune/nli/#geniusrise_text.nli.fine_tune.NLIFineTuner.load_dataset--sqlite-db","title":"SQLite (.db)","text":"<p>Should contain a table with 'premise', 'hypothesis', and 'label' columns.</p>"},{"location":"text/fine_tune/nli/#geniusrise_text.nli.fine_tune.NLIFineTuner.load_dataset--feather","title":"Feather","text":"<p>Should contain 'premise', 'hypothesis', and 'label' columns.</p>"},{"location":"text/fine_tune/nli/#geniusrise_text.nli.fine_tune.NLIFineTuner.prepare_train_features","title":"<code>prepare_train_features(examples)</code>","text":"<p>Tokenize the examples and prepare the features for training.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>dict</code> <p>A dictionary of examples.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict</code> <p>The processed features.</p>"},{"location":"text/fine_tune/question_answering/","title":"Question Answering","text":"<p>             Bases: <code>TextFineTuner</code></p> <p>A bolt for fine-tuning Hugging Face models on question answering tasks.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>The batch input data.</p> required <code>output</code> <code>OutputConfig</code> <p>The output data.</p> required <code>state</code> <code>State</code> <p>The state manager.</p> required <p>CLI Usage:</p> <pre><code>    genius QAFineTuner rise \\\nbatch \\\n--input_folder ./input \\\nbatch \\\n--output_folder ./output \\\nnone \\\n--id microsoft/tapex-large-finetuned-wtq-lol \\\nfine_tune \\\n--args \\\nmodel_name=my_model \\\ntokenizer_name=my_tokenizer \\\nnum_train_epochs=3 \\\nper_device_train_batch_size=8\n</code></pre>"},{"location":"text/fine_tune/question_answering/#geniusrise_text.qa.fine_tune.QAFineTuner.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>Initialize the bolt.</p> <pre><code>Args:\n    input (BatchInput): The batch input data.\n    output (OutputConfig): The output data.\n    state (State): The state manager.\n    **kwargs: Additional keyword arguments.\n</code></pre>"},{"location":"text/fine_tune/question_answering/#geniusrise_text.qa.fine_tune.QAFineTuner.compute_metrics","title":"<code>compute_metrics(eval_pred)</code>","text":"<p>Compute the accuracy of the model's predictions.</p> <p>Parameters:</p> Name Type Description Default <code>eval_pred</code> <code>tuple</code> <p>A tuple containing two elements: - predictions (np.ndarray): The model's predictions. - label_ids (np.ndarray): The true labels.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[Dict[str, float]]</code> <p>A dictionary mapping metric names to computed values.</p>"},{"location":"text/fine_tune/question_answering/#geniusrise_text.qa.fine_tune.QAFineTuner.load_dataset","title":"<code>load_dataset(dataset_path, pad_on_right=True, max_length=None, doc_stride=None, evaluate_squadv2=False, **kwargs)</code>","text":"<p>Load a dataset from a directory.</p>"},{"location":"text/fine_tune/question_answering/#geniusrise_text.qa.fine_tune.QAFineTuner.load_dataset--supported-data-formats-and-structures","title":"Supported Data Formats and Structures:","text":""},{"location":"text/fine_tune/question_answering/#geniusrise_text.qa.fine_tune.QAFineTuner.load_dataset--jsonl","title":"JSONL","text":"<p>Each line is a JSON object representing an example. <pre><code>{\"context\": \"The context content\", \"question\": \"The question\", \"answers\": {\"answer_start\": [int], \"text\": [str]}}\n</code></pre></p>"},{"location":"text/fine_tune/question_answering/#geniusrise_text.qa.fine_tune.QAFineTuner.load_dataset--csv","title":"CSV","text":"<p>Should contain 'context', 'question', and 'answers' columns. <pre><code>context,question,answers\n\"The context content\",\"The question\",\"{'answer_start': [int], 'text': [str]}\"\n</code></pre></p>"},{"location":"text/fine_tune/question_answering/#geniusrise_text.qa.fine_tune.QAFineTuner.load_dataset--parquet","title":"Parquet","text":"<p>Should contain 'context', 'question', and 'answers' columns.</p>"},{"location":"text/fine_tune/question_answering/#geniusrise_text.qa.fine_tune.QAFineTuner.load_dataset--json","title":"JSON","text":"<p>An array of dictionaries with 'context', 'question', and 'answers' keys. <pre><code>[{\"context\": \"The context content\", \"question\": \"The question\", \"answers\": {\"answer_start\": [int], \"text\": [str]}}]\n</code></pre></p>"},{"location":"text/fine_tune/question_answering/#geniusrise_text.qa.fine_tune.QAFineTuner.load_dataset--xml","title":"XML","text":"<p>Each 'record' element should contain 'context', 'question', and 'answers' child elements. <pre><code>&lt;record&gt;\n&lt;context&gt;The context content&lt;/context&gt;\n&lt;question&gt;The question&lt;/question&gt;\n&lt;answers answer_start=\"int\" text=\"str\"&gt;&lt;/answers&gt;\n&lt;/record&gt;\n</code></pre></p>"},{"location":"text/fine_tune/question_answering/#geniusrise_text.qa.fine_tune.QAFineTuner.load_dataset--yaml","title":"YAML","text":"<p>Each document should be a dictionary with 'context', 'question', and 'answers' keys. <pre><code>- context: \"The context content\"\nquestion: \"The question\"\nanswers:\nanswer_start: [int]\ntext: [str]\n</code></pre></p>"},{"location":"text/fine_tune/question_answering/#geniusrise_text.qa.fine_tune.QAFineTuner.load_dataset--tsv","title":"TSV","text":"<p>Should contain 'context', 'question', and 'answers' columns separated by tabs.</p>"},{"location":"text/fine_tune/question_answering/#geniusrise_text.qa.fine_tune.QAFineTuner.load_dataset--excel-xls-xlsx","title":"Excel (.xls, .xlsx)","text":"<p>Should contain 'context', 'question', and 'answers' columns.</p>"},{"location":"text/fine_tune/question_answering/#geniusrise_text.qa.fine_tune.QAFineTuner.load_dataset--sqlite-db","title":"SQLite (.db)","text":"<p>Should contain a table with 'context', 'question', and 'answers' columns.</p>"},{"location":"text/fine_tune/question_answering/#geniusrise_text.qa.fine_tune.QAFineTuner.load_dataset--feather","title":"Feather","text":"<p>Should contain 'context', 'question', and 'answers' columns.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the dataset directory.</p> required <code>pad_on_right</code> <code>bool</code> <p>Whether to pad on the right.</p> <code>True</code> <code>max_length</code> <code>int</code> <p>The maximum length of the sequences.</p> <code>None</code> <code>doc_stride</code> <code>int</code> <p>The document stride.</p> <code>None</code> <code>evaluate_squadv2</code> <code>bool</code> <p>Whether to evaluate using SQuAD v2 metrics.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Optional[Dataset]</code> <p>The loaded dataset.</p>"},{"location":"text/fine_tune/question_answering/#geniusrise_text.qa.fine_tune.QAFineTuner.prepare_train_features","title":"<code>prepare_train_features(examples, cls_token_id=None)</code>","text":"<p>Tokenize our examples with truncation and padding, but keep the overflows using a stride.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>Dict[str, Union[str, List[str]]]</code> <p>The examples to be tokenized.</p> required <p>Returns:</p> Type Description <code>Optional[Dict[str, Union[List[int], List[List[int]]]]]</code> <p>The tokenized examples.</p>"},{"location":"text/fine_tune/summarization/","title":"Summarization","text":"<p>             Bases: <code>TextFineTuner</code></p> <p>A bolt for fine-tuning Hugging Face models on summarization tasks.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>The batch input data.</p> required <code>output</code> <code>OutputConfig</code> <p>The output data.</p> required <code>state</code> <code>State</code> <p>The state manager.</p> required <p>CLI Usage:</p> <pre><code>    genius SummarizationFineTuner rise \\\nbatch \\\n--input_folder ./input \\\nbatch \\\n--output_folder ./output \\\nnone \\\nfine_tune \\\n--args \\\nmodel_name=my_model \\\ntokenizer_name=my_tokenizer \\\nnum_train_epochs=3 \\\nper_device_train_batch_size=8\n</code></pre>"},{"location":"text/fine_tune/summarization/#geniusrise_text.summarization.fine_tune.SummarizationFineTuner.compute_metrics","title":"<code>compute_metrics(pred)</code>","text":"<p>Compute ROUGE metrics.</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <code>EvalPrediction</code> <p>The predicted results.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, float]</code> <p>A dictionary with ROUGE-1, ROUGE-2, and ROUGE-L scores.</p>"},{"location":"text/fine_tune/summarization/#geniusrise_text.summarization.fine_tune.SummarizationFineTuner.data_collator","title":"<code>data_collator(examples)</code>","text":"<p>Customize the data collator.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>List[Dict[str, Union[str, List[int]]]]</code> <p>The examples to collate.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Union[List[int], List[List[int]]]]</code> <p>The collated data.</p>"},{"location":"text/fine_tune/summarization/#geniusrise_text.summarization.fine_tune.SummarizationFineTuner.load_dataset","title":"<code>load_dataset(dataset_path, **kwargs)</code>","text":"<p>Load a dataset from a directory.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the dataset directory.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[DatasetDict]</code> <p>Dataset | DatasetDict: The loaded dataset.</p>"},{"location":"text/fine_tune/summarization/#geniusrise_text.summarization.fine_tune.SummarizationFineTuner.load_dataset--supported-data-formats-and-structures","title":"Supported Data Formats and Structures:","text":""},{"location":"text/fine_tune/summarization/#geniusrise_text.summarization.fine_tune.SummarizationFineTuner.load_dataset--jsonl","title":"JSONL","text":"<p>Each line is a JSON object representing an example. <pre><code>{\"text\": \"The text content\", \"summary\": \"The summary\"}\n</code></pre></p>"},{"location":"text/fine_tune/summarization/#geniusrise_text.summarization.fine_tune.SummarizationFineTuner.load_dataset--csv","title":"CSV","text":"<p>Should contain 'text' and 'summary' columns. <pre><code>text,summary\n\"The text content\",\"The summary\"\n</code></pre></p>"},{"location":"text/fine_tune/summarization/#geniusrise_text.summarization.fine_tune.SummarizationFineTuner.load_dataset--parquet","title":"Parquet","text":"<p>Should contain 'text' and 'summary' columns.</p>"},{"location":"text/fine_tune/summarization/#geniusrise_text.summarization.fine_tune.SummarizationFineTuner.load_dataset--json","title":"JSON","text":"<p>An array of dictionaries with 'text' and 'summary' keys. <pre><code>[{\"text\": \"The text content\", \"summary\": \"The summary\"}]\n</code></pre></p>"},{"location":"text/fine_tune/summarization/#geniusrise_text.summarization.fine_tune.SummarizationFineTuner.load_dataset--xml","title":"XML","text":"<p>Each 'record' element should contain 'text' and 'summary' child elements. <pre><code>&lt;record&gt;\n&lt;text&gt;The text content&lt;/text&gt;\n&lt;summary&gt;The summary&lt;/summary&gt;\n&lt;/record&gt;\n</code></pre></p>"},{"location":"text/fine_tune/summarization/#geniusrise_text.summarization.fine_tune.SummarizationFineTuner.load_dataset--yaml","title":"YAML","text":"<p>Each document should be a dictionary with 'text' and 'summary' keys. <pre><code>- text: \"The text content\"\nsummary: \"The summary\"\n</code></pre></p>"},{"location":"text/fine_tune/summarization/#geniusrise_text.summarization.fine_tune.SummarizationFineTuner.load_dataset--tsv","title":"TSV","text":"<p>Should contain 'text' and 'summary' columns separated by tabs.</p>"},{"location":"text/fine_tune/summarization/#geniusrise_text.summarization.fine_tune.SummarizationFineTuner.load_dataset--excel-xls-xlsx","title":"Excel (.xls, .xlsx)","text":"<p>Should contain 'text' and 'summary' columns.</p>"},{"location":"text/fine_tune/summarization/#geniusrise_text.summarization.fine_tune.SummarizationFineTuner.load_dataset--sqlite-db","title":"SQLite (.db)","text":"<p>Should contain a table with 'text' and 'summary' columns.</p>"},{"location":"text/fine_tune/summarization/#geniusrise_text.summarization.fine_tune.SummarizationFineTuner.load_dataset--feather","title":"Feather","text":"<p>Should contain 'text' and 'summary' columns.</p>"},{"location":"text/fine_tune/summarization/#geniusrise_text.summarization.fine_tune.SummarizationFineTuner.prepare_train_features","title":"<code>prepare_train_features(examples)</code>","text":"<p>Tokenize the examples and prepare the features for training.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>dict</code> <p>A dictionary of examples.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[Dict[str, List[int]]]</code> <p>The processed features.</p>"},{"location":"text/fine_tune/translation/","title":"Translation","text":"<p>             Bases: <code>TextFineTuner</code></p> <p>A bolt for fine-tuning Hugging Face models on translation tasks.</p> <pre><code>Args:\n    input (BatchInput): The batch input data.\n    output (OutputConfig): The output data.\n    state (State): The state manager.\n    **kwargs: Arbitrary keyword arguments for extended functionality.\n</code></pre> <p>CLI Usage:</p> <pre><code>    genius TranslationFineTuner rise \\\nbatch \\\n--input_s3_bucket geniusrise-test \\\n--input_s3_folder input/trans \\\nbatch \\\n--output_s3_bucket geniusrise-test \\\n--output_s3_folder output/trans \\\npostgres \\\n--postgres_host 127.0.0.1 \\\n--postgres_port 5432 \\\n--postgres_user postgres \\\n--postgres_password postgres \\\n--postgres_database geniusrise\\\n--postgres_table state \\\n--id facebook/mbart-large-50-many-to-many-mmt-lol \\\nfine_tune \\\n--args \\\nmodel_name=my_model \\\ntokenizer_name=my_tokenizer \\\nnum_train_epochs=3 \\\nper_device_train_batch_size=8 \\\ndata_max_length=512\n</code></pre>"},{"location":"text/fine_tune/translation/#geniusrise_text.translation.fine_tune.TranslationFineTuner.data_collator","title":"<code>data_collator(examples)</code>","text":"<p>Customize the data collator.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <p>The examples to collate.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>The collated data.</p>"},{"location":"text/fine_tune/translation/#geniusrise_text.translation.fine_tune.TranslationFineTuner.load_dataset","title":"<code>load_dataset(dataset_path, max_length=512, origin='en', target='fr', **kwargs)</code>","text":"<p>Load a dataset from a directory.</p>"},{"location":"text/fine_tune/translation/#geniusrise_text.translation.fine_tune.TranslationFineTuner.load_dataset--supported-data-formats-and-structures-for-translation-tasks","title":"Supported Data Formats and Structures for Translation Tasks:","text":""},{"location":"text/fine_tune/translation/#geniusrise_text.translation.fine_tune.TranslationFineTuner.load_dataset--jsonl","title":"JSONL","text":"<p>Each line is a JSON object representing an example. <pre><code>{\n\"translation\": {\n\"en\": \"English text\",\n\"fr\": \"French text\"\n}\n}\n</code></pre></p>"},{"location":"text/fine_tune/translation/#geniusrise_text.translation.fine_tune.TranslationFineTuner.load_dataset--csv","title":"CSV","text":"<p>Should contain 'en' and 'fr' columns. <pre><code>en,fr\n\"English text\",\"French text\"\n</code></pre></p>"},{"location":"text/fine_tune/translation/#geniusrise_text.translation.fine_tune.TranslationFineTuner.load_dataset--parquet","title":"Parquet","text":"<p>Should contain 'en' and 'fr' columns.</p>"},{"location":"text/fine_tune/translation/#geniusrise_text.translation.fine_tune.TranslationFineTuner.load_dataset--json","title":"JSON","text":"<p>An array of dictionaries with 'en' and 'fr' keys. <pre><code>[\n{\n\"en\": \"English text\",\n\"fr\": \"French text\"\n}\n]\n</code></pre></p>"},{"location":"text/fine_tune/translation/#geniusrise_text.translation.fine_tune.TranslationFineTuner.load_dataset--xml","title":"XML","text":"<p>Each 'record' element should contain 'en' and 'fr' child elements. <pre><code>&lt;record&gt;\n&lt;en&gt;English text&lt;/en&gt;\n&lt;fr&gt;French text&lt;/fr&gt;\n&lt;/record&gt;\n</code></pre></p>"},{"location":"text/fine_tune/translation/#geniusrise_text.translation.fine_tune.TranslationFineTuner.load_dataset--yaml","title":"YAML","text":"<p>Each document should be a dictionary with 'en' and 'fr' keys. <pre><code>- en: \"English text\"\nfr: \"French text\"\n</code></pre></p>"},{"location":"text/fine_tune/translation/#geniusrise_text.translation.fine_tune.TranslationFineTuner.load_dataset--tsv","title":"TSV","text":"<p>Should contain 'en' and 'fr' columns separated by tabs.</p>"},{"location":"text/fine_tune/translation/#geniusrise_text.translation.fine_tune.TranslationFineTuner.load_dataset--excel-xls-xlsx","title":"Excel (.xls, .xlsx)","text":"<p>Should contain 'en' and 'fr' columns.</p>"},{"location":"text/fine_tune/translation/#geniusrise_text.translation.fine_tune.TranslationFineTuner.load_dataset--sqlite-db","title":"SQLite (.db)","text":"<p>Should contain a table with 'en' and 'fr' columns.</p>"},{"location":"text/fine_tune/translation/#geniusrise_text.translation.fine_tune.TranslationFineTuner.load_dataset--feather","title":"Feather","text":"<p>Should contain 'en' and 'fr' columns.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the directory containing the dataset files.</p> required <code>max_length</code> <code>int</code> <p>The maximum length for tokenization. Defaults to 512.</p> <code>512</code> <code>origin</code> <code>str</code> <p>The origin language. Defaults to 'en'.</p> <code>'en'</code> <code>target</code> <code>str</code> <p>The target language. Defaults to 'fr'.</p> <code>'fr'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>DatasetDict</code> <code>Optional[DatasetDict]</code> <p>The loaded dataset.</p>"},{"location":"text/fine_tune/translation/#geniusrise_text.translation.fine_tune.TranslationFineTuner.prepare_train_features","title":"<code>prepare_train_features(examples)</code>","text":"<p>Tokenize the examples and prepare the features for training.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>dict</code> <p>A dictionary of examples.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>The processed features.</p>"},{"location":"vision/api/base/","title":"Vision Base","text":"<p>             Bases: <code>VisionBulk</code></p> <p>The VisionAPI class inherits from VisionBulk and is designed to facilitate the handling of vision-based tasks using a pre-trained machine learning model. It sets up a server to process image-related requests using a specified model.</p>"},{"location":"vision/api/base/#geniusrise_vision.base.api.VisionAPI.__init__","title":"<code>__init__(input, output, state)</code>","text":"<p>Initializes the VisionAPI object with batch input, output, and state.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>Object to handle batch input operations.</p> required <code>output</code> <code>BatchOutput</code> <p>Object to handle batch output operations.</p> required <code>state</code> <code>State</code> <p>Object to maintain the state of the API.</p> required"},{"location":"vision/api/base/#geniusrise_vision.base.api.VisionAPI.listen","title":"<code>listen(model_name, model_class='AutoModel', processor_class='AutoProcessor', device_map='auto', max_memory={0: '24GB'}, use_cuda=False, precision='float16', quantization=0, torchscript=False, compile=False, flash_attention=False, better_transformers=False, concurrent_queries=False, endpoint='*', port=3000, cors_domain='http://localhost:3000', username=None, password=None, **model_args)</code>","text":"<p>Configures and starts a CherryPy server to listen for image processing requests.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the pre-trained vision model.</p> required <code>model_class</code> <code>str</code> <p>The class of the pre-trained vision model. Defaults to \"AutoModel\".</p> <code>'AutoModel'</code> <code>processor_class</code> <code>str</code> <p>The class of the processor for input image preprocessing. Defaults to \"AutoProcessor\".</p> <code>'AutoProcessor'</code> <code>device_map</code> <code>str | Dict | None</code> <p>Device mapping for model inference. Defaults to \"auto\".</p> <code>'auto'</code> <code>max_memory</code> <code>Dict[int, str]</code> <p>Maximum memory allocation for model inference. Defaults to {0: \"24GB\"}.</p> <code>{0: '24GB'}</code> <code>precision</code> <code>str</code> <p>The floating-point precision to be used by the model. Options are 'float32', 'float16', 'bfloat16'.</p> <code>'float16'</code> <code>quantization</code> <code>int</code> <p>The bit level for model quantization (0 for none, 8 for 8-bit quantization).</p> <code>0</code> <code>torchscript</code> <code>bool</code> <p>Whether to use TorchScript for model optimization. Defaults to True.</p> <code>False</code> <code>compile</code> <code>bool</code> <p>Whether to compile the model before fine-tuning. Defaults to False.</p> <code>False</code> <code>flash_attention</code> <code>bool</code> <p>Whether to use flash attention 2. Default is False.</p> <code>False</code> <code>better_transformers</code> <code>bool</code> <p>Flag to enable Better Transformers optimization for faster processing.</p> <code>False</code> <code>concurrent_queries</code> <code>bool</code> <p>(bool): Whether the API supports concurrent API calls (usually false).</p> <code>False</code> <code>endpoint</code> <code>str</code> <p>The network endpoint for the server. Defaults to \"*\".</p> <code>'*'</code> <code>port</code> <code>int</code> <p>The network port for the server. Defaults to 3000.</p> <code>3000</code> <code>cors_domain</code> <code>str</code> <p>The domain to allow for CORS requests. Defaults to \"http://localhost:3000\".</p> <code>'http://localhost:3000'</code> <code>username</code> <code>Optional[str]</code> <p>Username for server authentication. Defaults to None.</p> <code>None</code> <code>password</code> <code>Optional[str]</code> <p>Password for server authentication. Defaults to None.</p> <code>None</code> <code>**model_args</code> <code>Any</code> <p>Additional arguments for the vision model.</p> <code>{}</code>"},{"location":"vision/api/base/#geniusrise_vision.base.api.VisionAPI.validate_password","title":"<code>validate_password(realm, username, password)</code>","text":"<p>Validate the username and password against expected values.</p> <p>Parameters:</p> Name Type Description Default <code>realm</code> <code>str</code> <p>The authentication realm.</p> required <code>username</code> <code>str</code> <p>The provided username.</p> required <code>password</code> <code>str</code> <p>The provided password.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if credentials are valid, False otherwise.</p>"},{"location":"vision/api/imgclass/","title":"Image Classsification API","text":"<p>             Bases: <code>VisionAPI</code></p> <p>ImageClassificationAPI extends the VisionAPI for image classification tasks. This API provides functionalities to classify images into various categories based on the trained model it uses. It supports both single-label and multi-label classification problems.</p> Methods <p>classify_image(self): Endpoint to classify an uploaded image and return the classification scores. sigmoid(self, _outputs): Applies the sigmoid function to the model's outputs. softmax(self, _outputs): Applies the softmax function to the model's outputs.</p> <p>Example CLI Usage:</p> <pre><code>genius ImageClassificationAPI rise \\\nbatch \\\n--input_folder ./input \\\nbatch \\\n--output_folder ./output \\\nnone \\\nlisten \\\n--args \\\nmodel_name=\"Kaludi/food-category-classification-v2.0\" \\\nmodel_class=\"AutoModelForImageClassification\" \\\nprocessor_class=\"AutoImageProcessor\" \\\ndevice_map=\"cuda:0\" \\\nuse_cuda=True \\\nprecision=\"float\" \\\nquantization=0 \\\nmax_memory=None \\\ntorchscript=False \\\ncompile=False \\\nflash_attention=False \\\nbetter_transformers=False \\\nendpoint=\"*\" \\\nport=3000 \\\ncors_domain=\"http://localhost:3000\" \\\nusername=\"user\" \\\npassword=\"password\"\n</code></pre>"},{"location":"vision/api/imgclass/#geniusrise_vision.imgclass.api.ImageClassificationAPI.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>Initializes the ImageClassificationAPI with the necessary configurations for input, output, and state management, along with model-specific parameters.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>Configuration for the input data.</p> required <code>output</code> <code>BatchOutput</code> <p>Configuration for the output data.</p> required <code>state</code> <code>State</code> <p>State management for the API.</p> required <code>**kwargs</code> <p>Additional keyword arguments for extended functionality, such as model configuration.</p> <code>{}</code>"},{"location":"vision/api/imgclass/#geniusrise_vision.imgclass.api.ImageClassificationAPI.classify_image","title":"<code>classify_image()</code>","text":"<p>Endpoint for classifying an image. It accepts a base64-encoded image, decodes it, preprocesses it, and runs it through the classification model. It supports both single-label and multi-label classification by applying the appropriate post-processing function to the model outputs.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the predictions with the highest scores and all prediction scores.</p> <code>Dict[str, Any]</code> <p>Each prediction includes the label and its corresponding score.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If an error occurs during image processing or classification.</p> <p>Example CURL Request: <pre><code>curl -X POST localhost:3000/api/v1/classify_image             -H \"Content-Type: application/json\"             -d '{\"image_base64\": \"&lt;base64-encoded-image&gt;\"}'\n</code></pre></p> <p>or to feed an image: <pre><code>(base64 -w 0 cat.jpg | awk '{print \"{\"image_base64\": \"\"$0\"\"}\"}' &gt; /tmp/image_payload.json)\ncurl -X POST http://localhost:3000/api/v1/classify_image             -H \"Content-Type: application/json\"             -u user:password             -d @/tmp/image_payload.json | jq\n</code></pre></p>"},{"location":"vision/api/imgclass/#geniusrise_vision.imgclass.api.ImageClassificationAPI.sigmoid","title":"<code>sigmoid(_outputs)</code>","text":"<p>Applies the sigmoid function to the model's outputs for binary classification or multi-label classification tasks.</p> <p>Parameters:</p> Name Type Description Default <code>_outputs</code> <code>np.ndarray</code> <p>The raw outputs from the model.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: The outputs after applying the sigmoid function.</p>"},{"location":"vision/api/imgclass/#geniusrise_vision.imgclass.api.ImageClassificationAPI.softmax","title":"<code>softmax(_outputs)</code>","text":"<p>Applies the softmax function to the model's outputs for single-label classification tasks, ensuring the output scores sum to 1 across classes.</p> <p>Parameters:</p> Name Type Description Default <code>_outputs</code> <code>np.ndarray</code> <p>The raw outputs from the model.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: The outputs after applying the softmax function.</p>"},{"location":"vision/api/ocr/","title":"OCR API","text":"<p>             Bases: <code>VisionAPI</code></p> <p>ImageOCRAPI provides Optical Character Recognition (OCR) capabilities for images, leveraging different OCR engines like EasyOCR, PaddleOCR, and Hugging Face models tailored for OCR tasks. This API can decode base64-encoded images, process them through the chosen OCR engine, and return the recognized text.</p> <p>The API supports dynamic selection of OCR engines and configurations based on the provided model name and arguments, offering flexibility in processing various languages and image types.</p> Methods <p>ocr(self): Processes an uploaded image for OCR and returns the recognized text.</p> <p>Example CLI Usage:</p> <p>EasyOCR:</p> <pre><code>genius ImageOCRAPI rise \\\nbatch \\\n--input_folder ./input \\\nbatch \\\n--output_folder ./output \\\nnone \\\nlisten \\\n--args \\\nmodel_name=\"easyocr\" \\\ndevice_map=\"cuda:0\" \\\nendpoint=\"*\" \\\nport=3000 \\\ncors_domain=\"http://localhost:3000\" \\\nusername=\"user\" \\\npassword=\"password\"\n</code></pre> <p>Paddle OCR:</p> <pre><code>genius ImageOCRAPI rise \\\nbatch \\\n--input_folder ./input \\\nbatch \\\n--output_folder ./output \\\nnone \\\nlisten \\\n--args \\\nmodel_name=\"paddleocr\" \\\ndevice_map=\"cuda:0\" \\\nendpoint=\"*\" \\\nport=3000 \\\ncors_domain=\"http://localhost:3000\" \\\nusername=\"user\" \\\npassword=\"password\"\n</code></pre> <p>Huggingface models:</p> <pre><code>genius ImageOCRAPI rise \\\nbatch \\\n--input_folder ./input \\\nbatch \\\n--output_folder ./output \\\nnone \\\nlisten \\\n--args \\\nmodel_name=\"facebook/nougat-base\" \\\nmodel_class=\"VisionEncoderDecoderModel\" \\\nprocessor_class=\"NougatProcessor\" \\\ndevice_map=\"cuda:0\" \\\nuse_cuda=True \\\nprecision=\"float\" \\\nquantization=0 \\\nmax_memory=None \\\ntorchscript=False \\\ncompile=False \\\nflash_attention=False \\\nbetter_transformers=False \\\nendpoint=\"*\" \\\nport=3000 \\\ncors_domain=\"http://localhost:3000\" \\\nusername=\"user\" \\\npassword=\"password\"\n</code></pre>"},{"location":"vision/api/ocr/#geniusrise_vision.ocr.api.ImageOCRAPI.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>Initializes the ImageOCRAPI with configurations for input, output, state management, and OCR model specifics.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>Configuration for the input data.</p> required <code>output</code> <code>BatchOutput</code> <p>Configuration for the output data.</p> required <code>state</code> <code>State</code> <p>State management for the API.</p> required <code>**kwargs</code> <p>Additional keyword arguments for extended functionality.</p> <code>{}</code>"},{"location":"vision/api/ocr/#geniusrise_vision.ocr.api.ImageOCRAPI.ocr","title":"<code>ocr()</code>","text":"<p>Endpoint for performing OCR on an uploaded image. It accepts a base64-encoded image, decodes it, preprocesses it through the specified OCR model, and returns the recognized text.</p> <p>Returns:</p> Type Description <p>Dict[str, Any]: A dictionary containing the success status, recognized text ('result'), and the original</p> <p>image name ('image_name') if provided.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If an error occurs during image processing or OCR.</p> <p>Example CURL Request: <pre><code>curl -X POST localhost:3000/api/v1/ocr             -H \"Content-Type: application/json\"             -d '{\"image_base64\": \"&lt;base64-encoded-image&gt;\", \"model_name\": \"easyocr\", \"use_easyocr_bbox\": true}'\n</code></pre></p> <p>or</p> <pre><code>(base64 -w 0 test_images_ocr/ReceiptSwiss.jpg | awk '{print \"{\"image_base64\": \"\"$0\"\", \"max_length\": 1024}\"}' &gt; /tmp/image_payload.json)\ncurl -X POST http://localhost:3000/api/v1/ocr             -H \"Content-Type: application/json\"             -u user:password             -d @/tmp/image_payload.json | jq\n</code></pre>"},{"location":"vision/api/ocr/#geniusrise_vision.ocr.api.ImageOCRAPI.process_huggingface_models","title":"<code>process_huggingface_models(image, use_easyocr_bbox)</code>","text":"<p>Processes the image using a Hugging Face model specified for OCR tasks. Supports advanced configurations and post-processing to handle various OCR-related challenges.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image.Image</code> <p>The image to process.</p> required <code>use_easyocr_bbox</code> <code>bool</code> <p>Whether to use EasyOCR to detect text bounding boxes before processing with                      Hugging Face models.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The recognized text from the image.</p>"},{"location":"vision/api/ocr/#geniusrise_vision.ocr.api.ImageOCRAPI.process_other_models","title":"<code>process_other_models(image)</code>","text":"<p>Processes the image using non-Hugging Face OCR models like EasyOCR or PaddleOCR based on the initialization.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image.Image</code> <p>The image to process.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The OCR results which might include text, bounding boxes, and confidence scores depending on the model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid or unsupported OCR model is specified.</p>"},{"location":"vision/api/segment/","title":"Image Segmentation API","text":"<p>             Bases: <code>VisionAPI</code></p> <p>VisionSegmentationAPI extends VisionAPI to provide image segmentation functionalities, including panoptic, instance, and semantic segmentation. This API supports different segmentation tasks based on the model's capabilities and the specified subtask in the request.</p> Methods <p>segment_image(self): Processes an image for segmentation and returns the segmentation masks along with labels.</p> <p>Example CLI Usage:</p> <pre><code>genius VisionSegmentationAPI rise \\\nbatch \\\n--input_folder ./input \\\nbatch \\\n--output_folder ./output \\\nnone \\\nlisten \\\n--args \\\nmodel_name=\"facebook/mask2former-swin-large-mapillary-vistas-semantic\" \\\nmodel_class=\"Mask2FormerForUniversalSegmentation\" \\\nprocessor_class=\"AutoImageProcessor\" \\\ndevice_map=\"cuda:0\" \\\nuse_cuda=True \\\nprecision=\"float\" \\\nquantization=0 \\\nmax_memory=None \\\ntorchscript=False \\\ncompile=False \\\nflash_attention=False \\\nbetter_transformers=False \\\nendpoint=\"*\" \\\nport=3000 \\\ncors_domain=\"http://localhost:3000\" \\\nusername=\"user\" \\\npassword=\"password\"\n</code></pre>"},{"location":"vision/api/segment/#geniusrise_vision.segment.api.VisionSegmentationAPI.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>Initializes the VisionSegmentationAPI with configurations for input, output, and state management, along with any model-specific parameters for segmentation tasks.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>Configuration for the input data.</p> required <code>output</code> <code>BatchOutput</code> <p>Configuration for the output data.</p> required <code>state</code> <code>State</code> <p>State management for the API.</p> required <code>**kwargs</code> <p>Additional keyword arguments for extended functionality.</p> <code>{}</code>"},{"location":"vision/api/segment/#geniusrise_vision.segment.api.VisionSegmentationAPI.segment_image","title":"<code>segment_image()</code>","text":"<p>Endpoint for segmenting an image according to the specified subtask (panoptic, instance, or semantic segmentation). It decodes the base64-encoded image, processes it through the model, and returns the segmentation masks along with labels and scores (if applicable) in base64 format.</p> <p>The method supports dynamic task inputs for models requiring specific task descriptions and applies different post-processing techniques based on the subtask.</p> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: A list of dictionaries where each dictionary contains a 'label', a 'score' (if applicable),</p> <code>List[Dict[str, Any]]</code> <p>and a 'mask' (base64-encoded image of the segmentation mask).</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If an error occurs during image processing or segmentation.</p> <p>Example CURL Request: <pre><code>curl -X POST localhost:3000/api/v1/segment_image             -H \"Content-Type: application/json\"             -d '{\"image_base64\": \"&lt;base64-encoded-image&gt;\", \"subtask\": \"panoptic\"}'\n</code></pre></p> <p>or to save all masks:</p> <pre><code>(base64 -w 0 guy.jpg | awk '{print \"{\"image_base64\": \"\"$0\"\", \"subtask\": \"semantic\"}\"}' &gt; /tmp/image_payload.json)\ncurl -X POST http://localhost:3000/api/v1/segment_image             -H \"Content-Type: application/json\"             -u user:password             -d @/tmp/image_payload.json | jq -r '.[] | .mask + \" \" + .label' | while read mask label; do echo $mask | base64 --decode &gt; \"${label}.jpg\"; done\n</code></pre>"},{"location":"vision/api/vqa/","title":"Visual Question Answering","text":"<p>             Bases: <code>VisionAPI</code></p> <p>VisualQAAPI extends VisionAPI to provide an interface for visual question answering (VQA) tasks. This API supports answering questions about an image by utilizing deep learning models specifically trained for VQA. It processes requests containing an image and a question about the image, performs inference using the loaded model, and returns the predicted answer.</p> Methods <p>answer_question(self): Receives an image and a question, returns the answer based on visual content.</p> <p>Example CLI Usage:</p> <pre><code>genius VisualQAAPI rise \\\nbatch \\\n--input_folder ./input \\\nbatch \\\n--output_folder ./output \\\nnone \\\nlisten \\\n--args \\\nmodel_name=\"llava-hf/bakLlava-v1-hf\" \\\nmodel_class=\"LlavaForConditionalGeneration\" \\\nprocessor_class=\"AutoProcessor\" \\\ndevice_map=\"cuda:0\" \\\nuse_cuda=True \\\nprecision=\"bfloat16\" \\\nquantization=0 \\\nmax_memory=None \\\ntorchscript=False \\\ncompile=False \\\nflash_attention=False \\\nbetter_transformers=False \\\nendpoint=\"*\" \\\nport=3000 \\\ncors_domain=\"http://localhost:3000\" \\\nusername=\"user\" \\\npassword=\"password\"\n</code></pre>"},{"location":"vision/api/vqa/#geniusrise_vision.vqa.api.VisualQAAPI.__init__","title":"<code>__init__(input, output, state, **kwargs)</code>","text":"<p>Initializes the VisualQAAPI with configurations for input, output, state management, and any model-specific parameters for visual question answering tasks.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>BatchInput</code> <p>Configuration for the input data.</p> required <code>output</code> <code>BatchOutput</code> <p>Configuration for the output data.</p> required <code>state</code> <code>State</code> <p>State management for the API.</p> required <code>**kwargs</code> <p>Additional keyword arguments for extended functionality.</p> <code>{}</code>"},{"location":"vision/api/vqa/#geniusrise_vision.vqa.api.VisualQAAPI.answer_question","title":"<code>answer_question()</code>","text":"<p>Endpoint for receiving an image with a question and returning the answer based on the visual content of the image. It processes the request containing a base64-encoded image and a question string, and utilizes the loaded model to predict the answer to the question related to the image.</p> <p>Returns:</p> Type Description <p>Dict[str, Any]: A dictionary containing the original question and the predicted answer.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required fields 'image_base64' and 'question' are not provided in the request.</p> <code>Exception</code> <p>If an error occurs during image processing or inference.</p> <p>Example CURL Request: <pre><code>curl -X POST localhost:3000/api/v1/answer_question             -H \"Content-Type: application/json\"             -d '{\"image_base64\": \"&lt;base64-encoded-image&gt;\", \"question\": \"What is the color of the sky in the image?\"}'\n</code></pre></p> <p>or</p> <pre><code>(base64 -w 0 test_images_segment_finetune/image1.jpg | awk '{print \"{\"image_base64\": \"\"$0\"\", \"question\": \"how many cats are there?\"}\"}' &gt; /tmp/image_payload.json)\ncurl -X POST http://localhost:3000/api/v1/answer_question             -H \"Content-Type: application/json\"             -u user:password             -d @/tmp/image_payload.json | jq\n</code></pre>"}]}